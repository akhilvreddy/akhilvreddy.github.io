<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Reimplementations on Akhil’s Blog</title>
    <link>https://akhilvreddy.github.io/reimplementations/</link>
    <description>Recent content in Reimplementations on Akhil’s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 12 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://akhilvreddy.github.io/reimplementations/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ResNet: Deep Residual Learning for Image Recognition (He et al. - 2015)</title>
      <link>https://akhilvreddy.github.io/reimplementations/resnet/</link>
      <pubDate>Sat, 12 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/resnet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/resnet-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ResNet is one of those architectures that just works extremely well so I wanted to see what actually makes it work under the hood (specifically the residual connections). I reimplemented ResNet-18 and ResNet-50 in pure PyTorch, including the full residual block logic, downsampling with 1×1 convolutions, and identity shortcuts. I wanted everything to be modular, clean, and designed to mirror the original paper closely while still being testable and readable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adam: A Method for Stochastic Optimization (Kingma &amp; Ba - 2014)</title>
      <link>https://akhilvreddy.github.io/reimplementations/adam/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/adam/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6980&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/adam-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Optimizers are the heart of everything in deep learning and I knew how the math works behind them but I never implemented one from scratch. I wanted to do Adam because it&amp;rsquo;s my most favorite optimizer and I wanted to really understand how it works under the hood, not just pass &lt;code&gt;Adam(model.parameters())&lt;/code&gt; and move on. I reimplemented the optimizer from scratch using pure PyTorch by tracking moving averages for the first and second moments (m and v), bias correction, learning rate scheduling, and weight decay. Everything was done without relying on PyTorch’s built-in optimizer classes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attention Is All You Need (Vaswani et al. - 2017)</title>
      <link>https://akhilvreddy.github.io/reimplementations/attentionisallyouneed/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/attentionisallyouneed/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/transformers-reimplementation/tree/main&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Implementing this paper is something I&amp;rsquo;ve wanted to do for a while and after implementing LayerNorm and Adam I started to feel more comfortable with my PyTorch skills. I first implemented a full on encoder/decoder model but then decided that I actually wanted to make it &lt;em&gt;talk&lt;/em&gt; like ChatGPT so I switched the implementation to the full decoder-only transformer architecture from scratch (from token embeddings to multi-head self-attention to the output projection). Everything is structured in &lt;code&gt;/transformer&lt;/code&gt;: learned embeddings, sinusoidal positional encodings, residual connections, masking, batching, and MPS support to run fast on my Mac.&lt;/p&gt;</description>
    </item>
    <item>
      <title>LayerNorm (Ba, Kiros, and Hinton - 2016)</title>
      <link>https://akhilvreddy.github.io/reimplementations/layernorm/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/layernorm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.06450&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/layernorm-reimplementation/tree/main&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In my opinion, LayerNorm was easier to implement than BatchNorm because it&amp;rsquo;s basically the same idea, just applied in a different dimension. Unlike BatchNorm, we don&amp;rsquo;t have to keep track of batch-level statistics in training or running statistics in eval, which helps reduce a lot of overhead. I liked how LayerNorm is fully independent of external hyperparams (like batch size) making it super simple to add in at any layer.
I implemented the &lt;code&gt;__init__&lt;/code&gt; method and the forward pass (just stuck with &lt;code&gt;loss.backward()&lt;/code&gt;). Once I understood that you&amp;rsquo;re just normalizing all features within a single sample, and optionally learning a scale and shift (just like batchnorm), I was able to see how it is really working. Writing it from scratch helped me understand why it helps with stability and training speed, and why it shows up in every transformer or modern deep learning model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>BatchNorm (Ioffe &amp; Szegedy - 2015)</title>
      <link>https://akhilvreddy.github.io/reimplementations/batchnorm/</link>
      <pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/batchnorm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.03167&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/batchnorm-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Honestly BatchNorm wasn’t as complicated to implement as I expected. I implemented the &lt;code&gt;__init__&lt;/code&gt; method and the forward pass (just stuck with &lt;code&gt;loss.backward&lt;/code&gt;). Once I understood that you normalize each feature across the batch, then optionally learn a scale and shift, it was pretty straightforward. Writing it from scratch helped me understand why it helps with stability and training speed, and why it shows up in basically every model.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
