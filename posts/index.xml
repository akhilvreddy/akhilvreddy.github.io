<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Akhil’s Blog</title>
    <link>https://akhilvreddy.github.io/posts/</link>
    <description>Recent content in Posts on Akhil’s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://akhilvreddy.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton</title>
      <link>https://akhilvreddy.github.io/posts/cuda2/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/cuda2/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.&lt;/p&gt;
&lt;p&gt;The repsitory with the corresponding PyTorch implementation is available &lt;a href=&#34;https://github.com/akhilvreddy/word2vec-scratch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA really isn&#39;t that bad: Kernel Ops and Memory Hierarchy</title>
      <link>https://akhilvreddy.github.io/posts/cuda/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/cuda/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post demystifies the core concepts behind CUDA, walking through how GPU kernels work, how threads and memory hierarchies are structured, and how to write and launch a kernel. It’s a hands-on introduction to CUDA for engineers who want to understand how deep learning frameworks really run under the hood (and how writing a few lines of CUDA can unlock massive speedups).&lt;/p&gt;
&lt;p&gt;The repsitory with the corresponding PyTorch implementation is available &lt;a href=&#34;https://github.com/akhilvreddy/word2vec-scratch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing &amp; Building an Agentic AI framework from scratch</title>
      <link>https://akhilvreddy.github.io/posts/agentic-framework/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/agentic-framework/</guid>
      <description>I&amp;rsquo;ve seen a lot of people use pre-made agentic frameworks to handle various tasks but I wanted to create a framework from scratch to see the brain, actions, and tools behind these agents.</description>
    </item>
    <item>
      <title>Tesla&#39;s Robotaxi &gt; Google&#39;s Waymo: Vision vs. LiDAR</title>
      <link>https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/</guid>
      <description>In the coming years, I believe Robotaxi will scale like Uber, while Waymo will scale like Lyft - if at all. This reflects a broader paradigm shift: lean vision neural networks vs. bulky, sensor-heavy autonomy.</description>
    </item>
    <item>
      <title>Deploying a toy ML model to production</title>
      <link>https://akhilvreddy.github.io/posts/deploying-model/</link>
      <pubDate>Sat, 14 Jun 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/deploying-model/</guid>
      <description>I&amp;rsquo;ve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens &lt;em&gt;after&lt;/em&gt; training.</description>
    </item>
    <item>
      <title>Fun with LoRA: How low-rank can we go before adjacency matrices break down?</title>
      <link>https://akhilvreddy.github.io/posts/tiny-lora/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/tiny-lora/</guid>
      <description>How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.</description>
    </item>
    <item>
      <title>Designing AlphaTicTacToe (and AlphaTicTacToeZero)</title>
      <link>https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/</guid>
      <description>AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.</description>
    </item>
    <item>
      <title>Can a EMNIST model run on an Amazon Kindle from 2012?</title>
      <link>https://akhilvreddy.github.io/posts/quantized-emnist/</link>
      <pubDate>Thu, 10 Apr 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/quantized-emnist/</guid>
      <description>What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.</description>
    </item>
    <item>
      <title>Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT</title>
      <link>https://akhilvreddy.github.io/posts/attention-from-scratch/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/attention-from-scratch/</guid>
      <description>I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.</description>
    </item>
    <item>
      <title>How did we get to Transformers? The rise of Attention</title>
      <link>https://akhilvreddy.github.io/posts/attention-beginnings/</link>
      <pubDate>Fri, 28 Feb 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/attention-beginnings/</guid>
      <description>Attention is really useful but just like any great invention, it was built out of necessity for the problems at hand. In this post, I aim to dive into the issues researchers were dealing with between 2013-2017.</description>
    </item>
    <item>
      <title>Word2Vec from scratch: Intuition to Implementation</title>
      <link>https://akhilvreddy.github.io/posts/word2vec-scratch/</link>
      <pubDate>Fri, 10 Jan 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/word2vec-scratch/</guid>
      <description>Q(king) - Q(man) + Q(woman) ≈ Q(queen) is beautiful but the approach we take to get there is even cooler. I wanted to dive into how these emergent properties come to be and the bottlenecks we face.</description>
    </item>
    <item>
      <title>My goals for this blog</title>
      <link>https://akhilvreddy.github.io/posts/first-post/</link>
      <pubDate>Sun, 29 Dec 2024 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/first-post/</guid>
      <description>I guess I&amp;rsquo;m writing a blog now.</description>
    </item>
  </channel>
</rss>
