<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Can a EMNIST model run on an Amazon Kindle from 2012? | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.">
<meta name="author" content="ML Systems &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/quantized-emnist/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/quantized-emnist/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/quantized-emnist/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="Can a EMNIST model run on an Amazon Kindle from 2012?">
  <meta property="og:description" content="What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-04-10T18:12:56-04:00">
    <meta property="article:modified_time" content="2025-04-10T18:12:56-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Can a EMNIST model run on an Amazon Kindle from 2012?">
<meta name="twitter:description" content="What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Can a EMNIST model run on an Amazon Kindle from 2012?",
      "item": "https://akhilvreddy.github.io/posts/quantized-emnist/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Can a EMNIST model run on an Amazon Kindle from 2012?",
  "name": "Can a EMNIST model run on an Amazon Kindle from 2012?",
  "description": "What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: I trained a character recognition model on EMNIST, quantized it, and tried running it on hardware with less RAM than a Chrome tab. Here’s how I simulated its specs in Docker, built a tiny PyTorch inference loop, and learned why deploying to the real Kindle OS is basically impossible (but very fun).\nThe repsitory with the corresponding PyTorch implementation is available here.\nLately, I’ve been trying to level up from just training PyTorch models and evaluating them locally to actually wrapping them as APIs and deploying them. That naturally had me thinking: how far can you actually push a model? Not just in terms of scalable deployment, but portability. I’ve been curious about edge deployment, and I know modern iPhones have neural chips that make running on-device models easy. But what about the opposite end?\nI wanted to run an experiment: could a model trained on EMNIST (extended MNIST, a dataset from 2017), actually run on a Kindle? In this post, I’ll walk through how I trained and quantized the model, set up an interactive UI, and simulated decade-old hardware using Docker, all to answer a simple but oddly satisfying question:\nCan a fossilized Amazon Kindle recognize handwritten letters and numbers?\nBefore I went into EMNIST I wanted to start simple and make sure that I was training properly and my visualizations looked fine.\nTraining a simple MNIST Since I wanted to do everything from scratch in this project, I wanted to start with training MNIST just as a practice before EMNIST and also to visualize it with streamlit.\nThis is pretty simple and you’ve probably seen this many times:\n1) Setting up environment We can either do this in a docker container or a new .venv environment. I’m going to do the latter for simplicity (we’re going to have to use docker later anyways so I took the easy route here).\npython3 -m venv .venv source .venv/bin/activate pip install torch torchvision matplotlib 2) Download the dataset I love torchvision because it makes this super simple.\nfrom torchvision import datasets, transforms transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform) 3) Create dataloaders from torch.utils.data import DataLoader train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False) 4) Building the model import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super().__init__() self.fc1 = nn.Linear(28*28, 128) self.fc2 = nn.Linear(128, 64) self.out = nn.Linear(64, 10) def forward(self, x): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) return self.out(x) The hidden layer counts come from the orignal implementation.\n5) Training loop import torch import torch.optim as optim device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") model = MNISTNet().to(device) optimizer = optim.Adam(model.parameters(), lr=0.001) loss_fn = nn.CrossEntropyLoss() for epoch in range(5): model.train() for X, y in train_loader: X, y = X.to(device), y.to(device) optimizer.zero_grad() pred = model(X) loss = loss_fn(pred, y) loss.backward() optimizer.step() print(f\"Epoch {epoch+1} complete\") If you are training locally on a m-series mac you can flip the comment between the two device lines.\n6) Evaluate the model \u0026 saving it We can first save the model\ntorch.save(model.state_dict(), \"mnist_model.pth\") Then evaluate it\nmodel.eval() correct = 0 total = 0 with torch.no_grad(): for X, y in test_loader: X, y = X.to(device), y.to(device) pred = model(X) predicted = pred.argmax(dim=1) correct += (predicted == y).sum().item() total += y.size(0) print(f\"Test Accuracy: {100 * correct / total:.2f}%\") 7) Connecting to streamlit To connect to streamlit and create a nice UI, we need some more packages\npip install streamlit streamlit-drawable-canvas numpy opencv-python And now we can create a file for streamlit\nstreamlist_mnist.py:\nimport streamlit as st from streamlit_drawable_canvas import st_canvas import torch import numpy as np import pandas as pd import cv2 from model import MNISTNet st.title(\"Draw a Digit - MNIST Inference Demo\") device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") model = MNISTNet().to(device) model.load_state_dict(torch.load(\"mnist_model.pth\", map_location=device)) model.eval() # canvas for drawing canvas_result = st_canvas( fill_color=\"white\", stroke_width=15, stroke_color=\"black\", background_color=\"white\", height=280, width=280, drawing_mode=\"freedraw\", key=\"canvas\" ) # when the user draws something if canvas_result.image_data is not None: img = canvas_result.image_data[:, :, 0] # grab only one channel (since they all have same val) img = cv2.resize(img, (28, 28)) # resize to MNIST dims img = 255 - img # invert: black digit on white img = img / 255.0 # normalize to [0, 1] img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device) with torch.no_grad(): logits = model(img_tensor) probs = torch.nn.functional.softmax(logits, dim=1) pred = torch.argmax(probs, dim=1) st.write(f\"### Prediction: {pred.item()}\") probs_np = probs.cpu().numpy()[0] prob_df = pd.DataFrame({ \"Digit\": list(range(10)), \"Confidence\": probs_np }) st.write(\"### Confidence for each digit:\") prob_df = prob_df.sort_values(\"Confidence\", ascending=False) st.bar_chart(prob_df.set_index(\"Digit\")) else: st.info(\"Draw a digit above to see the prediction.\") We can now view by running\nstreamlit run mnist_streamlit.py at http://localhost:8501.\nI was able to deploy my verison to streamlit’s cloud (streamlit community cloud). Here is the link: https://your-username.streamlit.app\nLet’s take a look at some more evals to make sure my model is fine\ninsert evals\nSeems like it is generalizing fine and we can move on to scaling to EMNIST.\nScaling to EMNIST The process would look kind of similar to what we had above but a little bit more complicated because EMNIST has 10 (digits) + 26 (lowercase letters) + 26 (uppercase letters) = 62 classes compared to 10 from MNIST.\n1) Setting up environment (again) Make sure to do this in a different directory but modularity.\npython3 -m venv .venv source .venv/bin/activate pip install torch torchvision 2) Download the dataset After some thinking, I decided to go with the balanced EMNIST dataset instead of the full. This way, I didn’t have to deal with issues with the model getting confused between an uppercase O and a 0.\nfrom torchvision import datasets, transforms # preprocessing (same as MNIST) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) train_data = datasets.EMNIST( root='./data', split='balanced', train=True, download=True, transform=transform ) test_data = datasets.EMNIST( root='./data', split='balanced', train=False, download=True, transform=transform ) The split='balanced' line was the one where I set that.\n3) Class Mapping Since our balanced EMNIST has 47 classes, the labels are integers (0-46). We would need to map them from label -\u003e character/digit if we wanted to get meaning out of that. When downloading this from, torchvision also downloads a .mapping file but here’s also a quick way to see it:\nlabel_map = [ '0','1','2','3','4','5','6','7','8','9', 'A','B','C','D','E','F','G','H','I','J', 'K','L','M','N','O','P','Q','R','S','T', 'U','V','W','X','Y','Z', 'a','b','d','e','f','g','h','n','q','r','t' ] 4) Building the model import torch.nn as nn import torch.nn.functional as F class EMNIST_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 3, padding=1) # 28x28 → 28x28 self.conv2 = nn.Conv2d(16, 32, 3, padding=1) # 28x28 → 28x28 self.pool = nn.MaxPool2d(2, 2) # 28x28 → 14x14 self.fc1 = nn.Linear(32 * 14 * 14, 128) self.fc2 = nn.Linear(128, 47) # for EMNIST balanced def forward(self, x): x = self.pool(F.relu(self.conv1(x))) # conv1 + relu + pool x = self.pool(F.relu(self.conv2(x))) # conv2 + relu + pool x = x.view(-1, 32 * 7 * 7) # flatten x = F.relu(self.fc1(x)) return self.fc2(x) I went with a CNN here because we need more fine-grained detail to handle character variability. We are now dealing with curvy letters and characters that look super similar which would need spatial information (how features are arranged in space).\nThis CNN model is just slightly larger than our previous MNIST MLP and is still quantizable while giving us a much higher accuracy on EMNIST.\n5) Training Loop import torch from torch.utils.data import DataLoader device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\") model = EMNIST_CNN().to(device) train_loader = DataLoader(train_data, batch_size=64, shuffle=True) test_loader = DataLoader(test_data, batch_size=1000, shuffle=False) optimizer = torch.optim.Adam(model.parameters(), lr=0.001) loss_fn = nn.CrossEntropyLoss() for epoch in range(5): # I used 5 epochs here for speed but we can increase this model.train() running_loss = 0.0 for X, y in train_loader: X, y = X.to(device), y.to(device) optimizer.zero_grad() out = model(X) loss = loss_fn(out, y) loss.backward() optimizer.step() running_loss += loss.item() print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\") 6) Evaluation \u0026 saving the model model.eval() correct = 0 total = 0 with torch.no_grad(): for X, y in test_loader: X, y = X.to(device), y.to(device) out = model(X) preds = torch.argmax(out, dim=1) correct += (preds == y).sum().item() total += y.size(0) print(f\"Test Accuracy: {100 * correct / total:.2f}%\") and we can save this by doing the following\ntorch.save(model.state_dict(), \"emnist_cnn.pth\") 7) Streamlit deployment Once the repository is structured like this\nemnist_project/ ├── model.py # has EMNIST_CNN class ├── emnist_cnn.pth # trained weights ├── emnist_streamlit.py # Streamlit UI ├── requirements.txt └── ... we can do the following:\nemnist_streamlit.py\nimport streamlit as st from streamlit_drawable_canvas import st_canvas import torch import torch.nn.functional as F import numpy as np import cv2 import pandas as pd from model import EMNIST_CNN # ← use your own model file # we need to copy over our label map label_map = [ '0','1','2','3','4','5','6','7','8','9', 'A','B','C','D','E','F','G','H','I','J', 'K','L','M','N','O','P','Q','R','S','T', 'U','V','W','X','Y','Z', 'a','b','d','e','f','g','h','n','q','r','t' ] # load model st.title(\"✍️ EMNIST Character Classifier\") device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\") model = EMNIST_CNN().to(device) model.load_state_dict(torch.load(\"emnist_cnn.pth\", map_location=device)) model.eval() # set up drawing canvas canvas_result = st_canvas( fill_color=\"white\", stroke_width=15, stroke_color=\"black\", background_color=\"white\", height=280, width=280, drawing_mode=\"freedraw\", key=\"canvas\" ) # inference logic if canvas_result.image_data is not None: img = canvas_result.image_data[:, :, 0] img = cv2.resize(img, (28, 28)) img = 255 - img img = img / 255.0 img_tensor = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device) with torch.no_grad(): logits = model(img_tensor) probs = F.softmax(logits, dim=1) pred_idx = torch.argmax(probs, dim=1).item() pred_char = label_map[pred_idx] st.markdown(f\"### Prediction: **{pred_char}**\") prob_df = pd.DataFrame({ \"Character\": label_map, \"Confidence\": probs.cpu().numpy()[0] }).sort_values(\"Confidence\", ascending=False) st.write(\"### Top Predictions\") st.dataframe(prob_df.head(10).reset_index(drop=True)) st.write(\"### Full Confidence Distribution\") st.bar_chart(prob_df.set_index(\"Character\")) else: st.info(\"Draw a character above to see the model's prediction.\") Compressing the model There are two main ways to quantize models Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). The first is done after training, and given our .pth file we can either run a calibration dataset to estimate activation ranges adn then quantize weights or we can quantize weights ahead of time, but activations are qunatized on-the-fly during inference. Since our model is already trained, I’m going to move ahead with static quantization (best for images / CNNs). Dynamic quantization’s architecture favores NLP models like LSTMs or transformers.\nLet’s get to quantizing this:\nLoad in the model import torch from model import EMNIST_CNN # your CNN model_fp32 = EMNIST_CNN() model_fp32.load_state_dict(torch.load(\"emnist_cnn.pth\")) model_fp32.eval() Fuse layers (for perfomance) model_fp32.fuse_model() Prepare for quantization import torch.quantization model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm') model_prepared = torch.quantization.prepare(model_fp32) Running a few batches in eval mode (checking validity) from torchvision import datasets, transforms from torch.utils.data import DataLoader transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ]) calibration_dataset = datasets.EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform) calibration_loader = DataLoader(calibration_dataset, batch_size=32, shuffle=True) with torch.no_grad(): for i, (x, _) in enumerate(calibration_loader): model_prepared(x) if i \u003e 10: # 10–20 batches is enough break Converting to quantized version model_int8 = torch.quantization.convert(model_prepared) Save the quantized model torch.save(model_int8.state_dict(), \"emnist_cnn_quantized.pth\") Final model structure import torch.nn as nn import torch.nn.functional as F import torch class EMNIST_CNN(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(1, 16, 3, padding=1) self.relu1 = nn.ReLU(inplace=True) # separate relus for fusing self.conv2 = nn.Conv2d(16, 32, 3, padding=1) self.relu2 = nn.ReLU(inplace=True) self.pool = nn.MaxPool2d(2, 2) self.fc1 = nn.Linear(32 * 7 * 7, 128) self.relu3 = nn.ReLU(inplace=True) self.fc2 = nn.Linear(128, 47) def forward(self, x): x = self.pool(self.relu1(self.conv1(x))) x = self.pool(self.relu2(self.conv2(x))) x = x.view(-1, 32 * 7 * 7) x = self.relu3(self.fc1(x)) return self.fc2(x) def fuse_model(self): torch.quantization.fuse_modules(self, [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3']], inplace=True) Model Type Size (MB) Accuracy EMNIST CNN (float) 1.8 MB 89.4% EMNIST CNN (INT8) 0.55 MB 88.3% Containerization Okay let’s get to the part we’ve been waiting for. Let’s try to spin up a docker container with the same specs as an Amazon Kindle from 2012. I found the specs online:\nComponent Spec Release Year 2012 CPU 800 MHz ARM Cortex-A8 RAM 256 MB Storage 2 GB internal flash Display 6\" eInk (1024 × 758) with built-in frontlight OS Linux-based (custom Kindle OS) Battery ~1400 mAh (weeks of battery life) Connectivity Wi-Fi (some versions had 3G) GPU None (no acceleration, just framebuffer) USB Micro USB 2.0 Since I obviously can’t install PyTorch or run code directly on a Kindle (or even get my hands on a 2012 kindle), I simulated its hardware constraints using Docker:\n256MB RAM ~0.3 CPUs No GPU (obviously) We can run\ndocker run -it --cpus=\"0.3\" --memory=\"256m\" python:3.10 bash to spin up our container with the correct size. Then we can do\napt update \u0026\u0026 apt install -y git git clone https://github.com/akhilvreddy/emnist-on-a-potato cd emnist-on-a-potato pip install -r requirements.txt and we technically have a 2012 kindle running in our terminal right now.\nTo actually run the model we would have to then run\npython run_kindle.py We didn’t define run_kindle.py yet. It would be a loop where we evaluate the model against a hidden set and then return the eval metrics after that. We can use the /hidden_set data to run this and then check the evals on this. We could also just run this on a set that we know the evals for and just make sure its giving us back the right information. Either way, we just want to try to run the model in eval mode.\nThis brings us to our last (and biggest) problem: KindleOS can’t run PyTorch at all.\nPyTorch relies on modern Linux kernel support and glibc compatibility, neither of which are present on KindleOS. That’s expected — the Kindle’s operating system is a stripped-down Linux variant optimized for ultra-low-power tasks like e-ink rendering and page flipping, not running deep learning frameworks.\nHere are our options:\nOption 1 - Wipe the OS and flash a new one We can wipe the Kindle’s OS and flash it with a lightweight Linux distro like Debian (similar to our Docker setup). This would give us full control and compatibility with PyTorch, but would brick its default functionality and require jailbreaking.\nOption 2 - Convert our .pth to a .onnx / .tflite We can convert the trained PyTorch model into a portable format like ONNX or TFLite, then run it using a minimal C++ inference runtime, sidestepping the need for PyTorch entirely. This approach lets us keep KindleOS intact but KindleOS wasn’t designed to run arbitrary binaries or heavy numerical code. We would have to reverse engineer parts of the system.\nSpecifically, we would need to:\nIdentify the libc version and whether dynamic linking is supported Confirm access to basic syscalls like mmap, mprotect, and fork Understand the CPU instruction set \u0026 floating point support for ARMv6 with limited math acceleration Write custom replacements for math operations (softmax and matmul) This would be like creating a model inference pipeline through a system designed for flipping pages, not matmuls. Not ideal, but doable with a lot of time and effort (but out of the scope of what I am trying to simulate here).\nFor this blog, I’m going to go with Option 1 for simplicity and because I don’t want to dive into system internals. With this, we don’t have to make any changes to our setup - the current docker container we have works fine. If we were acutally using a kindle, it would be bricked because of the jailbreak and OS flash.\nrun_kindle.py\nimport torch # Step 1: Define your model architecture (must match the saved model) class MyModel(torch.nn.Module): def __init__(self): super(MyModel, self).__init__() # define layers here (same as original model) self.linear = torch.nn.Linear(784, 10) # Example for MNIST-like def forward(self, x): return self.linear(x) # Step 2: Load the model model = MyModel() model.load_state_dict(torch.load('emnist_cnn_quantized.pth', map_location='cpu')) model.eval() # Step 3: Loop through the hidden_set and print predicted labels # Assuming hidden_set is a list or torch.utils.data.DataLoader of tensors for i, x in enumerate(hidden_set): with torch.no_grad(): if isinstance(x, tuple): # (x, _) if label is also present x = x[0] output = model(x.unsqueeze(0)) # Add batch dim if needed predicted_label = torch.argmax(output, dim=1).item() print(f\"Sample {i}: Predicted label = {predicted_label}\") Here is what we got in return:\nsiyuhhh I’m now confident to say this:\nOn a jailbroken Amazon Kindle flashed with a modern, lightweight Linux OS, it can run a EMNIST recognition model although the inference time is extremeley slow.\nConclusion This blog post wasn’t about Kindles or hardware from 2012 - it was about thinking at extremes. If I could get a MNSIT recognizer to almost run on a e-reader, deploying models to phones, Raspberry PIs, and embedded IoT chips doesn’t feel that bad anymore.\n",
  "wordCount" : "2684",
  "inLanguage": "en",
  "datePublished": "2025-04-10T18:12:56-04:00",
  "dateModified": "2025-04-10T18:12:56-04:00",
  "author":{
    "@type": "Person",
    "name": "ML Systems + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/quantized-emnist/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Can a EMNIST model run on an Amazon Kindle from 2012?
    </h1>
    <div class="post-meta"><span title='2025-04-10 18:12:56 -0400 EDT'>April 10, 2025</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;ML Systems &#43; Code

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: I trained a character recognition model on EMNIST, quantized it, and tried running it on hardware with less RAM than a Chrome tab. Here’s how I simulated its specs in Docker, built a tiny PyTorch inference loop, and learned why deploying to the real Kindle OS is basically impossible (but very fun).</p>
<p>The repsitory with the corresponding PyTorch implementation is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<p>Lately, I’ve been trying to level up from just training PyTorch models and evaluating them locally to actually wrapping them as APIs and deploying them. That naturally had me thinking: how far can you actually push a model? Not just in terms of scalable deployment, but portability. I’ve been curious about edge deployment, and I know modern iPhones have neural chips that make running on-device models easy. But what about the opposite end?</p>
<p>I wanted to run an experiment: could a model trained on EMNIST (extended MNIST, a dataset from 2017), actually run on a Kindle? In this post, I’ll walk through how I trained and quantized the model, set up an interactive UI, and simulated decade-old hardware using Docker, all to answer a simple but oddly satisfying question:</p>
<blockquote>
<p>Can a fossilized Amazon Kindle recognize handwritten letters and numbers?</p></blockquote>
<hr>
<p>Before I went into EMNIST I wanted to start simple and make sure that I was training properly and my visualizations looked fine.</p>
<h2 id="training-a-simple-mnist">Training a simple MNIST<a hidden class="anchor" aria-hidden="true" href="#training-a-simple-mnist">#</a></h2>
<p>Since I wanted to do everything from scratch in this project, I wanted to start with training MNIST just as a practice before EMNIST and also to visualize it with streamlit.</p>
<p>This is pretty simple and you&rsquo;ve probably seen this many times:</p>
<h3 id="1-setting-up-environment">1) Setting up environment<a hidden class="anchor" aria-hidden="true" href="#1-setting-up-environment">#</a></h3>
<p>We can either do this in a docker container or a new <code>.venv</code> environment. I&rsquo;m going to do the latter for simplicity (we&rsquo;re going to have to use docker later anyways so I took the easy route here).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3 -m venv .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span><span style="display:flex;"><span>pip install torch torchvision matplotlib
</span></span></code></pre></div><h3 id="2-download-the-dataset">2) Download the dataset<a hidden class="anchor" aria-hidden="true" href="#2-download-the-dataset">#</a></h3>
<p>I love torchvision because it makes this super simple.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.1307</span>,), (<span style="color:#ae81ff">0.3081</span>,))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>MNIST(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span><span style="display:flex;"><span>test_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>MNIST(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span></code></pre></div><h3 id="3-create-dataloaders">3) Create dataloaders<a hidden class="anchor" aria-hidden="true" href="#3-create-dataloaders">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(train_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>test_loader <span style="color:#f92672">=</span> DataLoader(test_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span></code></pre></div><h3 id="4-building-the-model">4) Building the model<a hidden class="anchor" aria-hidden="true" href="#4-building-the-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MNISTNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc2(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>out(x)
</span></span></code></pre></div><p>The hidden layer counts come from the orignal implementation.</p>
<h3 id="5-training-loop">5) Training loop<a hidden class="anchor" aria-hidden="true" href="#5-training-loop">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e"># device = torch.device(&#34;mps&#34; if torch.backends.mps.is_available() else &#34;cpu&#34;)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> MNISTNet()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)
</span></span><span style="display:flex;"><span>loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> train_loader:
</span></span><span style="display:flex;"><span>        X, y <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        pred <span style="color:#f92672">=</span> model(X)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(pred, y)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> complete&#34;</span>)
</span></span></code></pre></div><p>If you are training locally on a m-series mac you can flip the comment between the two <code>device</code> lines.</p>
<h3 id="6-evaluate-the-model--saving-it">6) Evaluate the model &amp; saving it<a hidden class="anchor" aria-hidden="true" href="#6-evaluate-the-model--saving-it">#</a></h3>
<p>We can first save the model</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>save(model<span style="color:#f92672">.</span>state_dict(), <span style="color:#e6db74">&#34;mnist_model.pth&#34;</span>)
</span></span></code></pre></div><p>Then evaluate it</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>total <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> test_loader:
</span></span><span style="display:flex;"><span>        X, y <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        pred <span style="color:#f92672">=</span> model(X)
</span></span><span style="display:flex;"><span>        predicted <span style="color:#f92672">=</span> pred<span style="color:#f92672">.</span>argmax(dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">+=</span> (predicted <span style="color:#f92672">==</span> y)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        total <span style="color:#f92672">+=</span> y<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Test Accuracy: </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> correct <span style="color:#f92672">/</span> total<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>)
</span></span></code></pre></div><h3 id="7-connecting-to-streamlit">7) Connecting to streamlit<a hidden class="anchor" aria-hidden="true" href="#7-connecting-to-streamlit">#</a></h3>
<p>To connect to streamlit and create a nice UI, we need some more packages</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install streamlit streamlit-drawable-canvas numpy opencv-python
</span></span></code></pre></div><p>And now we can create a file for streamlit</p>
<p>streamlist_mnist.py:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> streamlit <span style="color:#66d9ef">as</span> st
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> streamlit_drawable_canvas <span style="color:#f92672">import</span> st_canvas
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> model <span style="color:#f92672">import</span> MNISTNet
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>st<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Draw a Digit - MNIST Inference Demo&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;mps&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> MNISTNet()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;mnist_model.pth&#34;</span>, map_location<span style="color:#f92672">=</span>device))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># canvas for drawing</span>
</span></span><span style="display:flex;"><span>canvas_result <span style="color:#f92672">=</span> st_canvas(
</span></span><span style="display:flex;"><span>    fill_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    stroke_width<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>,
</span></span><span style="display:flex;"><span>    stroke_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;black&#34;</span>,
</span></span><span style="display:flex;"><span>    background_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    height<span style="color:#f92672">=</span><span style="color:#ae81ff">280</span>,
</span></span><span style="display:flex;"><span>    width<span style="color:#f92672">=</span><span style="color:#ae81ff">280</span>,
</span></span><span style="display:flex;"><span>    drawing_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;freedraw&#34;</span>,
</span></span><span style="display:flex;"><span>    key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;canvas&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># when the user draws something</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> canvas_result<span style="color:#f92672">.</span>image_data <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> canvas_result<span style="color:#f92672">.</span>image_data[:, :, <span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># grab only one channel (since they all have same val)</span>
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>resize(img, (<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>))          <span style="color:#75715e"># resize to MNIST dims</span>
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">-</span> img                          <span style="color:#75715e"># invert: black digit on white</span>
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> img <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>                        <span style="color:#75715e"># normalize to [0, 1]</span>
</span></span><span style="display:flex;"><span>    img_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(img, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> model(img_tensor)
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        pred <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(probs, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>write(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;### Prediction: </span><span style="color:#e6db74">{</span>pred<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    probs_np <span style="color:#f92672">=</span> probs<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    prob_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Digit&#34;</span>: list(range(<span style="color:#ae81ff">10</span>)),
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Confidence&#34;</span>: probs_np
</span></span><span style="display:flex;"><span>    })
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;### Confidence for each digit:&#34;</span>)
</span></span><span style="display:flex;"><span>    prob_df <span style="color:#f92672">=</span> prob_df<span style="color:#f92672">.</span>sort_values(<span style="color:#e6db74">&#34;Confidence&#34;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>bar_chart(prob_df<span style="color:#f92672">.</span>set_index(<span style="color:#e6db74">&#34;Digit&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#34;Draw a digit above to see the prediction.&#34;</span>)
</span></span></code></pre></div><p>We can now view by running</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>streamlit run mnist_streamlit.py
</span></span></code></pre></div><p>at <code>http://localhost:8501</code>.</p>
<p>I was able to deploy my verison to streamlit&rsquo;s cloud (streamlit community cloud). Here is the link: <a href="https://your-username.streamlit.app">https://your-username.streamlit.app</a></p>
<p>Let&rsquo;s take a look at some more evals to make sure my model is fine</p>
<blockquote>
<p>insert evals</p></blockquote>
<p>Seems like it is generalizing fine and we can move on to scaling to EMNIST.</p>
<h2 id="scaling-to-emnist">Scaling to EMNIST<a hidden class="anchor" aria-hidden="true" href="#scaling-to-emnist">#</a></h2>
<p>The process would look kind of similar to what we had above but a little bit more complicated because EMNIST has 10 (digits) + 26 (lowercase letters) + 26 (uppercase letters) = 62 classes compared to 10 from MNIST.</p>
<h3 id="1-setting-up-environment-again">1) Setting up environment (again)<a hidden class="anchor" aria-hidden="true" href="#1-setting-up-environment-again">#</a></h3>
<p>Make sure to do this in a different directory but modularity.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python3 -m venv .venv
</span></span><span style="display:flex;"><span>source .venv/bin/activate
</span></span><span style="display:flex;"><span>pip install torch torchvision
</span></span></code></pre></div><h3 id="2-download-the-dataset-1">2) Download the dataset<a hidden class="anchor" aria-hidden="true" href="#2-download-the-dataset-1">#</a></h3>
<p>After some thinking, I decided to go with the <em>balanced EMNIST</em> dataset instead of the full. This way, I didn&rsquo;t have to deal with issues with the model getting confused between an uppercase <code>O</code> and a <code>0</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># preprocessing (same as MNIST)</span>
</span></span><span style="display:flex;"><span>transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.1307</span>,), (<span style="color:#ae81ff">0.3081</span>,))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_data <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>EMNIST(
</span></span><span style="display:flex;"><span>    root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>,
</span></span><span style="display:flex;"><span>    split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>,
</span></span><span style="display:flex;"><span>    train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    transform<span style="color:#f92672">=</span>transform
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>test_data <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>EMNIST(
</span></span><span style="display:flex;"><span>    root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>,
</span></span><span style="display:flex;"><span>    split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>,
</span></span><span style="display:flex;"><span>    train<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>    download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    transform<span style="color:#f92672">=</span>transform
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>The <code>split='balanced'</code> line was the one where I set that.</p>
<h3 id="3-class-mapping">3) Class Mapping<a hidden class="anchor" aria-hidden="true" href="#3-class-mapping">#</a></h3>
<p>Since our balanced EMNIST has 47 classes, the labels are integers (0-46). We would need to map them from label -&gt; character/digit if we wanted to get meaning out of that. When downloading this from, torchvision also downloads a <code>.mapping</code> file but here&rsquo;s also a quick way to see it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>label_map = [
</span></span><span style="display:flex;"><span>    &#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,
</span></span><span style="display:flex;"><span>    &#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;,&#39;F&#39;,&#39;G&#39;,&#39;H&#39;,&#39;I&#39;,&#39;J&#39;,
</span></span><span style="display:flex;"><span>    &#39;K&#39;,&#39;L&#39;,&#39;M&#39;,&#39;N&#39;,&#39;O&#39;,&#39;P&#39;,&#39;Q&#39;,&#39;R&#39;,&#39;S&#39;,&#39;T&#39;,
</span></span><span style="display:flex;"><span>    &#39;U&#39;,&#39;V&#39;,&#39;W&#39;,&#39;X&#39;,&#39;Y&#39;,&#39;Z&#39;,
</span></span><span style="display:flex;"><span>    &#39;a&#39;,&#39;b&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;n&#39;,&#39;q&#39;,&#39;r&#39;,&#39;t&#39;
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><h3 id="4-building-the-model-1">4) Building the model<a hidden class="anchor" aria-hidden="true" href="#4-building-the-model-1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EMNIST_CNN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 28x28 → 28x28</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># 28x28 → 28x28</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># 28x28 → 14x14</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">14</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">14</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">47</span>) <span style="color:#75715e"># for EMNIST balanced</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x))) <span style="color:#75715e"># conv1 + relu + pool</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x))) <span style="color:#75715e"># conv2 + relu + pool</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span>) <span style="color:#75715e"># flatten</span>
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span></code></pre></div><p>I went with a CNN here because we need more fine-grained detail to handle character variability. We are now dealing with curvy letters and characters that look super similar which would need spatial information (how features are arranged in space).</p>
<p>This CNN model is just slightly larger than our previous MNIST MLP and is still quantizable while giving us a much higher accuracy on EMNIST.</p>
<h3 id="5-training-loop-1">5) Training Loop<a hidden class="anchor" aria-hidden="true" href="#5-training-loop-1">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;mps&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> EMNIST_CNN()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(train_data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>test_loader  <span style="color:#f92672">=</span> DataLoader(test_data, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)
</span></span><span style="display:flex;"><span>loss_fn <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">5</span>): <span style="color:#75715e"># I used 5 epochs here for speed but we can increase this</span>
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    running_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> train_loader:
</span></span><span style="display:flex;"><span>        X, y <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> model(X)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> loss_fn(out, y)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        running_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>running_loss <span style="color:#f92672">/</span> len(train_loader)<span style="color:#e6db74">:</span><span style="color:#e6db74">.4f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="6-evaluation--saving-the-model">6) Evaluation &amp; saving the model<a hidden class="anchor" aria-hidden="true" href="#6-evaluation--saving-the-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>correct <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>total <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> X, y <span style="color:#f92672">in</span> test_loader:
</span></span><span style="display:flex;"><span>        X, y <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>to(device), y<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> model(X)
</span></span><span style="display:flex;"><span>        preds <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(out, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        correct <span style="color:#f92672">+=</span> (preds <span style="color:#f92672">==</span> y)<span style="color:#f92672">.</span>sum()<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        total <span style="color:#f92672">+=</span> y<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Test Accuracy: </span><span style="color:#e6db74">{</span><span style="color:#ae81ff">100</span> <span style="color:#f92672">*</span> correct <span style="color:#f92672">/</span> total<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">%&#34;</span>)
</span></span></code></pre></div><p>and we can save this by doing the following</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>save(model<span style="color:#f92672">.</span>state_dict(), <span style="color:#e6db74">&#34;emnist_cnn.pth&#34;</span>)
</span></span></code></pre></div><h3 id="7-streamlit-deployment">7) Streamlit deployment<a hidden class="anchor" aria-hidden="true" href="#7-streamlit-deployment">#</a></h3>
<p>Once the repository is structured like this</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>emnist_project/
</span></span><span style="display:flex;"><span>├── model.py                # has EMNIST_CNN class
</span></span><span style="display:flex;"><span>├── emnist_cnn.pth          # trained weights
</span></span><span style="display:flex;"><span>├── emnist_streamlit.py     # Streamlit UI
</span></span><span style="display:flex;"><span>├── requirements.txt
</span></span><span style="display:flex;"><span>└── ...
</span></span></code></pre></div><p>we can do the following:</p>
<p>emnist_streamlit.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> streamlit <span style="color:#66d9ef">as</span> st
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> streamlit_drawable_canvas <span style="color:#f92672">import</span> st_canvas
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> model <span style="color:#f92672">import</span> EMNIST_CNN  <span style="color:#75715e"># ← use your own model file</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># we need to copy over our label map</span>
</span></span><span style="display:flex;"><span>label_map <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;0&#39;</span>,<span style="color:#e6db74">&#39;1&#39;</span>,<span style="color:#e6db74">&#39;2&#39;</span>,<span style="color:#e6db74">&#39;3&#39;</span>,<span style="color:#e6db74">&#39;4&#39;</span>,<span style="color:#e6db74">&#39;5&#39;</span>,<span style="color:#e6db74">&#39;6&#39;</span>,<span style="color:#e6db74">&#39;7&#39;</span>,<span style="color:#e6db74">&#39;8&#39;</span>,<span style="color:#e6db74">&#39;9&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;A&#39;</span>,<span style="color:#e6db74">&#39;B&#39;</span>,<span style="color:#e6db74">&#39;C&#39;</span>,<span style="color:#e6db74">&#39;D&#39;</span>,<span style="color:#e6db74">&#39;E&#39;</span>,<span style="color:#e6db74">&#39;F&#39;</span>,<span style="color:#e6db74">&#39;G&#39;</span>,<span style="color:#e6db74">&#39;H&#39;</span>,<span style="color:#e6db74">&#39;I&#39;</span>,<span style="color:#e6db74">&#39;J&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;K&#39;</span>,<span style="color:#e6db74">&#39;L&#39;</span>,<span style="color:#e6db74">&#39;M&#39;</span>,<span style="color:#e6db74">&#39;N&#39;</span>,<span style="color:#e6db74">&#39;O&#39;</span>,<span style="color:#e6db74">&#39;P&#39;</span>,<span style="color:#e6db74">&#39;Q&#39;</span>,<span style="color:#e6db74">&#39;R&#39;</span>,<span style="color:#e6db74">&#39;S&#39;</span>,<span style="color:#e6db74">&#39;T&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;U&#39;</span>,<span style="color:#e6db74">&#39;V&#39;</span>,<span style="color:#e6db74">&#39;W&#39;</span>,<span style="color:#e6db74">&#39;X&#39;</span>,<span style="color:#e6db74">&#39;Y&#39;</span>,<span style="color:#e6db74">&#39;Z&#39;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;a&#39;</span>,<span style="color:#e6db74">&#39;b&#39;</span>,<span style="color:#e6db74">&#39;d&#39;</span>,<span style="color:#e6db74">&#39;e&#39;</span>,<span style="color:#e6db74">&#39;f&#39;</span>,<span style="color:#e6db74">&#39;g&#39;</span>,<span style="color:#e6db74">&#39;h&#39;</span>,<span style="color:#e6db74">&#39;n&#39;</span>,<span style="color:#e6db74">&#39;q&#39;</span>,<span style="color:#e6db74">&#39;r&#39;</span>,<span style="color:#e6db74">&#39;t&#39;</span>
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># load model</span>
</span></span><span style="display:flex;"><span>st<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;✍️ EMNIST Character Classifier&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;mps&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>backends<span style="color:#f92672">.</span>mps<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> EMNIST_CNN()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;emnist_cnn.pth&#34;</span>, map_location<span style="color:#f92672">=</span>device))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># set up drawing canvas</span>
</span></span><span style="display:flex;"><span>canvas_result <span style="color:#f92672">=</span> st_canvas(
</span></span><span style="display:flex;"><span>    fill_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    stroke_width<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>,
</span></span><span style="display:flex;"><span>    stroke_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;black&#34;</span>,
</span></span><span style="display:flex;"><span>    background_color<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>,
</span></span><span style="display:flex;"><span>    height<span style="color:#f92672">=</span><span style="color:#ae81ff">280</span>,
</span></span><span style="display:flex;"><span>    width<span style="color:#f92672">=</span><span style="color:#ae81ff">280</span>,
</span></span><span style="display:flex;"><span>    drawing_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;freedraw&#34;</span>,
</span></span><span style="display:flex;"><span>    key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;canvas&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># inference logic</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> canvas_result<span style="color:#f92672">.</span>image_data <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> canvas_result<span style="color:#f92672">.</span>image_data[:, :, <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> cv2<span style="color:#f92672">.</span>resize(img, (<span style="color:#ae81ff">28</span>, <span style="color:#ae81ff">28</span>))
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> <span style="color:#ae81ff">255</span> <span style="color:#f92672">-</span> img
</span></span><span style="display:flex;"><span>    img <span style="color:#f92672">=</span> img <span style="color:#f92672">/</span> <span style="color:#ae81ff">255.0</span>
</span></span><span style="display:flex;"><span>    img_tensor <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(img, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> model(img_tensor)
</span></span><span style="display:flex;"><span>        probs <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(logits, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        pred_idx <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(probs, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        pred_char <span style="color:#f92672">=</span> label_map[pred_idx]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>markdown(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;### Prediction: **</span><span style="color:#e6db74">{</span>pred_char<span style="color:#e6db74">}</span><span style="color:#e6db74">**&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    prob_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Character&#34;</span>: label_map,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Confidence&#34;</span>: probs<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    })<span style="color:#f92672">.</span>sort_values(<span style="color:#e6db74">&#34;Confidence&#34;</span>, ascending<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;### Top Predictions&#34;</span>)
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>dataframe(prob_df<span style="color:#f92672">.</span>head(<span style="color:#ae81ff">10</span>)<span style="color:#f92672">.</span>reset_index(drop<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>write(<span style="color:#e6db74">&#34;### Full Confidence Distribution&#34;</span>)
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>bar_chart(prob_df<span style="color:#f92672">.</span>set_index(<span style="color:#e6db74">&#34;Character&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    st<span style="color:#f92672">.</span>info(<span style="color:#e6db74">&#34;Draw a character above to see the model&#39;s prediction.&#34;</span>)
</span></span></code></pre></div><hr>
<h2 id="compressing-the-model">Compressing the model<a hidden class="anchor" aria-hidden="true" href="#compressing-the-model">#</a></h2>
<p>There are two main ways to quantize models <strong>Post-Training Quantization (PTQ)</strong> and <strong>Quantization-Aware Training (QAT)</strong>. The first is done after training, and given our <code>.pth</code> file we can either run a calibration dataset to estimate activation ranges adn then quantize weights or we can quantize weights ahead of time, but activations are qunatized on-the-fly during inference. Since our model is already trained, I&rsquo;m going to move ahead with static quantization (best for images / CNNs). Dynamic quantization&rsquo;s architecture favores NLP models like LSTMs or transformers.</p>
<p>Let&rsquo;s get to quantizing this:</p>
<h3 id="load-in-the-model">Load in the model<a hidden class="anchor" aria-hidden="true" href="#load-in-the-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> model <span style="color:#f92672">import</span> EMNIST_CNN  <span style="color:#75715e"># your CNN</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_fp32 <span style="color:#f92672">=</span> EMNIST_CNN()
</span></span><span style="display:flex;"><span>model_fp32<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;emnist_cnn.pth&#34;</span>))
</span></span><span style="display:flex;"><span>model_fp32<span style="color:#f92672">.</span>eval()
</span></span></code></pre></div><h3 id="fuse-layers-for-perfomance">Fuse layers (for perfomance)<a hidden class="anchor" aria-hidden="true" href="#fuse-layers-for-perfomance">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_fp32<span style="color:#f92672">.</span>fuse_model()
</span></span></code></pre></div><h3 id="prepare-for-quantization">Prepare for quantization<a hidden class="anchor" aria-hidden="true" href="#prepare-for-quantization">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.quantization
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model_fp32<span style="color:#f92672">.</span>qconfig <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>get_default_qconfig(<span style="color:#e6db74">&#39;fbgemm&#39;</span>)
</span></span><span style="display:flex;"><span>model_prepared <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>prepare(model_fp32)
</span></span></code></pre></div><h3 id="running-a-few-batches-in-eval-mode-checking-validity">Running a few batches in eval mode (checking validity)<a hidden class="anchor" aria-hidden="true" href="#running-a-few-batches-in-eval-mode-checking-validity">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> datasets, transforms
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>transform <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose([
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>ToTensor(),
</span></span><span style="display:flex;"><span>    transforms<span style="color:#f92672">.</span>Normalize((<span style="color:#ae81ff">0.1307</span>,), (<span style="color:#ae81ff">0.3081</span>,))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>calibration_dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>EMNIST(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;./data&#39;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;balanced&#39;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span><span style="display:flex;"><span>calibration_loader <span style="color:#f92672">=</span> DataLoader(calibration_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, (x, _) <span style="color:#f92672">in</span> enumerate(calibration_loader):
</span></span><span style="display:flex;"><span>        model_prepared(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">10</span>:  <span style="color:#75715e"># 10–20 batches is enough</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span></code></pre></div><h3 id="converting-to-quantized-version">Converting to quantized version<a hidden class="anchor" aria-hidden="true" href="#converting-to-quantized-version">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model_int8 <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>convert(model_prepared)
</span></span></code></pre></div><h3 id="save-the-quantized-model">Save the quantized model<a hidden class="anchor" aria-hidden="true" href="#save-the-quantized-model">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>torch<span style="color:#f92672">.</span>save(model_int8<span style="color:#f92672">.</span>state_dict(), <span style="color:#e6db74">&#34;emnist_cnn_quantized.pth&#34;</span>)
</span></span></code></pre></div><h3 id="final-model-structure">Final model structure<a hidden class="anchor" aria-hidden="true" href="#final-model-structure">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EMNIST_CNN</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  <span style="color:#75715e"># separate relus for fusing</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>pool <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">128</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>relu3 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">47</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(self<span style="color:#f92672">.</span>relu1(self<span style="color:#f92672">.</span>conv1(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pool(self<span style="color:#f92672">.</span>relu2(self<span style="color:#f92672">.</span>conv2(x)))
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">32</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">7</span>)
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu3(self<span style="color:#f92672">.</span>fc1(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>fc2(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fuse_model</span>(self):
</span></span><span style="display:flex;"><span>        torch<span style="color:#f92672">.</span>quantization<span style="color:#f92672">.</span>fuse_modules(self, [[<span style="color:#e6db74">&#39;conv1&#39;</span>, <span style="color:#e6db74">&#39;relu1&#39;</span>], [<span style="color:#e6db74">&#39;conv2&#39;</span>, <span style="color:#e6db74">&#39;relu2&#39;</span>], [<span style="color:#e6db74">&#39;fc1&#39;</span>, <span style="color:#e6db74">&#39;relu3&#39;</span>]], inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><table>
  <thead>
      <tr>
          <th>Model Type</th>
          <th>Size (MB)</th>
          <th>Accuracy</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>EMNIST CNN (float)</td>
          <td>1.8 MB</td>
          <td>89.4%</td>
      </tr>
      <tr>
          <td>EMNIST CNN (INT8)</td>
          <td>0.55 MB</td>
          <td>88.3%</td>
      </tr>
  </tbody>
</table>
<h3 id="containerization">Containerization<a hidden class="anchor" aria-hidden="true" href="#containerization">#</a></h3>
<p>Okay let&rsquo;s get to the part we&rsquo;ve been waiting for. Let&rsquo;s try to spin up a docker container with the same specs as an Amazon Kindle from 2012. I found the specs online:</p>
<table>
  <thead>
      <tr>
          <th>Component</th>
          <th>Spec</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>Release Year</strong></td>
          <td>2012</td>
      </tr>
      <tr>
          <td><strong>CPU</strong></td>
          <td>800 MHz ARM Cortex-A8</td>
      </tr>
      <tr>
          <td><strong>RAM</strong></td>
          <td>256 MB</td>
      </tr>
      <tr>
          <td><strong>Storage</strong></td>
          <td>2 GB internal flash</td>
      </tr>
      <tr>
          <td><strong>Display</strong></td>
          <td>6&quot; eInk (1024 × 758) with built-in frontlight</td>
      </tr>
      <tr>
          <td><strong>OS</strong></td>
          <td>Linux-based (custom Kindle OS)</td>
      </tr>
      <tr>
          <td><strong>Battery</strong></td>
          <td>~1400 mAh (weeks of battery life)</td>
      </tr>
      <tr>
          <td><strong>Connectivity</strong></td>
          <td>Wi-Fi (some versions had 3G)</td>
      </tr>
      <tr>
          <td><strong>GPU</strong></td>
          <td>None (no acceleration, just framebuffer)</td>
      </tr>
      <tr>
          <td><strong>USB</strong></td>
          <td>Micro USB 2.0</td>
      </tr>
  </tbody>
</table>
<p>Since I obviously can’t install PyTorch or run code directly on a Kindle (or even get my hands on a 2012 kindle), I simulated its hardware constraints using Docker:</p>
<ul>
<li>256MB RAM</li>
<li>~0.3 CPUs</li>
<li>No GPU (obviously)</li>
</ul>
<p>We can run</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -it --cpus<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;0.3&#34;</span> --memory<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;256m&#34;</span> python:3.10 bash
</span></span></code></pre></div><p>to spin up our container with the correct size. Then we can do</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>apt update <span style="color:#f92672">&amp;&amp;</span> apt install -y git
</span></span><span style="display:flex;"><span>git clone https://github.com/akhilvreddy/emnist-on-a-potato
</span></span><span style="display:flex;"><span>cd emnist-on-a-potato
</span></span><span style="display:flex;"><span>pip install -r requirements.txt
</span></span></code></pre></div><p>and we <em>technically</em> have a 2012 kindle running in our terminal right now.</p>
<p>To actually run the model we would have to then run</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python run_kindle.py
</span></span></code></pre></div><p>We didn&rsquo;t define <code>run_kindle.py</code> yet. It would be a loop where we evaluate the model against a hidden set and then return the eval metrics after that. We can use the <code>/hidden_set</code> data to run this and then check the evals on this. We could also just run this on a set that we know the evals for and just make sure its giving us back the right information. Either way, we just want to try to run the model in eval mode.</p>
<p>This brings us to our last (and biggest) problem: <strong>KindleOS can’t run PyTorch at all</strong>.</p>
<p>PyTorch relies on modern Linux kernel support and glibc compatibility, neither of which are present on KindleOS. That’s expected — the Kindle’s operating system is a stripped-down Linux variant optimized for ultra-low-power tasks like e-ink rendering and page flipping, not running deep learning frameworks.</p>
<p>Here are our options:</p>
<h4 id="option-1---wipe-the-os-and-flash-a-new-one">Option 1 - Wipe the OS and flash a new one<a hidden class="anchor" aria-hidden="true" href="#option-1---wipe-the-os-and-flash-a-new-one">#</a></h4>
<p>We can wipe the Kindle’s OS and flash it with a lightweight Linux distro like Debian (similar to our Docker setup). This would give us full control and compatibility with PyTorch, but would brick its default functionality and require jailbreaking.</p>
<h4 id="option-2---convert-our-pth-to-a-onnx--tflite">Option 2 - Convert our .pth to a .onnx / .tflite<a hidden class="anchor" aria-hidden="true" href="#option-2---convert-our-pth-to-a-onnx--tflite">#</a></h4>
<p>We can convert the trained PyTorch model into a portable format like ONNX or TFLite, then run it using a minimal C++ inference runtime, sidestepping the need for PyTorch entirely. This approach lets us keep KindleOS intact but KindleOS wasn&rsquo;t designed to run arbitrary binaries or heavy numerical code. We would have to reverse engineer parts of the system.</p>
<p>Specifically, we would need to:</p>
<ul>
<li>Identify the libc version and whether dynamic linking is supported</li>
<li>Confirm access to basic syscalls like <code>mmap</code>, <code>mprotect</code>, and <code>fork</code></li>
<li>Understand the CPU instruction set &amp; floating point support for ARMv6 with limited math acceleration</li>
<li>Write custom replacements for math operations (softmax and matmul)</li>
</ul>
<p>This would be like creating a model inference pipeline through a system designed for flipping pages, not matmuls. Not ideal, but doable with a lot of time and effort (but out of the scope of what I am trying to simulate here).</p>
<p>For this blog, I&rsquo;m going to go with Option 1 for simplicity and because I don&rsquo;t want to dive into system internals. With this, we don&rsquo;t have to make any changes to our setup - the current docker container we have works fine. If we were acutally using a kindle, it would be bricked because of the jailbreak and OS flash.</p>
<p>run_kindle.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 1: Define your model architecture (must match the saved model)</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MyModel</span>(torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self):
</span></span><span style="display:flex;"><span>        super(MyModel, self)<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># define layers here (same as original model)</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>linear <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">784</span>, <span style="color:#ae81ff">10</span>)  <span style="color:#75715e"># Example for MNIST-like</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linear(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 2: Load the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> MyModel()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>load_state_dict(torch<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;emnist_cnn_quantized.pth&#39;</span>, map_location<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cpu&#39;</span>))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Step 3: Loop through the hidden_set and print predicted labels</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Assuming hidden_set is a list or torch.utils.data.DataLoader of tensors</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, x <span style="color:#f92672">in</span> enumerate(hidden_set):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(x, tuple):  <span style="color:#75715e"># (x, _) if label is also present</span>
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> x[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(x<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>))  <span style="color:#75715e"># Add batch dim if needed</span>
</span></span><span style="display:flex;"><span>        predicted_label <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(output, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>item()
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Sample </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">: Predicted label = </span><span style="color:#e6db74">{</span>predicted_label<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Here is what we got in return:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>siyuhhh
</span></span></code></pre></div><p>I&rsquo;m now confident to say this:</p>
<blockquote>
<p>On a jailbroken Amazon Kindle flashed with a modern, lightweight Linux OS, it can run a EMNIST recognition model although the inference time is extremeley slow.</p></blockquote>
<hr>
<h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>This blog post wasn&rsquo;t about Kindles or hardware from 2012 - it was about thinking at extremes. If I could get a MNSIT recognizer to almost run on a e-reader, deploying models to phones, Raspberry PIs, and embedded IoT chips doesn&rsquo;t feel that bad anymore.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
