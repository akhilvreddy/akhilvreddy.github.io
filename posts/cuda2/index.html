<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="
TL;DR: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.
The repsitory with the corresponding PyTorch implementation is available here.">
<meta name="author" content="ML Theory &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/cuda2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/cuda2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/cuda2/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton">
  <meta property="og:description" content=" TL;DR: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.
The repsitory with the corresponding PyTorch implementation is available here.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-07-11T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-07-11T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton">
<meta name="twitter:description" content="
TL;DR: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.
The repsitory with the corresponding PyTorch implementation is available here.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "CUDA really isn't that bad: Tiling, Fusion, and Triton",
      "item": "https://akhilvreddy.github.io/posts/cuda2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "CUDA really isn't that bad: Tiling, Fusion, and Triton",
  "name": "CUDA really isn\u0027t that bad: Tiling, Fusion, and Triton",
  "description": " TL;DR: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.\nThe repsitory with the corresponding PyTorch implementation is available here.\n",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.\nThe repsitory with the corresponding PyTorch implementation is available here.\nTiling and Blocking in Matrix Multiplication Matrix multiplication is one of the most fundamental operations in deep learning. It powers everything from linear layers to conv nets and attention heads. So while writing @ or torch.matul() (or even torch.bmm()) feels like a one-liner, under the hood it’s a battlefield of memory access patterns, cache misses, and thread synchronization to get the matrix multiply done as quick as possible. To get this fast on GPUs, we use two key ideas: blocking and tiling.\nThe naive alogrithm for matmul runs in O(n³) and does\nfor (int i = 0; i \u003c N; i++) for (int j = 0; j \u003c N; j++) for (int k = 0; k \u003c N; k++) C[i][j] += A[i][k] * B[k][j]; This works on a CPU or GPU but on the GPU it’s slower than the CPU (which is exactly the opposite of what we want). This is because:\nWe’re constantly reading and writing from global memory We’re re-fetching the same values repeatedly across threads We’re not taking advantage of sharing memory, coalesced reads, or warp-level reuse We need to warp (pun intended) this algorithm such that our GPU can parallelize and speed it up and not call from global memory every call.\nBlocking: Divide the work The idea here is to divide the matrix into square blocks. Each block computes a submatrix of the output C.\nImagine if each block computes a BLOCK_SIZE x BLOCK_SIZE patch of C.\nEach thread in the block computes only a single element in that tile. So if the block size is 16, the block launches 256 threads (16x16) and each thread computes a single [i][j].\nEach thread block loads a tile of input, weight, and output data into fast shared memory, minimizing slow global memory access. Threads then collaboratively compute a submatrix of the output using those tiles, reusing data efficiently through blocking and tiling.\nTiling: Reuse data in shared memory Inside each block, we tile the k dimension so that\nA tile from matrix A (A[i][j]) is loaded into shared memory A tile from matrix B (B[i][j]) is also loaded into shared memory Given this, each thread now loads a chunk of A and B once (from global → shared). It then reuses it multiple time for the inner k loop and only writes back to global memory once (at the end).\nThis is what GPUs were designed for: get your computations done quick and read/write to memory only at the end of the computation.\nHere is what matmul looks like after the upgrade\n__global__ void matmul_shared(float* A, float* B, float* C, int N) { __shared__ float tileA[BLOCK_SIZE][BLOCK_SIZE]; __shared__ float tileB[BLOCK_SIZE][BLOCK_SIZE]; int row = blockIdx.y * BLOCK_SIZE + threadIdx.y; int col = blockIdx.x * BLOCK_SIZE + threadIdx.x; float sum = 0.0f; for (int k = 0; k \u003c N / BLOCK_SIZE; k++) { tileA[threadIdx.y][threadIdx.x] = A[row * N + k * BLOCK_SIZE + threadIdx.x]; tileB[threadIdx.y][threadIdx.x] = B[(k * BLOCK_SIZE + threadIdx.y) * N + col]; __syncthreads(); // all threads have to load their tiles for (int n = 0; n \u003c BLOCK_SIZE; n++) { sum += tileA[threadIdx.y][n] * tileB[n][threadIdx.x]; } __syncthreads(); // sync before loading next tile } C[row * N + col] = sum; } As we can see, this loads a tile of A and B just once per block. All the threads reuse the data and this reduces global memory bandwith usage dramatically.\nLet’s dive into an example so that this really clicks.\nHere are two 4x4 matrices, A and B.\nA = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10,11,12], [13,14,15,16] ] B = [ [17, 18, 19, 20], [21, 22, 23, 24], [25, 26, 27, 28], [29,30,31,32] ] we can split up those matrices into sub-matrices like the following\nThe equation to compute C00 is\nC00 = A00×B00 + A01×B10 we can split that up into two different computations.\nC00_partial_1 = A00 × B00 C00_partial_2 = A01 × B10 For C00_partial_1 we can look at the computation thread-wise.\nC00P1[0][0] = 1×17 + 2×21 = 17 + 42 = 59 C00P1[0][1] = 1×18 + 2×22 = 18 + 44 = 62 C00P1[1][0] = 5×17 + 6×21 = 85 + 126 = 211 C00P1[1][1] = 5×18 + 6×22 = 90 + 132 = 222 a similar thing would happen on the GPU for C00P2 as well.\nC00P2[0][0] = 3×25 + 4×29 = 75 + 116 = 191 C00P2[0][1] = 3×26 + 4×30 = 78 + 120 = 198 C00P2[1][0] = 7×25 + 8×29 = 175 + 232 = 407 C00P2[1][1] = 7×26 + 8×30 = 182 + 240 = 422 We can then add both of them together to get back C00.\nC00P1[0][0] + C00P2[0][0] = 59 + 191 = 250 C00P1[0][1] + C00P2[0][1] = 62 + 198 = 260 C00P1[1][0] + C00P2[1][0] = 211 + 407 = 618 C00P1[1][1] + C00P2[1][1] = 222 + 422 = 644 Here’s the magic. The GPU isn’t just computing C00. It’s computing all of C at once. We just walked through one quartile but the GPU is going to do C00, C01, C10, and C11 simultaneously.\nEach GPU block takes care of one submatrix of C, and each thread inside the block computes one individual element like C[i][j].\nThe last thing we should think about is memory usage. We have two parts to think about: shared memory and global memory and being smart about when we use each is vital to efficiency.\nA tile from A and a tile from B is loaded onto shared memory, once per block Individual threads in the block reuse this data to compute the full partial product (through register memory) Intermediate values (like C00_partial1) are held in registers or local memory, not written to global immediately After summing all the partials, we write the final result only once to global memory. As a recap, we read and write from global memory once (to get input and write output), (partial) tiles from A and B are stored in the current block, and intermeditate values (like sum) are stored in registers.\nOccupancy, Registers, and Thread Utilization In my opinion, this is a huge leap from what we were doing before. If previously (including last post) we were writing CUDA kernels, we move into performance engineering now.\nBut before we dive in, we need to define a key component in GPUs: streaming multiprocessor (SM). These are the basic building blocks of an NVIDIA GPU, they can be thought of as a “mini CPU cluster” inside the GPU. Every modern NVIDIA GPU contains dozens of SMs and each SM can run multiple warps concurrently.\nWhat is occupancy? Occupancy is the ratio active warps to maximum possible warps on a SM. High occupancy means that we have more warps ready to run meaning lower latency. However, if we have too high occupancy, it can hurt if it means each thread gets fewer registers or shared memory.\nEmpirically, more occupany helps hide memory latency but after ~70%, we hit diminishing returns.\nRegister Usage? Each SM has a limited nmber of registers. If your kernel uses too many registers per thread, fewer threads can be launched per block. This in turn reduces occupancy and may leave GPU cores underutilized.\nLuckily, CUDA’s compiler can quickly tell us register usage when compiling.\nnvcc --ptxas-options=-v We can even ask the compiler to limit registers by using specific flags like\n--maxrregcount=32 There’s a tradeoff here as well: limiting register usage might cause register spilling, where the compiler offloads variables from fast registers to local memory.\nDespite the name, local memory is actually stored in global memory, just scoped per thread. This means spilled variables suffer from global memory latency, which can silently wreck performance, especially inside tight loops or compute-heavy kernels.\nThread utilization? For maximum efficiency, each thread should be doing useful work. A common way that threads are wasted is when they run into\nif (i \u003e= N) return; An alternative to this is to use grid-stride loop when N might be bigger than your grid:\nfor (int i = threadIdx.x + blockIdx.x * blockDim.x; i \u003c N; i + blockDim.x * gridDim.x) { // do work on element i } Let’s understand exactly what’s going on here. Index i is set to threadIdx.x + blockIdx.x * blockDim.x which makes sense, this gives you the global thread index, uniquely identifying each thread across the grid. blockDim.x * gridDim.x is the total number of threads in the grid, which becomes the stride size (stride). So when we increment i by stride (i += stride) each thread jumps by the total thread count and does work again. This ensures all elements from 0 to N-1 get covered, even if there are more elements than threads.\nCUDA performance tuning is a balancing act: maximize thread usage and occupancy, but stay under register and shared memory limits or risk silent slowdowns.\nLoop Unrolling and Compiler Hints This is how you squeeze out that last 20-30% gains from GPU kernels.\nLoop Unrolling This is a performance optimization which can reduce loop overhead and increase instruction level parallelism on a GPU.\nInstead of\nfor (int i = 0; i \u003c 4; ++i) { sum += a[i] * b[i]; } the compiler (or the dev, manually) can unroll it to\nsum += a[0] * b[0]; sum += a[1] * b[1]; sum += a[2] * b[2]; sum += a[3] * b[3]; Even though the above is a simple loop and threads are launched independently, it still has to ocheck the loop conditions (i \u003c 4), increment i, jump back to the top of the loop, and possibly predict branches. This costs instruction cycles, especially when the loop is small and fixed-size.\nManual vs Automatic Unrolling Modern compilers like nvcc, clang, or gcc often auto-unroll small loops if the loop bounds are known at compile time.\nHowever, manual unrolling gives you strong control.\n#pragma unroll for (int i = 0; i \u003c 8; ++i) { sum += a[i] * b[i]; } would unroll your whole loop for you.\nWhen to unroll? You should unroll when:\nThe loop bound is known at compile time (like i \u003c 20) The logic inside the loop is simple and has no early exits You only care about maxing out throughput on critical compute On the flip side, don’t unroll when the logic inside the loop is huge or has control flow or if you are at risk of register pressure (too many variables per thread). Over-unrolling can lead to register spilling which turns into a silent performance crash.\nCUDA is all about balance. Unroll too little and leave performance on the table. Unroll too much and risk register spilling, silent crashes, or warp inefficiency. The real skill is finding that sweet spot in the middle.\nDouble Buffering and Pipelining Using these techniques, we make our kernels never wait for memory.\nIn a perfect CUDA kernel, computation and memory transfers happen at the same time. In most naive implementations, we either have to\nwait to load data into shared memory wait to compute after it’s been loaded Waiting for memory is wasted time. Double bluffering and pipelining fix this by overlapping memory loads and computation so the GPU is always doing sommething useful.\nDouble Buffering The core idea here is that while you are computing on tile A, you’re already loading tile B to memory. Once you are done computing tile A, you switch to computing tile B (which is preloaded in memory, reducing latency). Similarly, while tile B is being computed, we start loading tile C.\nAt the same time, one buffer is being used and another one is being loaded. This keeps SMs busy while memory latency is hidden in the background.\nHere’s how double buffering looks in CUDA\n__shared__ float tileA[2][BLOCK_SIZE][BLOCK_SIZE]; int curr = 0; for (int k = 0; k \u003c numTiles; ++k) { load_tile(..., tileA[1 - curr]); // load next tile into the \"other\" buffer __syncthreads(); // ensure load is done compute_with(tileA[curr]); // compute current buffer curr = 1 - curr; // flip buffers } The key reason why double buffering works is that the time to toggle is less than the time to write to memory\nPipelining Double buffering is one type of software pipelining.\nPipelining is just a method where you break your loop into stages (memory load, compute, and store) and overlap them. Double buffering just becomes one of the most effective way to actually go ahead and make us of pipelining for GPUs.\nThe backbone of all pipelining is\nfor (tile in tiles) { load tile X (while computing tile X-1) compute tile X write tile X to global memory (while computing tile X+1) } The main goal is to stagger the load stage, compute stage, and storage stage across multiple tiles.\nProfiling Double Buffering / Pipelining helps a lot when\nglobal memory latency is a bottleneck shared memory bandwidth \u003e latency and you can hide loads your workloads can easily be tiled (matmul, conv, attention) Just like anything with CUDA, trying to do double buffering incorrectly can also silently wreck performance or crash your code.\nSince we are doubling shared memory usage, we could run into overflow limits. Also, more frequent thread syncs makes it more vulnerable for race conditions to sneak in easily.\nGrid-Stride Loops (for arbitrary input sizes) Here, we focus on making our kernels usable on real-world input sizes. I touched on this part before, but wanted to dive in deeper because of how crucial grid-stride loops are in real world CUDA programming.\nPreviously, we saw\nint idx = threadIdx.x + blockIdx.x * blockDim.x; if (idx \u003c N) { // do work } and found that this only works for a small N where the total number of threads covers the whole array. This starts to break down when N is huge, our grid isn’t big enough, and when we want kernels that work regardless of input size. The fix for these issues is a grid-stride loop.\nEssentially, a grid-stride loop is a loop inside our kernel that lets each thread process multiple elements spaced across the input.\nint idx = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for (; idx \u003c N; idx += stride) { // process element idx } Here, every thread starts at its own unique idx but keeps going every stride steps until it covers the whole array. We don’t need to worry about how big the grid is, this will always work. Let’s see why:\nIf we take a simple example like adding two vectors element wise\n__global__ void vectorAdd(float* a, float* b, float* out, int N) { int idx = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for (; idx \u003c N; idx += stride) { out[idx] = a[idx] + b[idx]; } } If we saw\nblockDim.x = 32 (threads per block) gridDim.x = 2 (number of blocks) Hence stride = 32 * 2 = 64. So every thread starts at its own unique index idx and hops ahead by 64 (stride) on each loop iteration. It would look like\nThread 0: idx = 0, 64, 128, 192, ... Thread 1: idx = 1, 65, 129, 193, ... Thread 2: idx = 2, 66, 130, 194, ... . . . Thread 63: idx = 63, 127, 191, 255, ... Because each thread starts at a unique index and they each loop over different parts of the input spaces stride apart, no two threads will ever touch the same index (no double processing of a single thread). We also cover all elements in the array [0, N) as long as the loop condition is idx \u003c N.\nHere’s an example of a fused op that uses a grid-stride loop\n__global__ void relu_square(float* x, float* out, int N) { int idx = threadIdx.x + blockIdx.x * blockDim.x; int stride = blockDim.x * gridDim.x; for (; idx \u003c N; idx += stride) { float val = max(0.0f, x[idx]); out[idx] = val * val; } } This now works for N = 1024, N = 3057, or even N = 10000000 and we don’t waste threads on bounds checking. We can see how easy it is to scale with whatever grid configuration you want.\nOpenAI’s Triton In my opinion, Triton might be one of the cleanest and most dangerous tools in the CUDA stack today.\nTriton lets you write high-performance GPU kernels in Python with CUDA level speed. Using NumPy/PyTorch syntax we are able to write functions that compile into custom fused kernels that are extremely efficient.\nRemember the ReLU square kernel from before? This is what it turns into\n@triton.jit def relu_square_kernel(x_ptr, out_ptr, N, BLOCK: tl.constexpr): pid = tl.program_id(0) offsets = pid * BLOCK + tl.arange(0, BLOCK) mask = offsets \u003c N x = tl.load(x_ptr + offsets, mask=mask) out = tl.where(x \u003e 0, x * x, 0.0) tl.store(out_ptr + offsets, out, mask=mask) there’s definitely some new complexities here but we didn’t have to worry about __global__, \u003c\u003c\u003c\u003e\u003e\u003e, or warp math. Triton independently handles tiling, thread/block math, conditional masking (essential in ML), and fast memory loads automatically. In my opinion, it’s like when CUDA meets the simplicity of Python.\nLet’s unwrap the function to understand it more.\n@triton.jit def relu_square_kernel(x_ptr, out_ptr, N, BLOCK: tl.constexpr): The first part that should jump out to you is the decorator @triton.jit which tells Triton to compile the function using it’s just-in-time compiler into a GPU kernel.\nx_ptr and out_ptr are pointers to memory on the GPU like float* in CUDA C. N is the total elements in the input BLOCK: tl.constexpr is a constant known at compile-time and is used for tile/block size In the first three lines\npid = tl.program_id(0) offsets = pid * BLOCK + tl.arange(0, BLOCK) mask = offsets \u003c N pid and offsets set up the block and threads while the mask prevent out-of-bounds access at the end of the array.\nWe get to the core of our computation (Load, Compute, Store) with the next few lines of code\nx = tl.load(x_ptr + offsets, mask=mask) out = tl.where(x \u003e 0, x * x, 0.0) tl.store(out_ptr + offsets, out, mask=mask) The first line pulls in the data for thread group from memroy and only loads where mask is set to True.\nThe second line is the core of the function, its the logic for ReLU squared (x * x if x \u003e 0, else 0.0).\nLaslty, tl.store writes the result back to out_ptr safely using the mask.\nWith this function, we were able to launch an entire kernel that\ndivides work among blocks using program_id loads a slice of input with bounds checking (with mask) applies a fused op (relu + square) writes results back safely (vectorized and masked) Triton lets us forget about warp syncs and memory alignment headaches. It autocompiles down to PTX (NVIDIA assembly) giving us CUDA-level performance with NumPy syntax (a game changer for ML).\nFamous kernels Here, I wanted to discuss some of the most important kernels that reshaped the way we do ML under the hood and improved training \u0026 inference times for everyone in the world. These aren’t just optimizations, they’re performance enablers that made LLMs, ViTs, and giant training runs even possible.\nFlashAttention (Tri Dao et al., 2022 — “Reducing Memory Bottlenecks in Attention”)\nFlashAttention is a tiled + fused implementation of the attention mechanism that rewrites how we compute softmax(QKᵀ / √d) · V.\nInstead of computing QKᵀ, writing it to global memory, then applying softmax, FlashAttention fuses everything inside shared memory for maximum performance.\nHere’s what gets fused\nTiled matmul: QKᵀ Softmax (numerically stable softmax) Dropout (only during training) Final matmul with V This means that no intermediate QK matrix ever touches global memory since everything stays in registers / shared memory. This enables longest context lengths without blowing up vRAM.\nFlashAttention showed a 2-4x speedup vs standard attention\nNow imagine that speedup compounding at every layer of the transformer.\nHere’s a simplified version of that code written in Triton\nimport triton import triton.language as tl @triton.jit def flash_attn_kernel(Q_ptr, K_ptr, V_ptr, Out_ptr, seq_len, d_model BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl constexpr): # program IDs = where this block of threads is in Q and K q_row = tl.program_id(0) k_col = tl.program_id(1) # offset for Q offs_m = q_row * BLOCK_M + tl.arange(0, BLOCK_M) offs_n = k_col * BLOCK_N + tl.arange(0, BLOCK_N) offs_d = tl.arange(0, BLOCK_DMODEL) # load tiles from Q and K Q_tile = tl.load(Q_ptr + offs_m[:, None] * d_model + offs_d[None, :]) K_tile = tl.load(K_ptr + offs_n[:, None] * d_model + offs_d[None, :]) # compute dot product Q @ K.T (BLOCK_M x BLOCK_N) scores = tl.dot(Q_tile, tl.trans(K_tile)) # scale scale = 1.0 / (d_model ** 0.5) scores *= scale # softmax over rows max_scores = tl.max(scores, axis=1) scores = scores - max_scores[:, None] scores = tl.exp(scores) denom = tl.sum(scores, axis=1) scores = scores / denom[:, None] # load V tile V_tile = tl.load(V_ptr + offs_n[:, None] * d_model + offs_d[None, :]) # output = scores @ V (BLOCK_M x D_MODEL) output = tl.dot(scores, V_tile) # write back to output tl.store(Out_ptr + offs_m[:, None] * d_model + offs_d[None, :], output) If you read between the lines, it’s easy to see that we’re still doing the same computation for transfomer style attention. The same Q @ K.T is scaled and softmaxed but everything is optimized to be as quick as possible for the GPU.\nfused_add_bias_relu_dropout This kernel shows up quite literally everywhere inside MLP blocks of a transformer. It fuses a few simple ops but the payoff is massive.\ny = dropout(relu(x + bias)) Here, each of these steps alone would load and store a full tensor and cost 3-4 global memory trips. This kernel fuses all 3 by adding bias, applying ReLU, then finally applying dropout mask and scaling. In fused form this operation saves huge on memory bandwith and reduces kernel launch overhead.\nThis operation compounds - this bias + relu + dropout combination shows up so much in the transformer and speeding up the kernel at the core will happen at every single part of the transformer where this is called. This fusion saves millions of tensor element read/writes per forward pass.\n@triton.jit def fused_add_bias_relu_dropout(x_ptr, bias_ptr, out_ptr, mask_ptr, N, H, p_dropout: tl.constexpr, BLOCK_SIZE: tl.constexpr): row_idx = tl.program_id(0) cols = tl.arange(0, BLOCK_SIZE) # offsets offs = row_idx * H + cols # load input and bias x = tl.load(x_ptr + offs, mask=cols \u003c H, other=0.0) b = tl.load(bias_ptr + cols, mask=cols \u003c H, other=0.0) # add bias and apply ReLU x = x + b x = tl.maximum(x, 0.0) # apply dropout rng = tl.rand(cols.shape) keep = rng \u003e p_dropout scale = 1.0 / (1.0 - p_dropout) x = tl.where(keep, x * scale, 0.0) # store results and dropout mask tl.store(out_ptr + offs, x, mask=cols \u003c H) tl.store(mask_ptr + offs, keep.to(tl.int1), mask=cols \u003c H) Fused LayerNorm Just like the kernel above, LayerNorm is critical in every transformer block and shows up multiple times per layer. Similarly, speeding up this kernel will compound into saving us many tensor read/writes.\nStandard LayerNorm computes the mean and variance over a vector, normalizes by subtracting the mean and dividing by std, and finally sclaes with γ. If we were to do this regularly, there would be multiple times we would read/write from global memory.\nIn Fused LayerNorm, we do everything in a single kernel:\nLoads the vector once Run Welford’s algorithm for numerically stable mean/std Normalizes and applies affine weights Writes once Here, we reduce intermediate memory usage, kernel launch latency, and redundant passes over the same vector.\n@triton.jit def fused_layernorm_kernel(x_ptr, gamma_ptr, beta_ptr, y_ptr, N, EPS,BLOCK_SIZE: tl.constexpr): # get the program ID (1D grid) pid = tl.program_id(0) block_start = pid * BLOCK_SIZE offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u003c N x = tl.load(x_ptr + offsets, mask=mask) # welford's algorithm for mean/variance mean = tl.sum(x, mask=mask) / N var = tl.sum((x - mean) ** 2, mask=mask) / N inv_std = 1.0 / tl.sqrt(var + EPS) # normalize x_hat = (x - mean) * inv_std gamma = tl.load(gamma_ptr + offsets, mask=mask) beta = tl.load(beta_ptr + offsets, mask=mask) y = x_hat * gamma + beta # write output tl.store(y_ptr + offsets, y, mask=mask) On average, we see a 10-20% LayerNorm speedup with this fused kernel, but that gets amplified across hundreds of layers in transformers.\nfused_multihead_attention_backward Learning about this kernel blew my mind because it fuses the entire backward pass of multi-head attention.\nThis fused op deals with quite literally everyting:\nGradients for Q, K, V are calculated Softmax derivative is applied Handles matmul and broadcasted add Backpropagtes through making, scaling, and dropout And all of this is done without touching global memory between steps. This kernel is what enables large-scale training runs like GPT-3. It’s crazy to think about that the current state of LLMs would look wildly differnet (not as good) if we didn’t have this kernel.\nWithout this kernel, we would need to recompute or store every intermediate (QKᵀ, softmax, dropout mask) and we would likely stall on memory bandwidth and VRAM usage.\nHere’s a slightly simpler version of what the actual kernel would look like (I didn’t include block tiling / batching of multiple heads).\n@triton.jit def fused_attention_backward(dY_ptr, Q_ptr, K_ptr, V_ptr, attn_weights_ptr, dQ_ptr, dK_ptr, dV_ptr, BLOCK_SIZE: tl.constexpr, HEAD_DIM: tl.constexpr, SCALE: tl.constexpr): pid = tl.program_id(0) # load blocks of dY, Q, K, V offs = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE) dY = tl.load(dY_ptr + offs) Q = tl.load(Q_ptr + offs) K = tl.load(K_ptr + offs) V = tl.load(V_ptr + offs) attn_weights = tl.load(attn_weights_ptr + offs) # compute grad wrt softmax inputs, ds = softmax_grad(dY, attn_weights) — this must be numerically stable ds = dY * attn_weights - attn_weights * tl.sum(dY * attn_weights, axis=-1, keepdim=True) # compute dQ, dK, dV dQ = tl.dot(ds, K) * SCALE dK = tl.dot(ds.T, Q) * SCALE dV = tl.dot(attn_weights.T, dY) # store results tl.store(dQ_ptr + offs, dQ) tl.store(dK_ptr + offs, dK) tl.store(dV_ptr + offs, dV) This kernel fuses everything a backward pass normally splits across 6-10 kernels. You can imagine the compounding speedup.\nConclusion “If you want to have the biggest impact on ML today, write a faster kernel”\nUnderneath every flashy AI product demo or multimodal model is a war for compute efficiency. Training future models isn’t just about bigger datasets or better prompts, it’s about fusing ops, minimizing memory transfers, and squeezing every cycle out of your GPU cores.\nAt the end of the day, the goal is simple: keep as many threads doing useful work as possible (but there’s a really fine line). Push too hard and you’ll drown in memory stalls. Push too little and you’re underutilizing, leaving speed and money on the table.\nCUDA and Triton aren’t just power tools, they’re leverage. A few lines of well-optimized kernel code can save millions in training costs.\n",
  "wordCount" : "4544",
  "inLanguage": "en",
  "datePublished": "2025-07-11T00:00:00Z",
  "dateModified": "2025-07-11T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "ML Theory + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/cuda2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/reimplementations/" title="Reimplementations">
                    <span>Reimplementations</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton
    </h1>
    <div class="post-meta"><span title='2025-07-11 00:00:00 +0000 UTC'>July 11, 2025</span>&nbsp;·&nbsp;22 min&nbsp;·&nbsp;ML Theory &#43; Code

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.</p>
<p>The repsitory with the corresponding PyTorch implementation is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<h2 id="tiling-and-blocking-in-matrix-multiplication">Tiling and Blocking in Matrix Multiplication<a hidden class="anchor" aria-hidden="true" href="#tiling-and-blocking-in-matrix-multiplication">#</a></h2>
<p>Matrix multiplication is one of the most fundamental operations in deep learning. It powers everything from linear layers to conv nets and attention heads. So while writing <code>@</code> or <code>torch.matul()</code> (or even <code>torch.bmm()</code>) feels like a one-liner, under the hood it&rsquo;s a battlefield of memory access patterns, cache misses, and thread synchronization to get the matrix multiply done <em>as quick as possible</em>. To get this fast on GPUs, we use two key ideas: <strong>blocking</strong> and <strong>tiling</strong>.</p>
<p>The naive alogrithm for matmul runs in O(n³) and does</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> N; i<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> j <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; j <span style="color:#f92672">&lt;</span> N; j<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> N; k<span style="color:#f92672">++</span>)
</span></span><span style="display:flex;"><span>      C[i][j] <span style="color:#f92672">+=</span> A[i][k] <span style="color:#f92672">*</span> B[k][j];
</span></span></code></pre></div><p>This works on a CPU or GPU but on the GPU it&rsquo;s slower than the CPU (which is exactly the opposite of what we want). This is because:</p>
<ul>
<li>We&rsquo;re constantly reading and writing from global memory</li>
<li>We&rsquo;re re-fetching the same values repeatedly across threads</li>
<li>We&rsquo;re not taking advantage of sharing memory, coalesced reads, or warp-level reuse</li>
</ul>
<p>We need to warp (pun intended) this algorithm such that our GPU can parallelize and speed it up and not call from global memory every call.</p>
<h3 id="blocking-divide-the-work">Blocking: Divide the work<a hidden class="anchor" aria-hidden="true" href="#blocking-divide-the-work">#</a></h3>
<p>The idea here is to divide the matrix into square blocks. Each block computes a submatrix of the output C.</p>
<p>Imagine if each block computes a <code>BLOCK_SIZE x BLOCK_SIZE</code> patch of C.</p>
<p>Each thread in the block computes only a single element in that tile. So if the block size is 16, the block launches 256 threads (16x16) and each thread computes a single <code>[i][j]</code>.</p>
<div align="center">
  <img src="/images/2cuda1.png" alt="wtfff" width="500"/>
  <p>Each thread block loads a tile of input, weight, and output data into fast shared memory, minimizing slow global memory access. Threads then collaboratively compute a submatrix of the output using those tiles, reusing data efficiently through blocking and tiling.</p>
</div>
<h3 id="tiling-reuse-data-in-shared-memory">Tiling: Reuse data in shared memory<a hidden class="anchor" aria-hidden="true" href="#tiling-reuse-data-in-shared-memory">#</a></h3>
<p>Inside each block, we tile the <code>k</code> dimension so that</p>
<ul>
<li>A tile from matrix A (<code>A[i][j]</code>) is loaded into shared memory</li>
<li>A tile from matrix B (<code>B[i][j]</code>) is also loaded into shared memory</li>
</ul>
<p>Given this, each thread now loads a chunk of A and B once (from global → shared). It then reuses it multiple time for the inner <code>k</code> loop and only writes back to global memory once (at the end).</p>
<p>This is what GPUs were designed for: get your computations done quick and read/write to memory only at the end of the computation.</p>
<p>Here is what matmul looks like after the upgrade</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>__global__ <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">matmul_shared</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> A, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> B, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> C, <span style="color:#66d9ef">int</span> N) {
</span></span><span style="display:flex;"><span>    __shared__ <span style="color:#66d9ef">float</span> tileA[BLOCK_SIZE][BLOCK_SIZE];
</span></span><span style="display:flex;"><span>    __shared__ <span style="color:#66d9ef">float</span> tileB[BLOCK_SIZE][BLOCK_SIZE];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> row <span style="color:#f92672">=</span> blockIdx.y <span style="color:#f92672">*</span> BLOCK_SIZE <span style="color:#f92672">+</span> threadIdx.y;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> col <span style="color:#f92672">=</span> blockIdx.x <span style="color:#f92672">*</span> BLOCK_SIZE <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> sum <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0f</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> N <span style="color:#f92672">/</span> BLOCK_SIZE; k<span style="color:#f92672">++</span>) {
</span></span><span style="display:flex;"><span>        tileA[threadIdx.y][threadIdx.x] <span style="color:#f92672">=</span> A[row <span style="color:#f92672">*</span> N <span style="color:#f92672">+</span> k <span style="color:#f92672">*</span> BLOCK_SIZE <span style="color:#f92672">+</span> threadIdx.x];
</span></span><span style="display:flex;"><span>        tileB[threadIdx.y][threadIdx.x] <span style="color:#f92672">=</span> B[(k <span style="color:#f92672">*</span> BLOCK_SIZE <span style="color:#f92672">+</span> threadIdx.y) <span style="color:#f92672">*</span> N <span style="color:#f92672">+</span> col];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        __syncthreads(); <span style="color:#75715e">// all threads have to load their tiles
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> n <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; n <span style="color:#f92672">&lt;</span> BLOCK_SIZE; n<span style="color:#f92672">++</span>) {
</span></span><span style="display:flex;"><span>            sum <span style="color:#f92672">+=</span> tileA[threadIdx.y][n] <span style="color:#f92672">*</span> tileB[n][threadIdx.x];
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        __syncthreads(); <span style="color:#75715e">// sync before loading next tile
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    C[row <span style="color:#f92672">*</span> N <span style="color:#f92672">+</span> col] <span style="color:#f92672">=</span> sum;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>As we can see, this loads a tile of A and B just <strong>once per block</strong>. All the threads reuse the data and this reduces global memory bandwith usage dramatically.</p>
<p>Let&rsquo;s dive into an example so that this really clicks.</p>
<p>Here are two 4x4 matrices, A and B.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>A <span style="color:#f92672">=</span>  [ [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">10</span>,<span style="color:#ae81ff">11</span>,<span style="color:#ae81ff">12</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">13</span>,<span style="color:#ae81ff">14</span>,<span style="color:#ae81ff">15</span>,<span style="color:#ae81ff">16</span>] ]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">=</span>  [ [<span style="color:#ae81ff">17</span>, <span style="color:#ae81ff">18</span>, <span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">20</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">21</span>, <span style="color:#ae81ff">22</span>, <span style="color:#ae81ff">23</span>, <span style="color:#ae81ff">24</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">26</span>, <span style="color:#ae81ff">27</span>, <span style="color:#ae81ff">28</span>],
</span></span><span style="display:flex;"><span>       [<span style="color:#ae81ff">29</span>,<span style="color:#ae81ff">30</span>,<span style="color:#ae81ff">31</span>,<span style="color:#ae81ff">32</span>] ]
</span></span></code></pre></div><p>we can split up those matrices into sub-matrices like the following</p>
<div align="center">
  <img src="/images/2cuda2.jpeg" alt="wtfff" width="750"/>
  <p></p>
</div>
<p>The equation to compute C00 is</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">C00</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">A00×B00 + A01×B10</span>
</span></span></code></pre></div><p>we can split that up into two different computations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">C00_partial_1</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">A00 × B00</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00_partial_2</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">A01 × B10</span>
</span></span></code></pre></div><p>For <code>C00_partial_1</code> we can look at the computation thread-wise.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[0][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">1×17 + 2×21 = 17 + 42 = 59</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[0][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">1×18 + 2×22 = 18 + 44 = 62</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[1][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">5×17 + 6×21 = 85 + 126 = 211</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[1][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">5×18 + 6×22 = 90 + 132 = 222</span>
</span></span></code></pre></div><p>a similar thing would happen on the GPU for <code>C00P2</code> as well.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">C00P2[0][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">3×25 + 4×29 = 75 + 116 = 191</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P2[0][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">3×26 + 4×30 = 78 + 120 = 198</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P2[1][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">7×25 + 8×29 = 175 + 232 = 407</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P2[1][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">7×26 + 8×30 = 182 + 240 = 422</span>
</span></span></code></pre></div><p>We can then add both of them together to get back <code>C00</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[0][0] + C00P2[0][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">59 + 191 = 250</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[0][1] + C00P2[0][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">62 + 198 = 260</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[1][0] + C00P2[1][0]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">211 + 407 = 618</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">C00P1[1][1] + C00P2[1][1]</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">222 + 422 = 644</span>
</span></span></code></pre></div><p>Here&rsquo;s the magic. The GPU isn&rsquo;t just computing C00. It&rsquo;s computing <strong>all of C</strong> at once. We just walked through one quartile but the GPU is going to do C00, C01, C10, and C11 simultaneously.</p>
<p>Each GPU block takes care of one submatrix of C, and each thread inside the block computes one individual element like <code>C[i][j]</code>.</p>
<p>The last thing we should think about is memory usage. We have two parts to think about: shared memory and global memory and being smart about when we use each is vital to efficiency.</p>
<ul>
<li>A tile from <code>A</code> and a tile from <code>B</code> is loaded onto <strong>shared memory</strong>, once per block</li>
<li>Individual threads in the block reuse this data to compute the full partial product (through <strong>register memory</strong>)</li>
<li>Intermediate values (like <code>C00_partial1</code>) are held in registers or local memory, not written to global immediately</li>
<li>After summing all the partials, we write the final result only <strong>once</strong> to global memory.</li>
</ul>
<p>As a recap, we read and write from global memory once (to get input and write output), (partial) tiles from A and B are stored in the current block, and intermeditate values (like sum) are stored in registers.</p>
<hr>
<h2 id="occupancy-registers-and-thread-utilization">Occupancy, Registers, and Thread Utilization<a hidden class="anchor" aria-hidden="true" href="#occupancy-registers-and-thread-utilization">#</a></h2>
<p>In my opinion, this is a huge leap from what we were doing before. If previously (including last post) we were writing CUDA kernels, we move into <em>performance engineering</em> now.</p>
<p>But before we dive in, we need to define a key component in GPUs: streaming multiprocessor (SM). These are the basic building blocks of an NVIDIA GPU, they can be thought of as a &ldquo;mini CPU cluster&rdquo; inside the GPU. Every modern NVIDIA GPU contains dozens of SMs and each SM can run multiple warps concurrently.</p>
<h3 id="what-is-occupancy">What is occupancy?<a hidden class="anchor" aria-hidden="true" href="#what-is-occupancy">#</a></h3>
<p>Occupancy is the ratio <em>active warps</em> to <em>maximum possible warps</em> on a SM. High occupancy means that we have more warps ready to run meaning lower latency. However, if we have too high occupancy, it can hurt if it means each thread gets fewer registers or shared memory.</p>
<p>Empirically, more occupany helps hide memory latency but after ~70%, we hit diminishing returns.</p>
<h3 id="register-usage">Register Usage?<a hidden class="anchor" aria-hidden="true" href="#register-usage">#</a></h3>
<p>Each SM has a limited nmber of registers. If your kernel uses too many registers per thread, fewer threads can be launched per block. This in turn reduces occupancy and may leave GPU cores underutilized.</p>
<p>Luckily, CUDA&rsquo;s compiler can quickly tell us register usage when compiling.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>nvcc --ptxas-options<span style="color:#f92672">=</span>-v
</span></span></code></pre></div><p>We can even ask the compiler to limit registers by using specific flags like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>--maxrregcount<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>
</span></span></code></pre></div><p>There’s a tradeoff here as well: limiting register usage might cause register spilling, where the compiler offloads variables from fast registers to local memory.</p>
<p>Despite the name, local memory is actually stored in global memory, just scoped per thread. This means spilled variables suffer from global memory latency, which can <em><strong>silently wreck performance</strong></em>, especially inside tight loops or compute-heavy kernels.</p>
<h3 id="thread-utilization">Thread utilization?<a hidden class="anchor" aria-hidden="true" href="#thread-utilization">#</a></h3>
<p>For maximum efficiency, each thread should be doing useful work. A common way that threads are wasted is when they run into</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (i <span style="color:#f92672">&gt;=</span> N) <span style="color:#66d9ef">return</span>; 
</span></span></code></pre></div><p>An alternative to this is to use grid-stride loop when N might be bigger than your grid:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">+</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x; i <span style="color:#f92672">&lt;</span> N; i <span style="color:#f92672">+</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// do work on element i
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><p>Let&rsquo;s understand exactly what&rsquo;s going on here. Index <code>i</code> is set to <code>threadIdx.x + blockIdx.x * blockDim.x</code> which makes sense, this gives you the global thread index, uniquely identifying each thread across the grid. <code>blockDim.x * gridDim.x</code> is the total number of threads in the grid, which becomes the <strong>stride size</strong> (<code>stride</code>). So when we increment <code>i</code> by <code>stride</code> (<code>i += stride</code>) each thread jumps by the total thread count and does work again. This ensures all elements from 0 to N-1 get covered, even if there are more elements than threads.</p>
<p>CUDA performance tuning is a balancing act: maximize thread usage and occupancy, but stay under register and shared memory limits or risk silent slowdowns.</p>
<hr>
<h2 id="loop-unrolling-and-compiler-hints">Loop Unrolling and Compiler Hints<a hidden class="anchor" aria-hidden="true" href="#loop-unrolling-and-compiler-hints">#</a></h2>
<p>This is how you squeeze out that last 20-30% gains from GPU kernels.</p>
<h3 id="loop-unrolling">Loop Unrolling<a hidden class="anchor" aria-hidden="true" href="#loop-unrolling">#</a></h3>
<p>This is a performance optimization which can reduce loop overhead and increase instruction level parallelism on a GPU.</p>
<p>Instead of</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">4</span>; <span style="color:#f92672">++</span>i) {
</span></span><span style="display:flex;"><span>    sum <span style="color:#f92672">+=</span> a[i] <span style="color:#f92672">*</span> b[i];
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>the compiler (or the dev, manually) can unroll it to</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>sum <span style="color:#f92672">+=</span> a[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> b[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>sum <span style="color:#f92672">+=</span> a[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> b[<span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>sum <span style="color:#f92672">+=</span> a[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">*</span> b[<span style="color:#ae81ff">2</span>];
</span></span><span style="display:flex;"><span>sum <span style="color:#f92672">+=</span> a[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">*</span> b[<span style="color:#ae81ff">3</span>];
</span></span></code></pre></div><p>Even though the above is a simple loop and threads are launched independently, it still has to ocheck the loop conditions (<code>i &lt; 4</code>), increment <code>i</code>, jump back to the top of the loop, and possibly predict branches. This costs instruction cycles, especially when the loop is small and fixed-size.</p>
<h3 id="manual-vs-automatic-unrolling">Manual vs Automatic Unrolling<a hidden class="anchor" aria-hidden="true" href="#manual-vs-automatic-unrolling">#</a></h3>
<p>Modern compilers like nvcc, clang, or gcc often auto-unroll small loops if the loop bounds are known at compile time.</p>
<p>However, manual unrolling gives you strong control.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#pragma unroll
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">8</span>; <span style="color:#f92672">++</span>i) {
</span></span><span style="display:flex;"><span>    sum <span style="color:#f92672">+=</span> a[i] <span style="color:#f92672">*</span> b[i];
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>would unroll your whole loop for you.</p>
<h3 id="when-to-unroll">When to unroll?<a hidden class="anchor" aria-hidden="true" href="#when-to-unroll">#</a></h3>
<p>You should unroll when:</p>
<ul>
<li>The loop bound is known at compile time (like <code>i &lt; 20</code>)</li>
<li>The logic inside the loop is <em>simple</em> and has no early exits</li>
<li>You only care about maxing out throughput on critical compute</li>
</ul>
<p>On the flip side, don&rsquo;t unroll when the logic inside the loop is huge or has control flow or if you are at risk of register pressure (too many variables per thread). Over-unrolling can lead to register spilling which turns into a silent performance crash.</p>
<p>CUDA is all about balance. Unroll too little and leave performance on the table. Unroll too much and risk register spilling, silent crashes, or warp inefficiency. The real skill is finding that sweet spot in the middle.</p>
<hr>
<h2 id="double-buffering-and-pipelining">Double Buffering and Pipelining<a hidden class="anchor" aria-hidden="true" href="#double-buffering-and-pipelining">#</a></h2>
<p>Using these techniques, we make our kernels never wait for memory.</p>
<p>In a perfect CUDA kernel, computation and memory transfers happen at the same time. In most naive implementations, we either have to</p>
<ul>
<li>wait to load data into shared memory</li>
<li>wait to compute after it&rsquo;s been loaded</li>
</ul>
<p>Waiting for memory is wasted time. Double bluffering and pipelining fix this by overlapping memory loads and computation so the GPU is always doing sommething useful.</p>
<h3 id="double-buffering">Double Buffering<a hidden class="anchor" aria-hidden="true" href="#double-buffering">#</a></h3>
<p>The core idea here is that while you are computing on tile A, you&rsquo;re already loading tile B to memory. Once you are done computing tile A, you switch to computing tile B (which is preloaded in memory, reducing latency). Similarly, while tile B is being computed, we start loading tile C.</p>
<p>At the same time, one buffer is being used and another one is being loaded. This keeps SMs busy while memory latency is hidden in the background.</p>
<p>Here&rsquo;s how double buffering looks in CUDA</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>__shared__ <span style="color:#66d9ef">float</span> tileA[<span style="color:#ae81ff">2</span>][BLOCK_SIZE][BLOCK_SIZE];
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">int</span> curr <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> k <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; k <span style="color:#f92672">&lt;</span> numTiles; <span style="color:#f92672">++</span>k) {
</span></span><span style="display:flex;"><span>    load_tile(..., tileA[<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> curr]); <span style="color:#75715e">// load next tile into the &#34;other&#34; buffer
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    __syncthreads(); <span style="color:#75715e">// ensure load is done
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    compute_with(tileA[curr]); <span style="color:#75715e">// compute current buffer
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    curr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> curr; <span style="color:#75715e">// flip buffers
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><blockquote>
<p>The key reason why double buffering works is that the time to toggle is less than the time to write to memory</p></blockquote>
<h3 id="pipelining">Pipelining<a hidden class="anchor" aria-hidden="true" href="#pipelining">#</a></h3>
<p>Double buffering is one type of software pipelining.</p>
<p>Pipelining is just a method where you break your loop into stages (memory load, compute, and store) and <em>overlap</em> them. Double buffering just becomes one of the most effective way to actually go ahead and make us of pipelining for GPUs.</p>
<p>The backbone of all pipelining is</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (tile in tiles) {
</span></span><span style="display:flex;"><span>    load tile X (<span style="color:#66d9ef">while</span> computing tile X<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    compute tile X
</span></span><span style="display:flex;"><span>    write tile X to global memory (<span style="color:#66d9ef">while</span> computing tile X<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The main goal is to stagger the load stage, compute stage, and storage stage across multiple tiles.</p>
<h3 id="profiling">Profiling<a hidden class="anchor" aria-hidden="true" href="#profiling">#</a></h3>
<p>Double Buffering / Pipelining helps a lot when</p>
<ul>
<li>global memory latency is a bottleneck</li>
<li>shared memory bandwidth &gt; latency and you can hide loads</li>
<li>your workloads can easily be tiled (matmul, conv, attention)</li>
</ul>
<p>Just like anything with CUDA, trying to do double buffering incorrectly can also silently wreck performance or crash your code.</p>
<p>Since we are doubling shared memory usage, we could run into overflow limits. Also, more frequent thread syncs makes it more vulnerable for race conditions to sneak in easily.</p>
<hr>
<h2 id="grid-stride-loops-for-arbitrary-input-sizes">Grid-Stride Loops (for arbitrary input sizes)<a hidden class="anchor" aria-hidden="true" href="#grid-stride-loops-for-arbitrary-input-sizes">#</a></h2>
<p>Here, we focus on making our kernels usable on real-world input sizes. I touched on this part before, but wanted to dive in deeper because of how crucial grid-stride loops are in real world CUDA programming.</p>
<p>Previously, we saw</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">int</span> idx <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">+</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> (idx <span style="color:#f92672">&lt;</span> N) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// do work
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><p>and found that this only works for a small N where the total number of threads covers the whole array. This starts to break down when N is huge, our grid isn&rsquo;t big enough, and when we want kernels that work regardless of input size. The fix for these issues is a grid-stride loop.</p>
<p>Essentially, a grid-stride loop is a loop inside our kernel that lets <em>each thread</em> process multiple elements spaced across the input.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">int</span> idx <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">+</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">int</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> (; idx <span style="color:#f92672">&lt;</span> N; idx <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// process element idx
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><p>Here, every thread starts at its own unique <code>idx</code> but keeps going every <code>stride</code> steps until it covers the whole array. We don&rsquo;t need to worry about how big the grid is, this will <strong>always work</strong>. Let&rsquo;s see why:</p>
<p>If we take a simple example like adding two vectors element wise</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>__global__ <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">vectorAdd</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> a, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> b, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> out, <span style="color:#66d9ef">int</span> N) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> idx <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">+</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (; idx <span style="color:#f92672">&lt;</span> N; idx <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>        out[idx] <span style="color:#f92672">=</span> a[idx] <span style="color:#f92672">+</span> b[idx];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>If we saw</p>
<ul>
<li><code>blockDim.x</code> = 32 (threads per block)</li>
<li><code>gridDim.x</code> = 2 (number of blocks)
Hence <code>stride</code> = 32 * 2 = 64.</li>
</ul>
<p>So every thread starts at its own unique index <code>idx</code> and hops ahead by 64 (<code>stride</code>) on each loop iteration. It would look like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Thread 0: idx = 0, 64, 128, 192, ...
</span></span><span style="display:flex;"><span>Thread 1: idx = 1, 65, 129, 193, ...
</span></span><span style="display:flex;"><span>Thread 2: idx = 2, 66, 130, 194, ...
</span></span><span style="display:flex;"><span>.
</span></span><span style="display:flex;"><span>.
</span></span><span style="display:flex;"><span>.
</span></span><span style="display:flex;"><span>Thread 63: idx = 63, 127, 191, 255, ...
</span></span></code></pre></div><p>Because each thread starts at a unique index and they each loop over different parts of the input spaces <code>stride</code> apart, no two threads will ever touch the same index (no double processing of a single thread). We also cover all elements in the array [0, N) as long as the loop condition is <code>idx &lt; N</code>.</p>
<p>Here&rsquo;s an example of a fused op that uses a grid-stride loop</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>__global__ <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">relu_square</span>(<span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> x, <span style="color:#66d9ef">float</span><span style="color:#f92672">*</span> out, <span style="color:#66d9ef">int</span> N) {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> idx <span style="color:#f92672">=</span> threadIdx.x <span style="color:#f92672">+</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int</span> stride <span style="color:#f92672">=</span> blockDim.x <span style="color:#f92672">*</span> gridDim.x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (; idx <span style="color:#f92672">&lt;</span> N; idx <span style="color:#f92672">+=</span> stride) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">float</span> val <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.0f</span>, x[idx]);
</span></span><span style="display:flex;"><span>        out[idx] <span style="color:#f92672">=</span> val <span style="color:#f92672">*</span> val;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This now works for N = 1024, N = 3057, or even N = 10000000 and we don&rsquo;t waste threads on bounds checking. We can see how easy it is to scale with whatever grid configuration you want.</p>
<hr>
<h2 id="openais-triton">OpenAI&rsquo;s Triton<a hidden class="anchor" aria-hidden="true" href="#openais-triton">#</a></h2>
<p>In my opinion, Triton might be one of the cleanest and most dangerous tools in the CUDA stack today.</p>
<p>Triton lets you write high-performance GPU kernels in Python with CUDA level speed. Using NumPy/PyTorch syntax we are able to write functions that compile into custom fused kernels that are extremely efficient.</p>
<p>Remember the ReLU square kernel from before? This is what it turns into</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu_square_kernel</span>(x_ptr, out_ptr, N, BLOCK: tl<span style="color:#f92672">.</span>constexpr):
</span></span><span style="display:flex;"><span>    pid <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    offsets <span style="color:#f92672">=</span> pid <span style="color:#f92672">*</span> BLOCK <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK)
</span></span><span style="display:flex;"><span>    mask <span style="color:#f92672">=</span> offsets <span style="color:#f92672">&lt;</span> N
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(x_ptr <span style="color:#f92672">+</span> offsets, mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>    out <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x <span style="color:#f92672">*</span> x, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(out_ptr <span style="color:#f92672">+</span> offsets, out, mask<span style="color:#f92672">=</span>mask)
</span></span></code></pre></div><p>there&rsquo;s definitely some new complexities here but we didn&rsquo;t have to worry about <code>__global__</code>, <code>&lt;&lt;&lt;&gt;&gt;&gt;</code>, or warp math. Triton independently handles tiling, thread/block math, conditional masking (essential in ML), and fast memory loads automatically. In my opinion, it&rsquo;s like when CUDA meets the simplicity of Python.</p>
<p>Let&rsquo;s unwrap the function to understand it more.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">relu_square_kernel</span>(x_ptr, out_ptr, N, BLOCK: tl<span style="color:#f92672">.</span>constexpr):
</span></span></code></pre></div><p>The first part that should jump out to you is the decorator <code>@triton.jit</code> which tells Triton to compile the function using it&rsquo;s just-in-time compiler into a GPU kernel.</p>
<ul>
<li><code>x_ptr</code> and <code>out_ptr</code> are pointers to memory on the GPU like <code>float*</code> in CUDA C.</li>
<li><code>N</code> is the total elements in the input</li>
<li><code>BLOCK: tl.constexpr</code> is a constant known at compile-time and is used for tile/block size</li>
</ul>
<p>In the first three lines</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>pid <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>offsets <span style="color:#f92672">=</span> pid <span style="color:#f92672">*</span> BLOCK <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK)
</span></span><span style="display:flex;"><span>mask <span style="color:#f92672">=</span> offsets <span style="color:#f92672">&lt;</span> N
</span></span></code></pre></div><p><code>pid</code> and <code>offsets</code> set up the block and threads while the mask prevent out-of-bounds access at the end of the array.</p>
<p>We get to the core of our computation (Load, Compute, Store) with the next few lines of code</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(x_ptr <span style="color:#f92672">+</span> offsets, mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>where(x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>, x <span style="color:#f92672">*</span> x, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>tl<span style="color:#f92672">.</span>store(out_ptr <span style="color:#f92672">+</span> offsets, out, mask<span style="color:#f92672">=</span>mask)
</span></span></code></pre></div><p>The first line pulls in the data for thread group from memroy and only loads where mask is set to True.</p>
<p>The second line is the core of the function, its the logic for ReLU squared (x * x if x &gt; 0, else 0.0).</p>
<p>Laslty, <code>tl.store</code> writes the result back to <code>out_ptr</code> safely using the mask.</p>
<p>With this function, we were able to launch an entire kernel that</p>
<ul>
<li>divides work among blocks using <code>program_id</code></li>
<li>loads a slice of input with bounds checking (with mask)</li>
<li>applies a fused op (relu + square)</li>
<li>writes results back safely (vectorized and masked)</li>
</ul>
<p>Triton lets us forget about warp syncs and memory alignment headaches. It autocompiles down to PTX (NVIDIA assembly) giving us CUDA-level performance with NumPy syntax (a game changer for ML).</p>
<hr>
<h2 id="famous-kernels">Famous kernels<a hidden class="anchor" aria-hidden="true" href="#famous-kernels">#</a></h2>
<p>Here, I wanted to discuss some of the most important kernels that reshaped the way we do ML under the hood and improved training &amp; inference times for <em>everyone in the world</em>. These aren’t just optimizations, they’re performance enablers that made LLMs, ViTs, and giant training runs even possible.</p>
<h3 id="flashattention">FlashAttention<a hidden class="anchor" aria-hidden="true" href="#flashattention">#</a></h3>
<p>(Tri Dao et al., 2022 — “Reducing Memory Bottlenecks in Attention”)</p>
<p>FlashAttention is a tiled + fused implementation of the attention mechanism that rewrites how we compute <code>softmax(QKᵀ / √d) · V</code>.</p>
<p>Instead of computing <code>QKᵀ</code>, writing it to global memory, then applying softmax, FlashAttention fuses everything inside shared memory for maximum performance.</p>
<p>Here&rsquo;s what gets fused</p>
<ul>
<li>Tiled matmul: <code>QKᵀ</code></li>
<li>Softmax (numerically stable softmax)</li>
<li>Dropout (only during training)</li>
<li>Final matmul with <code>V</code></li>
</ul>
<p>This means that no intermediate QK matrix ever touches global memory since everything stays in registers / shared memory. This enables longest context lengths without blowing up vRAM.</p>
<blockquote>
<p>FlashAttention showed a 2-4x speedup vs standard attention</p></blockquote>
<p>Now imagine that speedup compounding at <em>every layer</em> of the transformer.</p>
<p>Here&rsquo;s a simplified version of that code written in Triton</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> triton
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> triton.language <span style="color:#66d9ef">as</span> tl
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">flash_attn_kernel</span>(Q_ptr, K_ptr, V_ptr, Out_ptr, seq_len, d_model BLOCK_M: tl<span style="color:#f92672">.</span>constexpr, BLOCK_N: tl<span style="color:#f92672">.</span>constexpr, BLOCK_DMODEL: tl constexpr):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># program IDs = where this block of threads is in Q and K</span>
</span></span><span style="display:flex;"><span>    q_row <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    k_col <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># offset for Q</span>
</span></span><span style="display:flex;"><span>    offs_m <span style="color:#f92672">=</span> q_row <span style="color:#f92672">*</span> BLOCK_M <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_M)
</span></span><span style="display:flex;"><span>    offs_n <span style="color:#f92672">=</span> k_col <span style="color:#f92672">*</span> BLOCK_N <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_N)
</span></span><span style="display:flex;"><span>    offs_d <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_DMODEL)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load tiles from Q and K</span>
</span></span><span style="display:flex;"><span>    Q_tile <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(Q_ptr <span style="color:#f92672">+</span> offs_m[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> d_model <span style="color:#f92672">+</span> offs_d[<span style="color:#66d9ef">None</span>, :])
</span></span><span style="display:flex;"><span>    K_tile <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(K_ptr <span style="color:#f92672">+</span> offs_n[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> d_model <span style="color:#f92672">+</span> offs_d[<span style="color:#66d9ef">None</span>, :])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute dot product Q @ K.T (BLOCK_M x BLOCK_N)</span>
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(Q_tile, tl<span style="color:#f92672">.</span>trans(K_tile))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># scale</span>
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (d_model <span style="color:#f92672">**</span> <span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">*=</span> scale
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># softmax over rows</span>
</span></span><span style="display:flex;"><span>    max_scores <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>max(scores, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> scores <span style="color:#f92672">-</span> max_scores[:, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>exp(scores)
</span></span><span style="display:flex;"><span>    denom <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>sum(scores, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> scores <span style="color:#f92672">/</span> denom[:, <span style="color:#66d9ef">None</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load V tile</span>
</span></span><span style="display:flex;"><span>    V_tile <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(V_ptr <span style="color:#f92672">+</span> offs_n[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> d_model <span style="color:#f92672">+</span> offs_d[<span style="color:#66d9ef">None</span>, :])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># output = scores @ V (BLOCK_M x D_MODEL)</span>
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(scores, V_tile)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># write back to output</span>
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(Out_ptr <span style="color:#f92672">+</span> offs_m[:, <span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> d_model <span style="color:#f92672">+</span> offs_d[<span style="color:#66d9ef">None</span>, :], output)
</span></span></code></pre></div><p>If you read between the lines, it&rsquo;s easy to see that we&rsquo;re still doing the same computation for transfomer style attention. The same <code>Q @ K.T</code> is scaled and softmaxed but everything is optimized to be as quick as possible for the GPU.</p>
<h3 id="fused_add_bias_relu_dropout">fused_add_bias_relu_dropout<a hidden class="anchor" aria-hidden="true" href="#fused_add_bias_relu_dropout">#</a></h3>
<p>This kernel shows up quite literally everywhere inside MLP blocks of a transformer. It fuses a few simple ops but the payoff is massive.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>y = dropout(relu(x + bias))
</span></span></code></pre></div><p>Here, each of these steps alone would load and store a full tensor and cost 3-4 global memory trips. This kernel fuses all 3 by adding bias, applying ReLU, then finally applying dropout mask and scaling. In fused form this operation saves huge on memory bandwith and reduces kernel launch overhead.</p>
<p>This operation compounds - this bias + relu + dropout combination shows up so much in the transformer and speeding up the kernel at the core will happen at <em>every single part</em> of the transformer where this is called. This fusion <strong>saves millions</strong> of tensor element read/writes per forward pass.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fused_add_bias_relu_dropout</span>(x_ptr, bias_ptr, out_ptr, mask_ptr, N, H, p_dropout: tl<span style="color:#f92672">.</span>constexpr, BLOCK_SIZE: tl<span style="color:#f92672">.</span>constexpr):
</span></span><span style="display:flex;"><span>    row_idx <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    cols <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_SIZE)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># offsets</span>
</span></span><span style="display:flex;"><span>    offs <span style="color:#f92672">=</span> row_idx <span style="color:#f92672">*</span> H <span style="color:#f92672">+</span> cols
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load input and bias</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(x_ptr <span style="color:#f92672">+</span> offs, mask<span style="color:#f92672">=</span>cols <span style="color:#f92672">&lt;</span> H, other<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(bias_ptr <span style="color:#f92672">+</span> cols, mask<span style="color:#f92672">=</span>cols <span style="color:#f92672">&lt;</span> H, other<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># add bias and apply ReLU</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> b
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>maximum(x, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># apply dropout</span>
</span></span><span style="display:flex;"><span>    rng <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>rand(cols<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>    keep <span style="color:#f92672">=</span> rng <span style="color:#f92672">&gt;</span> p_dropout
</span></span><span style="display:flex;"><span>    scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> p_dropout)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>where(keep, x <span style="color:#f92672">*</span> scale, <span style="color:#ae81ff">0.0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># store results and dropout mask</span>
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(out_ptr <span style="color:#f92672">+</span> offs, x, mask<span style="color:#f92672">=</span>cols <span style="color:#f92672">&lt;</span> H)
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(mask_ptr <span style="color:#f92672">+</span> offs, keep<span style="color:#f92672">.</span>to(tl<span style="color:#f92672">.</span>int1), mask<span style="color:#f92672">=</span>cols <span style="color:#f92672">&lt;</span> H)
</span></span></code></pre></div><h3 id="fused-layernorm">Fused LayerNorm<a hidden class="anchor" aria-hidden="true" href="#fused-layernorm">#</a></h3>
<p>Just like the kernel above, LayerNorm is critical in every transformer block and shows up multiple times per layer. Similarly, speeding up this kernel will compound into saving us many tensor read/writes.</p>
<p>Standard LayerNorm computes the mean and variance over a vector, normalizes by subtracting the mean and dividing by std, and finally sclaes with <code>γ</code>. If we were to do this regularly, there would be multiple times we would read/write from global memory.</p>
<p>In Fused LayerNorm, we do everything in a single kernel:</p>
<ul>
<li>Loads the vector once</li>
<li>Run Welford&rsquo;s algorithm for numerically stable mean/std</li>
<li>Normalizes and applies affine weights</li>
<li>Writes once</li>
</ul>
<p>Here, we reduce intermediate memory usage, kernel launch latency, and redundant passes over the same vector.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fused_layernorm_kernel</span>(x_ptr, gamma_ptr, beta_ptr, y_ptr, N, EPS,BLOCK_SIZE: tl<span style="color:#f92672">.</span>constexpr):
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># get the program ID (1D grid)</span>
</span></span><span style="display:flex;"><span>    pid <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    block_start <span style="color:#f92672">=</span> pid <span style="color:#f92672">*</span> BLOCK_SIZE
</span></span><span style="display:flex;"><span>    offsets <span style="color:#f92672">=</span> block_start <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_SIZE)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    mask <span style="color:#f92672">=</span> offsets <span style="color:#f92672">&lt;</span> N
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(x_ptr <span style="color:#f92672">+</span> offsets, mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># welford&#39;s algorithm for mean/variance</span>
</span></span><span style="display:flex;"><span>    mean <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>sum(x, mask<span style="color:#f92672">=</span>mask) <span style="color:#f92672">/</span> N
</span></span><span style="display:flex;"><span>    var <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>sum((x <span style="color:#f92672">-</span> mean) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>, mask<span style="color:#f92672">=</span>mask) <span style="color:#f92672">/</span> N
</span></span><span style="display:flex;"><span>    inv_std <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> tl<span style="color:#f92672">.</span>sqrt(var <span style="color:#f92672">+</span> EPS)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># normalize</span>
</span></span><span style="display:flex;"><span>    x_hat <span style="color:#f92672">=</span> (x <span style="color:#f92672">-</span> mean) <span style="color:#f92672">*</span> inv_std
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    gamma <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(gamma_ptr <span style="color:#f92672">+</span> offsets, mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>    beta <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(beta_ptr <span style="color:#f92672">+</span> offsets, mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> x_hat <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> beta
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># write output</span>
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(y_ptr <span style="color:#f92672">+</span> offsets, y, mask<span style="color:#f92672">=</span>mask)
</span></span></code></pre></div><p>On average, we see a 10-20% LayerNorm speedup with this fused kernel, but that gets amplified across <strong>hundreds of layers</strong> in transformers.</p>
<h3 id="fused_multihead_attention_backward">fused_multihead_attention_backward<a hidden class="anchor" aria-hidden="true" href="#fused_multihead_attention_backward">#</a></h3>
<p>Learning about this kernel blew my mind because it fuses the <strong>entire backward pass</strong> of multi-head attention.</p>
<p>This fused op deals with quite literally everyting:</p>
<ul>
<li>Gradients for Q, K, V are calculated</li>
<li>Softmax derivative is applied</li>
<li>Handles matmul and broadcasted add</li>
<li>Backpropagtes through making, scaling, and dropout</li>
</ul>
<p>And all of this is done without touching global memory between steps. This kernel is what enables large-scale training runs like GPT-3. It&rsquo;s crazy to think about that the current state of LLMs would look wildly differnet (not as good) if we didn&rsquo;t have this kernel.</p>
<p>Without this kernel, we would need to recompute or store every intermediate (QKᵀ, softmax, dropout mask) and we would likely stall on memory bandwidth and VRAM usage.</p>
<p>Here&rsquo;s a slightly simpler version of what the actual kernel would look like (I didn&rsquo;t include block tiling / batching of multiple heads).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fused_attention_backward</span>(dY_ptr, Q_ptr, K_ptr, V_ptr, attn_weights_ptr, dQ_ptr, dK_ptr, dV_ptr, BLOCK_SIZE: tl<span style="color:#f92672">.</span>constexpr, HEAD_DIM: tl<span style="color:#f92672">.</span>constexpr, SCALE: tl<span style="color:#f92672">.</span>constexpr):
</span></span><span style="display:flex;"><span>    pid <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>program_id(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># load blocks of dY, Q, K, V</span>
</span></span><span style="display:flex;"><span>    offs <span style="color:#f92672">=</span> pid <span style="color:#f92672">*</span> BLOCK_SIZE <span style="color:#f92672">+</span> tl<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, BLOCK_SIZE)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    dY <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(dY_ptr <span style="color:#f92672">+</span> offs)
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(Q_ptr <span style="color:#f92672">+</span> offs)
</span></span><span style="display:flex;"><span>    K <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(K_ptr <span style="color:#f92672">+</span> offs)
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(V_ptr <span style="color:#f92672">+</span> offs)
</span></span><span style="display:flex;"><span>    attn_weights <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>load(attn_weights_ptr <span style="color:#f92672">+</span> offs)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute grad wrt softmax inputs, ds = softmax_grad(dY, attn_weights) — this must be numerically stable</span>
</span></span><span style="display:flex;"><span>    ds <span style="color:#f92672">=</span> dY <span style="color:#f92672">*</span> attn_weights <span style="color:#f92672">-</span> attn_weights <span style="color:#f92672">*</span> tl<span style="color:#f92672">.</span>sum(dY <span style="color:#f92672">*</span> attn_weights, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># compute dQ, dK, dV</span>
</span></span><span style="display:flex;"><span>    dQ <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(ds, K) <span style="color:#f92672">*</span> SCALE
</span></span><span style="display:flex;"><span>    dK <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(ds<span style="color:#f92672">.</span>T, Q) <span style="color:#f92672">*</span> SCALE
</span></span><span style="display:flex;"><span>    dV <span style="color:#f92672">=</span> tl<span style="color:#f92672">.</span>dot(attn_weights<span style="color:#f92672">.</span>T, dY)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># store results</span>
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(dQ_ptr <span style="color:#f92672">+</span> offs, dQ)
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(dK_ptr <span style="color:#f92672">+</span> offs, dK)
</span></span><span style="display:flex;"><span>    tl<span style="color:#f92672">.</span>store(dV_ptr <span style="color:#f92672">+</span> offs, dV)
</span></span></code></pre></div><p>This kernel fuses everything a backward pass normally splits across 6-10 kernels. You can imagine the compounding speedup.</p>
<hr>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<blockquote>
<p>&ldquo;If you want to have the biggest impact on ML today, write a faster kernel&rdquo;</p></blockquote>
<p>Underneath every flashy AI product demo or multimodal model is a war for compute efficiency. Training future models isn’t just about bigger datasets or better prompts, it’s about fusing ops, minimizing memory transfers, and squeezing every cycle out of your GPU cores.</p>
<p>At the end of the day, the goal is simple: keep as many threads doing useful work as possible (but there&rsquo;s a really fine line). Push too hard and you’ll drown in memory stalls. Push too little and you’re underutilizing, leaving speed and money on the table.</p>
<p>CUDA and Triton aren’t just power tools, they’re leverage. A few lines of well-optimized kernel code can save millions in training costs.</p>
<hr>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
