<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Word2Vec from scratch: Intuition to Implementation | Akhil‚Äôs Blog</title>
<meta name="keywords" content="word2vec, embeddings, nlp">
<meta name="description" content="Q(king) - Q(man) &#43; Q(woman) ‚âà Q(queen) is beautiful but the approach we take to get there is even cooler. In this post, I wanted to dive into how these emergent properties come to be.">
<meta name="author" content="ML Theory &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/word2vec-scratch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/word2vec-scratch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/word2vec-scratch/">
  <meta property="og:site_name" content="Akhil‚Äôs Blog">
  <meta property="og:title" content="Word2Vec from scratch: Intuition to Implementation">
  <meta property="og:description" content="Q(king) - Q(man) &#43; Q(woman) ‚âà Q(queen) is beautiful but the approach we take to get there is even cooler. In this post, I wanted to dive into how these emergent properties come to be.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-01-10T18:12:56-04:00">
    <meta property="article:modified_time" content="2025-01-10T18:12:56-04:00">
    <meta property="article:tag" content="Word2vec">
    <meta property="article:tag" content="Embeddings">
    <meta property="article:tag" content="Nlp">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word2Vec from scratch: Intuition to Implementation">
<meta name="twitter:description" content="Q(king) - Q(man) &#43; Q(woman) ‚âà Q(queen) is beautiful but the approach we take to get there is even cooler. In this post, I wanted to dive into how these emergent properties come to be.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Word2Vec from scratch: Intuition to Implementation",
      "item": "https://akhilvreddy.github.io/posts/word2vec-scratch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Word2Vec from scratch: Intuition to Implementation",
  "name": "Word2Vec from scratch: Intuition to Implementation",
  "description": "Q(king) - Q(man) + Q(woman) ‚âà Q(queen) is beautiful but the approach we take to get there is even cooler. In this post, I wanted to dive into how these emergent properties come to be.",
  "keywords": [
    "word2vec", "embeddings", "nlp"
  ],
  "articleBody": "Have you ever thought about how we teach machines which words are similar ‚Äî and which ones aren‚Äôt? It‚Äôs wild to realize that our phones had smart autocomplete features decades before modern AI tools like ChatGPT became mainstream.\nIn an n-dimensional space, words ‚Äúcling‚Äù to their own axes‚ÄîWord2Vec learns this by pushing true neighbors together and pulling random words apart (contrastive learning).\nWord embeddings laid the foundation for modern NLP models. Before transformer-style architectures took over (with their dynamic embeddings), Word2Vec was the go-to method for capturing the semantic meaning of words through dense (~ 100-300 dim) vectors.\nIn this post, I‚Äôm walking through the intuitions, architecture, methods, and math behind Word2Vec ‚Äî and eventually implementing it from scratch using PyTorch.\nüí° Why Word2Vec? The core idea behind Word2Vec is that words that appear in similar contexts should have similar representations and should appear closer together in higher dimensional vector spaces. This principle ‚Äî often summed up as ‚ÄúYou shall know a word by the company it keeps‚Äù ‚Äî forms the foundation of modern embedding models. Instead of manually designing features, Word2Vec lets the model learn them by predicting words based on context.\nThere are two main ways to solve this problem:\nSkip-gram ‚Äî Predict surrounding context words given the current word. CBOW (Continuous Bag of Words) ‚Äî Predict the current word given its surrounding context. In both cases, the model learns to assign each word a dense vector in a high-dimensional space ‚Äî an embedding ‚Äî such that semantically similar words are close together.\nAt the heart of the model is one hashmap and two embedding matrices:\nLookupTable - Maps words from your vocabulary to a number between 0 and len(vocab)-1 -\u003e each one of those numbers represents a row in the embedding matrices\nE_in ‚Äî Used to represent the input word\nE_out ‚Äî Used to compute predictions over context words\nHere‚Äôs an easy way to visualize a matrix like E_in or E_out.\nBoth are of shape (vocab_size, embedding_dim). During training, we take the dot product of the input word‚Äôs vector from E_in with all rows of E_out, yielding similarity scores. A softmax is typically applied to convert these into probabilities, and the model learns by maximizing the probability of true context words. After training, we often discard E_out and treat E_in as our final word embedding matrix.\nTLDR of this section: We are learning two embedding matrices, E_in and E_out which will contain dense vectors that have information about the semantics of our corpus. We also have a couple different ways to learn these matrices (skip gram, CBOW) which approach the problem in different ways.\nüîç Methods to approach word embeddings The simplest way in training Word2Vec is just having two matrices E_in and E_out and initialize them with Xavier initialization. We use two separate matrices because the role of a word as a center word (input) is distinct from its role as a context word (output). Keeping them separate allows the model to learn directional context ‚Äî e.g., ‚Äúdog‚Äù and ‚Äúbark‚Äù might be close in one direction but not the other.\nFormally, E_in, E_out ‚àà ‚Ñù^{|V| √ó d}, where |V| is the vocabulary size and d is the embedding dimension. Xavier initialization sets each weight to values drawn from a distribution whose variance keeps activations well-scaled, preventing gradients from exploding or vanishing‚Äîeven in this mostly linear architecture.\nFor a quick recap / introduction to Xavier style initialization, I recommend reading this other small post.\nVanilla Training Method (Warning: this will fry your GPU üò≠)\nThe the most straightfowrard way to train Word2Vec is to use the full softmax across the whole vocabulary. Here‚Äôs how is works:\nFor each center word, grab it‚Äôs embedding vector from E_in. Recall that each row in E_in is its own embedding. Compute dot products between this vector and every row in E_out, giving you a tensor of shape (vocab_size,). Apply a softmax over all vocabulary words to get probabilities of what are close. You have the option to sample top k here or just pick the argmax. The model learns by maximizing probability of the true context word -\u003e shows us that we must use cross-entropy loss. Backpropagate and update weights in both embedding matrices (E_in and E_out) Pretty straightforward right? Here‚Äôs the catch:\nEvery forward pass, we are doing a softmax on your entire vocabulary - and your vocabulary might be around 50000+ words in real world datasets. You are essentially doing thousands of unnecessary calculations per word pair, burning your compute and making training brutally slow, though the core weight updates are only from a small handful of words that you have.\nBottom line: This method is great for understanding the core math/backprop and what exactly is going on in Word2Vec but falls short in reality (unless you have time and money to burn).\nSkip Gram with Negative Sampling (Industry standard üî•)\nEssentially we are going to do what we did in the previous example but now we are going to address the bottleneck - the O(V) computation cost per forward pass. But other than that we are going to follow the same general structure.\nIn essance, what SGwNS says is that: ‚ÄúComputing for all 50000+ words in your vocabulary is too expensive. Let‚Äôs just compare the center word to 1 good word (ground truth word) and take a random sample of ‚Äúbad‚Äù words to push them away.‚Äù\nSo you go from softmaxxing 50k values -\u003e 1 positive dot product (your center word \u003c-\u003e the word you want (ground truth)) \u0026 5-20 negative dot products (your center word \u003c-\u003e unrelated words)\nWe can set k = 1 (positive dot product word) + 5-20 (negative dot product words). So now we pay O(k) instead of O(V) - huge gains. (k \u003c \u003c V)\nAnd this is exactly how we beat the bottleneck: contrastive learning.\nBottom line: We are using a smart trick to push away random words that are not meaningful to our current word, hopefully resulting in results similar to the vanilla method. (Spoiler: this trick does work really well and that‚Äôs why it is the industry standard)\nContinuous Bag of Words (also cool but slightly lower performance than SGwNS üôâ)\nThis essentially works in the opposite direction as Skip Gram:\nInstead of predicting cnontext from the center word, you predict the center word from it‚Äôs surrounding context words (kind of like what modern day LLMs do, but on 2015 technology) You take all the context word embeddings, average (or sum) them, and feed the result into the model. The model then tries to guess the center word using either of the methods we talked above: full softmax (like the vanilla version) negative sampling (like SGwNS) Bottom line: Since we are comibing multiple context words into one average (or sum), CBOW trains faster than Skip-Gram and is more stable for smaller datasets. However, the main tradeoff is the fine-grained semantic realtionships because of that averaging. Most of the times, this method performs slightly worse than SGwNS and hence insn‚Äôt that big in practice.\nNext, let‚Äôs talk about loss functions for each method.\nüß† Training Objective / Criterion Let‚Äôs split up the training objective by section (0, 1, or 2) and then we can ask two more sub-questions within each section:\nWhat should our loss function look like to actually pull together words that are similar (or push away words that are not similar)? What is going to get backpropagated once we actually have a scalar loss value? ‚Äì\nVanilla Method Here, we want the model to assign high probability to the correct context word. We don‚Äôt care about other words right now - we just want:\nLow loss when the predicted context word is the actual neighbor High loss when the predicition is far off (e.g., coffee showing up near bird) Since this is a multi-class classification task (predict 1 word out of vocab_size), the natural loss to use is cross-entropy, applied on top of a softmax.\nüßÆ The Goal:\nMaximize the probability of the true context word $w_o$, given the center word $w_c$: $$ P(w_o \\mid w_c) = \\frac{e^{u_o^\\top v_c}}{\\sum_{w=1}^{V} e^{u_w^\\top v_c}} $$\nMath symbols scare me too so I‚Äôll break down what that it saying. The numerator is how similar the correct word is to the center word. The denominator is how similar every word in the vocab is to the center word. So if the correct context word is super close in high dimensional space, we have a larger numerator (good!). If a bunch of random words are also closeby our denominator increases (bad!).\nWe want to maximize this probability so that means making the true word similarity as big as possible while keeping all other dot products in the denominator small. Since this is a probability, cross-entropy loss is the star.\nIt‚Äôs defined as:\n$$ L = -\\log\\ \\Bigl( \\frac{\\exp(u_o^T v_c)} {\\sum_{w=1}^{V} \\exp(u_w^T v_c)} \\Bigr) $$\nThis basically measures ‚Äúhow surprised are we by the correct answer?‚Äù, exactly what we should be asking when we predict a good/bad word. We now have a scalar loss that we can backpropagate through our network (loss.backward())\nDuring backpropagation, only the embeddings of the center word and the true context word actually get updated. All other words are just there to normalize the softmax, they aren‚Äôt contributing anything else. This is why is method is so inefficient - we are doing a HUGE amount of computation just to update 2 weights (crunching 50k numbers to update 2 weights).\nLet‚Äôs fix this O(V) nightmare in the next section with negative sampling. However, we would need to change our training objective now.\n‚Äì\nSkip Gram with Negative Sampling Let‚Äôs address this bottleneck now. Instead of predicting one word out of 50k, we can turn it into a binary classification task.\nIs this a true context word -\u003e label = 1\nIs this a random irrelevant word -\u003e label = 0\nFor every center word, we train on:\nOne positive pair (center, actual context word) k negative pairs (center, randomly sampled ‚Äúnoise‚Äù words) And we are going to use the sigmoid function here instead of softmax so that each pair is treated independently. (This used to confuse me a lot too at first, but I just don‚Äôt have the space to write about it more in this post ‚òπÔ∏è).\nProbability Objective:\nWe want to maximize the probability that a real word pair appears together:\n$$ P(\\text{real} \\mid w_c, w_o) = \\sigma(u_o^\\top v_c) $$\nAnd for a negative (fake) sample, we want:\n$$ P(\\text{fake} \\mid w_c, w_i) = \\sigma(-u_i^\\top v_c) $$\nLoss Function:\n$$ L = -\\log(\\sigma(u_o^\\top v_c)) - \\sum_{i=1}^{k} \\log(\\sigma(-u_i^\\top v_c)) $$\nThis is a really cool loss function because it has two jobs. One for pulling the true word closer together and one for pushing the fake words away. We can see that the first term is calculating the negative log of the similarity between the center word and the context word. This gives us a loss that is lower when they are highly similar - exactly what we wanted.\nThe second term is where we pull in embeddings for k negative sampled words that are not in the context. We want their similarity with the center word to be low - that‚Äôs why we negate the dot product, apply sigmoid (which favors small similarity), and penalize high values. In a nutshell this pushes away random/fake context words, minimizing the chance that the model falsely thinks they belong together.\nThese two terms combined teach the model ‚Äúpull what matters, push what doesn‚Äôt‚Äù.\n$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid function $u_o$ is the embedding of the true context word (positive) $u_i$ are the embeddings of $k$ negative words ‚Äì\nContinous Bag of Words (CBOW) You probably get the gist of what we do now: understand our probability objective and based on that, make a loss objective.\nHere, we have to be ready to flip our heads in the opposite way. We are going to predict the center word now, given a set of context words.\nThe probability objective:\n$$ P(w_c \\mid w_{c - m}, \\dots, w_{c + m}) = \\frac{e^{u_c^\\top \\bar{v}}}{\\sum_{w=1}^{V} e^{u_w^\\top \\bar{v}}} $$\nOur loss function (with negative sampling) is:\n$$ L = -\\log(\\sigma(u_c^\\top \\bar{v})) - \\sum_{i=1}^{k} \\log(\\sigma(-u_i^\\top \\bar{v})) $$\n$\\sigma(x) = \\frac{1}{1 + e^{-x}}$ ‚Äî the sigmoid function, which squashes scores into a range between 0 and 1. We use it to convert similarity scores into probabilities. $u_c$ ‚Äî the output embedding of the center word (the word we‚Äôre trying to predict). Think of this like a ‚Äútarget‚Äù vector. $\\bar{v}$ ‚Äî the average of the input (context) embeddings surrounding the center word. This is what we feed in to predict the center. $u_i$ ‚Äî the output embeddings of $k$ negative samples (randomly chosen words that are not the true center). We want the model to push these words away from the context representation. üõ†Ô∏è Coding it out I‚Äôll be putting in snippets here but to view the entire repository, please go over to github.\ncoming tmr\nü¶æ Tradeoffs Recap I‚Äôll be putting in snippets here but to view the entire repository, please go over to github.\ncoming tmr\n‚öôÔ∏è TL;DR / Summary Word2Vec may be old school now, but it‚Äôs still one of the most elegant and interpretable ideas in the NLP world. It‚Äôs also a great way to build up your ML intuition, understand the baby steps towards contrastive learning, and get comfortable with training loops.\nIf you enjoyed reading this feel free to star this repo on github üôÇ\n",
  "wordCount" : "2252",
  "inLanguage": "en",
  "datePublished": "2025-01-10T18:12:56-04:00",
  "dateModified": "2025-01-10T18:12:56-04:00",
  "author":{
    "@type": "Person",
    "name": "ML Theory + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/word2vec-scratch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil‚Äôs Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil‚Äôs Blog (Alt + H)">Akhil‚Äôs Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Word2Vec from scratch: Intuition to Implementation
    </h1>
    <div class="post-meta"><span title='2025-01-10 18:12:56 -0400 -0400'>January 10, 2025</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;ML Theory &#43; Code

</div>
  </header> 
  <div class="post-content"><p>Have you ever thought about how we teach machines which words are similar ‚Äî and which ones aren‚Äôt? It&rsquo;s wild to realize that our phones had smart autocomplete features <em>decades</em> before modern AI tools like ChatGPT became mainstream.</p>
<div align="center">
  <img src="/images/word2vec_1.jpg" alt="Skip-gram architecture" width="600"/>
  <p>In an <code>n</code>-dimensional space, words ‚Äúcling‚Äù to their own axes‚ÄîWord2Vec learns this by pushing true neighbors together and pulling random words apart (contrastive learning).</p>
</div>
<p>Word embeddings laid the foundation for modern NLP models. Before transformer-style architectures took over (with their dynamic embeddings), Word2Vec was the go-to method for capturing the semantic meaning of words through dense (~ 100-300 dim) vectors.</p>
<p>In this post, I&rsquo;m walking through the <strong>intuitions, architecture, methods, and math behind Word2Vec</strong> ‚Äî and eventually implementing it from scratch using PyTorch.</p>
<hr>
<h2 id="-why-word2vec">üí° Why Word2Vec?<a hidden class="anchor" aria-hidden="true" href="#-why-word2vec">#</a></h2>
<p>The core idea behind Word2Vec is that <strong>words that appear in similar contexts should have similar representations</strong> and should appear closer together in higher dimensional vector spaces. This principle ‚Äî often summed up as <em>‚ÄúYou shall know a word by the company it keeps‚Äù</em> ‚Äî forms the foundation of modern embedding models. Instead of manually designing features, Word2Vec lets the model <em>learn</em> them by predicting words based on context.</p>
<p>There are two main ways to solve this problem:</p>
<ul>
<li><strong>Skip-gram</strong> ‚Äî Predict surrounding context words given the current word.</li>
<li><strong>CBOW (Continuous Bag of Words)</strong> ‚Äî Predict the current word given its surrounding context.</li>
</ul>
<p>In both cases, the model learns to assign each word a dense vector in a high-dimensional space ‚Äî an <strong>embedding</strong> ‚Äî such that semantically similar words are close together.</p>
<p>At the heart of the model is one hashmap and two embedding matrices:</p>
<ul>
<li>
<p><strong>LookupTable</strong> - Maps words from your vocabulary to a number between <code>0</code> and <code>len(vocab)-1</code> -&gt; each one of those numbers represents a row in the embedding matrices</p>
</li>
<li>
<p><strong>E_in</strong> ‚Äî Used to represent the input word</p>
</li>
<li>
<p><strong>E_out</strong> ‚Äî Used to compute predictions over context words</p>
</li>
</ul>
<p>Here&rsquo;s an easy way to visualize a matrix like <strong>E_in</strong> or <strong>E_out</strong>.</p>
<p>Both are of shape <em>(vocab_size, embedding_dim)</em>. During training, we take the dot product of the input word‚Äôs vector from <code>E_in</code> with all rows of <code>E_out</code>, yielding similarity scores. A softmax is typically applied to convert these into probabilities, and the model learns by maximizing the probability of true context words. After training, we often discard <code>E_out</code> and treat <code>E_in</code> as our final word embedding matrix.</p>
<p>TLDR of this section:
We are learning two embedding matrices, <strong>E_in</strong> and <strong>E_out</strong> which will contain dense vectors that have information about the semantics of our corpus. We also have a couple different ways to learn these matrices (skip gram, CBOW) which approach the problem in different ways.</p>
<hr>
<h2 id="-methods-to-approach-word-embeddings">üîç Methods to approach word embeddings<a hidden class="anchor" aria-hidden="true" href="#-methods-to-approach-word-embeddings">#</a></h2>
<p>The simplest way in training Word2Vec is just having two matrices <em>E_in</em> and <em>E_out</em> and initialize them with Xavier initialization. We use <strong>two separate matrices</strong> because the role of a word as a center word (input) is distinct from its role as a context word (output). Keeping them separate allows the model to learn directional context ‚Äî e.g., ‚Äúdog‚Äù and ‚Äúbark‚Äù might be close in one direction but not the other.</p>
<p>Formally, <code>E_in, E_out ‚àà ‚Ñù^{|V| √ó d}</code>, where |V| is the vocabulary size and <em>d</em> is the embedding dimension. Xavier initialization sets each weight to values drawn from a distribution whose variance keeps activations well-scaled, preventing gradients from exploding or vanishing‚Äîeven in this mostly linear architecture.</p>
<p>For a quick recap / introduction to Xavier style initialization, I recommend reading this other small <a href="https://www.geeksforgeeks.org/deep-learning/xavier-initialization/">post</a>.</p>
<ol start="0">
<li>
<p>Vanilla Training Method (Warning: this will fry your GPU üò≠)</p>
<p>The the most straightfowrard way to train Word2Vec is to use the full softmax across the whole vocabulary. Here&rsquo;s how is works:</p>
<ul>
<li>For each center word, grab it&rsquo;s embedding vector from <em>E_in</em>. Recall that each row in <em>E_in</em> is its own embedding.</li>
<li>Compute dot products between this vector and <strong>every</strong> row in <em>E_out</em>, giving you a tensor of shape (vocab_size,).</li>
<li>Apply a softmax over all vocabulary words to get probabilities of what are close. You have the option to sample top k here or just pick the argmax.</li>
<li>The model learns by maximizing probability of the true context word -&gt; shows us that we must use <em>cross-entropy loss</em>.</li>
<li>Backpropagate and update weights in both embedding matrices (<em>E_in</em> and <em>E_out</em>)</li>
</ul>
<p>Pretty straightforward right? Here&rsquo;s the catch:</p>
<p>Every forward pass, we are doing a softmax on your entire vocabulary - and your vocabulary might be around 50000+ words in real world datasets. You are essentially doing thousands of unnecessary calculations per word pair, burning your compute and making training brutally slow, though the core weight updates are only from a small handful of words that you have.</p>
<p><strong>Bottom line:</strong>
This method is great for understanding the core math/backprop and what exactly is going on in Word2Vec but falls short in reality (unless you have time and money to burn).</p>
</li>
<li>
<p>Skip Gram with Negative Sampling (Industry standard üî•)</p>
<p>Essentially we are going to do what we did in the previous example but now we are going to address the bottleneck - the O(V) computation cost per forward pass. But other than that we are going to follow the same general structure.</p>
<p>In essance, what SGwNS says is that:
&ldquo;Computing for all 50000+ words in your vocabulary is too expensive. Let&rsquo;s just compare the center word to 1 good word (ground truth word) and take a random sample of &ldquo;bad&rdquo; words to push them away.&rdquo;</p>
<p>So you go from softmaxxing 50k values -&gt; 1 positive dot product (your center word &lt;-&gt; the word you want (ground truth)) &amp; 5-20 negative dot products (your center word &lt;-&gt; unrelated words)</p>
<p>We can set <code>k</code> = <code>1 (positive dot product word)</code> + <code>5-20 (negative dot product words)</code>. So now we pay O(k) instead of O(V) - huge gains. (k &lt; &lt; V)</p>
<p>And this is exactly how we beat the bottleneck: contrastive learning.</p>
<p><strong>Bottom line:</strong>
We are using a smart trick to push away random words that are not meaningful to our current word, hopefully resulting in results similar to the vanilla method. (Spoiler: this trick does work really well and that&rsquo;s why it is the industry standard)</p>
</li>
<li>
<p>Continuous Bag of Words (also cool but slightly lower performance than SGwNS üôâ)</p>
<p>This essentially works in the opposite direction as Skip Gram:</p>
<ul>
<li>Instead of predicting cnontext from the center word, you predict the center word from it&rsquo;s surrounding context words (kind of like what modern day LLMs do, but on 2015 technology)</li>
<li>You take all the context word embeddings, average (or sum) them, and feed the result into the model.</li>
<li>The model then tries to guess the center word using either of the methods we talked above:
<ul>
<li>full softmax (like the vanilla version)</li>
<li>negative sampling (like SGwNS)</li>
</ul>
</li>
</ul>
<p><strong>Bottom line:</strong>
Since we are comibing multiple context words into one average (or sum), CBOW trains faster than Skip-Gram and is more stable for smaller datasets. However, the main tradeoff is the fine-grained semantic realtionships because of that averaging. Most of the times, this method performs slightly worse than SGwNS and hence insn&rsquo;t that big in practice.</p>
</li>
</ol>
<p>Next, let&rsquo;s talk about loss functions for each method.</p>
<hr>
<h2 id="-training-objective--criterion">üß† Training Objective / Criterion<a hidden class="anchor" aria-hidden="true" href="#-training-objective--criterion">#</a></h2>
<p>Let&rsquo;s split up the training objective by section (0, 1, or 2) and then we can ask two more sub-questions within each section:</p>
<ul>
<li>What should our loss function look like to actually pull together words that are similar (or push away words that are not similar)?</li>
<li>What is going to get backpropagated once we actually have a scalar loss value?</li>
</ul>
<p>&ndash;</p>
<ol start="0">
<li>Vanilla Method</li>
</ol>
<p>Here, we want the model to assign <strong>high probability to the correct context word</strong>. We don&rsquo;t care about other words right now - we just want:</p>
<ul>
<li>Low loss when the predicted context word is the <em>actual</em> neighbor</li>
<li>High loss when the predicition is far off (e.g., <code>coffee</code> showing up near <code>bird</code>)</li>
</ul>
<p>Since this is a <strong>multi-class classification</strong> task (predict 1 word out of vocab_size), the natural loss to use is <strong>cross-entropy</strong>, applied on top of a <strong>softmax</strong>.</p>
<p>üßÆ The Goal:</p>
<blockquote>
<p>Maximize the probability of the true context word $w_o$, given the center word $w_c$:
$$
P(w_o \mid w_c) = \frac{e^{u_o^\top v_c}}{\sum_{w=1}^{V} e^{u_w^\top v_c}}
$$</p></blockquote>
<p>Math symbols scare me too so I&rsquo;ll break down what that it saying. The numerator is how similar the correct word is to the center word. The denominator is how similar every word in the vocab is to the center word. So if the correct context word is super close in high dimensional space, we have a larger numerator (good!). If a bunch of random words are also closeby our denominator increases (bad!).</p>
<p>We want to maximize this probability so that means making the true word similarity as big as possible while keeping all other dot products in the denominator small. Since this is a probability, <em>cross-entropy loss</em> is the star.</p>
<p>It&rsquo;s defined as:</p>
<p>$$
L = -\log\ \Bigl(
\frac{\exp(u_o^T v_c)}
{\sum_{w=1}^{V} \exp(u_w^T v_c)}
\Bigr)
$$</p>
<p>This basically measures &ldquo;how surprised are we by the correct answer?&rdquo;, exactly what we should be asking when we predict a good/bad word. We now have a scalar loss that we can backpropagate through our network (loss.backward())</p>
<p>During backpropagation, only the embeddings of the center word and the true context word actually get updated. All other words are just there to normalize the softmax, they aren&rsquo;t contributing anything else. This is why is method is so inefficient - we are doing a HUGE amount of computation just to update 2 weights (crunching 50k numbers to update 2 weights).</p>
<p>Let&rsquo;s fix this <strong>O(V) nightmare</strong> in the next section with negative sampling. However, we would need to change our training objective now.</p>
<p>&ndash;</p>
<ol>
<li>Skip Gram with Negative Sampling</li>
</ol>
<p>Let&rsquo;s address this bottleneck now. Instead of predicting one word out of 50k, we can turn it into a binary classification task.</p>
<blockquote>
<p>Is this a <em>true context word</em> -&gt; label = 1</p></blockquote>
<blockquote>
<p>Is this a <em>random irrelevant word</em> -&gt; label = 0</p></blockquote>
<p>For every center word, we train on:</p>
<ul>
<li>One positive pair (center, actual context word)</li>
<li><code>k</code> negative pairs (center, randomly sampled &ldquo;noise&rdquo; words)</li>
</ul>
<p>And we are going to use the sigmoid function here instead of softmax so that each pair is treated independently. (This used to confuse me a lot too at first, but I just don&rsquo;t have the space to write about it more in this post ‚òπÔ∏è).</p>
<p><strong>Probability Objective:</strong></p>
<p>We want to maximize the probability that a real word pair appears together:</p>
<p>$$
P(\text{real} \mid w_c, w_o) = \sigma(u_o^\top v_c)
$$</p>
<p>And for a negative (fake) sample, we want:</p>
<p>$$
P(\text{fake} \mid w_c, w_i) = \sigma(-u_i^\top v_c)
$$</p>
<p><strong>Loss Function:</strong></p>
<p>$$
L = -\log(\sigma(u_o^\top v_c)) - \sum_{i=1}^{k} \log(\sigma(-u_i^\top v_c))
$$</p>
<p>This is a really cool loss function because it has two jobs. One for pulling the true word closer together and one for pushing the fake words away. We can see that the first term is calculating the negative log of the similarity between the center word and the context word. This gives us a loss that is lower when they are highly similar - exactly what we wanted.</p>
<p>The second term is where we pull in embeddings for k negative sampled words that are not in the context. We want their similarity with the center word to be low - that&rsquo;s why we negate the dot product, apply sigmoid (which favors small similarity), and penalize high values. In a nutshell this pushes away random/fake context words, minimizing the chance that the model falsely thinks they belong together.</p>
<p>These two terms combined teach the model &ldquo;pull what matters, push what doesn&rsquo;t&rdquo;.</p>
<ul>
<li>$\sigma(x) = \frac{1}{1 + e^{-x}}$ is the sigmoid function</li>
<li>$u_o$ is the embedding of the true context word (positive)</li>
<li>$u_i$ are the embeddings of $k$ negative words</li>
</ul>
<p>&ndash;</p>
<ol start="2">
<li>Continous Bag of Words (CBOW)</li>
</ol>
<p>You probably get the gist of what we do now: understand our probability objective and based on that, make a loss objective.</p>
<p>Here, we have to be ready to flip our heads in the opposite way. We are going to predict the center word now, given a set of context words.</p>
<p>The probability objective:</p>
<p>$$
P(w_c \mid w_{c - m}, \dots, w_{c + m}) = \frac{e^{u_c^\top \bar{v}}}{\sum_{w=1}^{V} e^{u_w^\top \bar{v}}}
$$</p>
<p>Our loss function (with negative sampling) is:</p>
<p>$$
L = -\log(\sigma(u_c^\top \bar{v})) - \sum_{i=1}^{k} \log(\sigma(-u_i^\top \bar{v}))
$$</p>
<ul>
<li>$\sigma(x) = \frac{1}{1 + e^{-x}}$ ‚Äî the sigmoid function, which squashes scores into a range between 0 and 1. We use it to convert similarity scores into probabilities.</li>
<li>$u_c$ ‚Äî the output embedding of the center word (the word we‚Äôre trying to predict). Think of this like a ‚Äútarget‚Äù vector.</li>
<li>$\bar{v}$ ‚Äî the average of the input (context) embeddings surrounding the center word. This is what we feed in to predict the center.</li>
<li>$u_i$ ‚Äî the output embeddings of $k$ negative samples (randomly chosen words that are not the true center). We want the model to push these words away from the context representation.</li>
</ul>
<hr>
<h2 id="-coding-it-out">üõ†Ô∏è Coding it out<a hidden class="anchor" aria-hidden="true" href="#-coding-it-out">#</a></h2>
<p>I&rsquo;ll be putting in snippets here but to view the entire repository, please go over to <a href="">github</a>.</p>
<p><em>coming tmr</em></p>
<hr>
<h2 id="-tradeoffs-recap">ü¶æ Tradeoffs Recap<a hidden class="anchor" aria-hidden="true" href="#-tradeoffs-recap">#</a></h2>
<p>I&rsquo;ll be putting in snippets here but to view the entire repository, please go over to <a href="">github</a>.</p>
<p><em>coming tmr</em></p>
<hr>
<h2 id="-tldr--summary">‚öôÔ∏è TL;DR / Summary<a hidden class="anchor" aria-hidden="true" href="#-tldr--summary">#</a></h2>
<p>Word2Vec may be old school now, but it&rsquo;s still one of the most elegant and interpretable ideas in the NLP world. It&rsquo;s also a great way to build up your ML intuition, understand the baby steps towards contrastive learning, and get comfortable with training loops.</p>
<p>If you enjoyed reading this feel free to star this <a href="">repo</a> on github üôÇ</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://akhilvreddy.github.io/tags/word2vec/">Word2vec</a></li>
      <li><a href="https://akhilvreddy.github.io/tags/embeddings/">Embeddings</a></li>
      <li><a href="https://akhilvreddy.github.io/tags/nlp/">Nlp</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
