<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Designing AlphaTicTacToe (and AlphaTicTacToeZero) | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.">
<meta name="author" content="ML Theory">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="Designing AlphaTicTacToe (and AlphaTicTacToeZero)">
  <meta property="og:description" content="AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-02T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Designing AlphaTicTacToe (and AlphaTicTacToeZero)">
<meta name="twitter:description" content="AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Designing AlphaTicTacToe (and AlphaTicTacToeZero)",
      "item": "https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Designing AlphaTicTacToe (and AlphaTicTacToeZero)",
  "name": "Designing AlphaTicTacToe (and AlphaTicTacToeZero)",
  "description": "AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: Even though RL feels kind of quiet nowadays outside of RLHF, it still blows my mind that agents can learn purely from experience — no labels, just trial and error. It’s messy, unstable, and hard to debug, but there’s something so raw and human-like about it. That’s what I’m chasing by building AlphaTicTacToe from scratch.\n[UPDATE]: The repsitory is completed and the corresponding PyTorch implementation is available here.\nThoughts on RL Recently, I don’t see reinforcement learning being used that much outside of things like RLHF. Before LLMs took off, RL was the frontier and showed us how powerful scaling up machine learning problems can be.\nRL is one of those fields that feels like magic when you first encounter it. Give an agent some rewards, throw it into an environment, and somehow, over time, it figures out how to win by minimizing loss.\nBehind that magic is a rigorous and delicate balance between trial and error learning, exploration, value estimation, and function approximation.\nWhat’s always fascinated me about reinforcement learning is how well it learns without labels. Learning purely from experience helps the model come up with it’s own features in a cool way, since it finds a structure in space that minimizes loss.\nIn some ways RL feels way more human-like. But learning from no labels and setting up the environment makes it hard to debug, unstable to train, and overly sensitive to design decisions. By designing and building AlphaTicTacToe I hope to learn more about the training and learning process in reinforcment learning.\nWhy TicTacToe? How does it compare to Go? TicTacToe might seem like a joke compared to Go, and it kind of is. Go has more board configurations than atoms in the universe whereas TicTacToe has just 765 unique board states. But that simplicity is what I want to take advantage of.\nAlphaGo combined deep learning, reinforcement learning, and tree search to beat world champions. My goal wasn’t to replicate that scale, it was to replicate the ideas. TicTacToe gives me a controlled, fully observable world where I can implement:\nSelf-play training A policy network A value network A touch of MCTS In other words, a tiny lab to test the same techniques that powered AlphaGo and AlphaZero but in minutes instead of months.\nCore concepts we’ll use Before we dive into architecture and code, here are the main concepts this project draws from:\nPolicy Network A neural net that maps board states → probabilities over next moves. This is how the agent decides what to do given the current input.\nHere’s a simple example for our case:\nPolicy Network input: Current board (a 3x3 grid encoded as a 1D tensor) Policy Network output: Probability distribution over all 9 positions If we had a 3x3 board like\n[ 0, 1, -1, 0, -1, 1, 1, 0, 0] where\n1 = our move (X) -1 = opponent move (O) 0 = empty cell Our policy network would return\n[0.05, 0.01, 0.00, 0.20, 0.00, 0.01, 0.40, 0.03, 0.30] which is a probability distribution over the whole input (what we usually see after a softmax). The above means that the network thinks\nMove 6 (index 6) is most likely ot be the best (40% chance) Move 8 is the next best (30% chance) Moves like 1, 2, 4, 5, 6 are either illegal (the board isn’t empty there) or bad (game losing moves) hence the 0 - 0.01% The agent would sample from this distribution or choose the argmax depending on the setting. Here’s an NLP analogy: does this kind of remind you of picking the right token after scaled attention (argmax, beam search, etc.)? It’s the same concept but now applied to moves in a game instead of picking the next best token.\nValue Network This is another neural net that maps board states → estimated outcome (win/loss/draw). It helps the agent understand which states are “good” and which states are “bad” (moves that will most likely to hurt your chances of winning).\nThink of this like asking:\n“If I’m in this position and play optimally from here, how likely am I to win?”\nLet’s dive deeper\nValue Network input: Current board (a 3x3 grid encoded as a tensor, just like what we saw above) Value Network output: A single scalar value between -1 and 1 describing winning likelihood of winning The scalar result of a value network is actually pretty easy to understand. A value of 1.0 means guaranteed win, a vaue of -1.0 means we’re definitely going to lose, and 0.0 means a draw or unclear outcome.\nThese two (policy and value networks) work together really well. We can look at them like the following.\nIf you’re standing at a fork in the road, the policy network says “turn left, I think it leads to the best destination” and the value network says “this spot you’re in right now? I’d rate it a 7/10. Not bad but not amazing either.” You would know to go left because of what your policy network says and given the fact that your value network thinks you can get to a better position (you want to get to a 10, not a 7).\nThis combination of knowing what to choose and instantly evaluating your current state helps agents look ahead smartly like humans do when thinking “if I go here, what’s likely to happen?”\nThis picture shows how the policy network and value network work in Go, but the same concept applies in tic-tac-toe.\nSelf-Play (for TicTacToeZero) In self play, the agent learns by playing against itself, without any human data. This is what made AlphaZero groundbreaking because it kind of “taught itself the game”. I’ll go more into this when we prepare for training.\nReward Signal This is super simple. For every game, we can assign\n+1 for a win -1 for a loss 0 for a draw This reward signal is sparse and very delayed (only received at the end of the game) but it encodes everything that matters (win, lose, draw).\nThough this is such a weak signal, the agent learns a lot through self-play, MCTS, and policy/value networks. This teaches the agent to evaluate board states and select promising moves, even when intermediate rewards aren’t available.\nFor these games:\nWinning is the only thing that matters MCTS provides intermediate guidance via simulation The networks generalize from outcomes to board positions You don’t need dense rewards if your agent is powerful enough to learn from sparse ones.\nExploration Strategy Early on, the agent needs randomness (like ε-greedy) to discover better moves before we move on to doing MCTS (Monte Carlo Tree Search, more on this later). This randomness counts as exploration noise and is crucial early in training.\nIn the beginning,\nYour networks are terrible (random weights) Terrible networks won’t explore less obvious (but potentially better) strategies You end up in a toxic feedback loop:\nBad network → bad MCTS → bad self-play → no learning\nWith ε-greedy, you pick a random move instead of the best one with probability ε. This forces the agent to try moves that currently look suboptimal, but might actually\nBe super helpful down the line Help MCTS explore deeper, richer branches of the tree Expose the neural net to more diverse set of board states To use a chess analogy: this is like giving the agent a chance to learn castling. At first, it looks like a wasted move. But over time, the agent realizes it’s actually super powerful for long-term king safety and control.\nMonte Carlo Tree Search (MCTS) I’ve been touching on MCTS a lot but haven’t explained it well yet. Here’s a proper breakdown.\nMCTS is a search algorithm that helps an agent choose the best move by running lots of simulations, keeping track of which actions led to wins, and gradually growing a search tree based on experience, not hardcoded rules. Think of it as the scalable alternative to Q-tables used in tabular RL — but for massive action spaces.\nThe core idea is:\nFrom the current board, simulate random (or guided) games to the end Record which moves led to wins more often Keep exploring promising branches of the tree Over time, we form a biased tree that favors strong moves.\nBy repeating these steps thousands of times, MCTS builds a tree that implicitly “learns” which branches are worth pursuing — without ever training a model.\nHowever, in AlphaZero, MCTS is used not just to pick moves, but to generate training data for the neural networks as well.\nLet’s take a look at one whole MCTS loop.\nSelection We start at the root (current board), and move down the tree by picking the most promising child using something like the UCB (Upper Confidence Bound) formula.\ndef select(node): while node.is_fully_expanded(): node = max(node.children, key=ucb_score) return node def ucb_score(child): # ucb1: exploitation + exploration C = 1.4 # exploration constant return (child.wins / child.visits) + C * math.sqrt(math.log(child.parent.visits) / child.visits) Expansion When you reach a node that isn’t fully explored, try a new move from that state and add a child node. def expand(node): untried_moves = node.get_untried_moves() move = random.choice(untried_moves) next_state = node.state.play(move) child_node = TreeNode(state=next_state, parent=node, move=move) node.children.append(child_node) return child_node Simulation (Rollout) From that child node, play random moves until the game ends. def simulate(state): current_state = state.clone() while not current_state.is_terminal(): move = random.choice(current_state.get_legal_moves()) current_state = current_state.play(move) return current_state.get_result() # +1, 0, or -1 Backpropagation Propagate the result up the tree, updating the win/visit stats for every node along the way. This is not backprop in the regular sense - there’s no gradients, no weight updates, no chain rule. We’re just logging outcomes like a scoreboard. This is enough for us because MCTS just needs to show us viable paths.\ndef backpropagate(node, result): while node is not None: node.visits += 1 node.wins += result result *= -1 # flip result for opponent's perspective node = node.parent These 4 steps combined become one loop (or “epoch” if you want to think about it in more of a DL sense). Repeating this thousands of times and we end up with a smart tree that recommends the best next move. This essentially becomes the “brain” of the whole model.\nHere’s what it looks like put together\ndef mcts_search(root, num_simulations): for _ in range(num_simulations): node = select(root) child = expand(node) result = simulate(child.state) backpropagate(child, result) return max(root.children, key=lambda c: c.visits).move not too bad right?\nRun this thousands of times and you get a smart tree that recommends the best next move based on experience, not brute force. This becomes the agent’s “thinking loop.”\nIn Go, there are ~10^170 board states, making brute force search absolutely useless. MCTS changed the game by allowing us to selectively explore only promising lines, guided by past outcomes instead of expanding every possible branch like in BFS.\nIn TicTacToe, the board is tiny. We can do a full-depth BFS style search if we reallly wanted to. I still wanted to see if I could use MCTS + neural networks in this smaller world to learn like AlphaGo did, instead of hardcoding perfect moves with exhaustive search algorithms.\nBecause of how different Go and TicTacToe are, I’ll have to use a differenet version of MCTS.\nInstead of a deep tree, I ran 50 simulations per move. I used random rollouts (no value network at first) This is definitley still overkill for TicTacToe (which can be done with an exhaustive search) but I wanted to model a network that could think and reason like AlphaGo on a smaller scale.\nDesign Choices for TicTacToe TicTacToe is very simple but designing a learning agent still means making real decisions around how to represent the game, structure the model, and define the environment. Here’s how I approached it\nState Representation I touched on this earlier - a simple 9 element 1D tensor works well here in my opinion.\n[ 0, 1, -1, 0, -1, 1, 1, 0, 0] When I was thinking about different representations, ChatGPT recommended me to use one-hot encoding (3 channels: player, opponent, empty) but I wanted to keep this as simple as possible since the board is already tiny.\nAction Space There are 9 possible moves (indexes 0-8) where each corresponds to placing your symbol in one of the 9 cells.\nIf the cell is already occupied → the move is illegal Policy network gives us a probability distribution over 9 cells During argmax (or sampling) selection, we mask the illegal moves so the agent doesn’t try to play in filled spaces This way, the agent doesn’t get punished for illegal moves, it just learns not to choose them with masking.\nNetwork Architecture Both the policy network and the value network would become small MLPs (multi-layer perceptrons).\nPolicy Network\nnn.Sequential( nn.Linear(9, 64), nn.ReLU(), nn.Linear(64, 9) # raw logits → softmax ) Let’s try to make sense of the hidden layer sizes. We first have an input of 9 because our input tensor is of size (9,) based on what we decided earlier and only has values of 1, -1, or 0.\nThe hidden layer size is 64. I chose this number as a sweet spot. It’s large enough to allow the network to learn useful patterns and combinations but small enough to avoid overfitting or unnecessary complexity for such a tiny input space.\nApplying a ReLU gives us non linearity instead of just linear transformations.\nWe end with squeezing it back down to an output of 9. The resulting 9 logits are going to be your probabilities as we talked about that the policy network return, one per board cell.\nThe policy network’s job is simple: take in the current board and return a probability distribution over all 9 moves.\nValue Network\nnn.Sequential( nn.Linear(9, 64), nn.ReLU(), nn.Linear(64, 1), # scalar value nn.Tanh() # outputs belong to [-1, 1] (tanh applied on resulting logit for \"squashing\" to the -1, 1 range) ) Just like the policy network, we have an input size of 9 and I still went with 64 as a sweet spot. The main change is the last two lines.\nOur output layer is now just a single scalar value that estimates how favorable the current state is for the agent. This final output is then passed through a tanh squeezing it into the range [-1, 1] (as explained before). +1 would be a very confident win and -1 would be a certain loss.\nThis output is used to help guide MCTS and inform learning. It acts like a learned evaluation function that replaces brute-force rollout results.\nThe value network’s job is simpler: take in the current board and return a score for how likely you think this will win the game in the range [-1, 1] (tanh helps with this).\nGame Logic Before we can train an agent to play TicTacToe, we need to build the environment it lives in - the rules, state transitions, and reward mechanics. Here’s how I structured the game engine that powers both self-play and MCTS.\nApplying Moves Each player takes turns selecting a move (an index from 0 to 8). We check if the move is legal and update the board accordingly.\ndef apply_move(self, move): if self.board[move] != 0: raise ValueError(\"Illegal move!\") self.board[move] = self.current_player self.current_player *= -1 # switch turns Terminal Check After every move I check for a win, loss, or draw. If the game is over, I return the final result\ndef check_winner(self): winning_lines = [ [0, 1, 2], [3, 4, 5], [6, 7, 8], # rows [0, 3, 6], [1, 4, 7], [2, 5, 8], # cols [0, 4, 8], [2, 4, 6] # diagonals ] for line in winning_lines: values = [self.board[i] for i in line] if sum(values) == 3: return 1 # X wins elif sum(values) == -3: return -1 # O wins if 0 not in self.board: return 0 # Draw return None # Game continues Resetting the game Each new game starts with a clean board and the first player to move\ndef reset(self): self.board = [0] * 9 self.current_player = 1 This reset function is essential for:\nSelf-play, where each game is an independent episode Training loops, where consistency across episodes matters Without resetting, the leftover board state or player turn bugs could corrupt learning.\nCloning for MCTS MCTS doesn’t need to reset the board, it needs to duplicate the current game state to run thousands of rollouts from it.\ndef clone(self): cloned = TicTacToe() cloned.board = self.board[:] cloned.current_player = self.current_player return cloned Between .reset() and .clone() we’ve got a clean way to simulate and learn without state leaks or bugs.\nIntegration with Agent At each step\nThe agent observes the current board state\nIt runs MCTS, using the policy network to guide exploration\nMCTS outputs a move distribution (based on visit counts)\nThe agent samples a move from that distribution (or just takes argmax)\nThe selected move is applied to the board\nThe tuple (state, move_probs, final_result) is saved for training\nHere’s what I optimized for:\nKeeping everything small, fast, and interpretable Using self-play, policy/value, and MCTS to mimic AlphaGo logic Letting the agent learn from scratch rather than relying on hardcoded strategies Training the Agent There are two main paradigms for training a game playing agent like this:\nLearn via human gameplay (supervised learning) Learn from scratch via self-play (reinforcement learning) The first is what AlphaGo did and the second is what AlphaZero did.\nIn this project, I chose to go with self-play. It’s slower and more complex than imitation learning, but it gives me a more scalable setup, an agent that can discover novel strategies on its own, and a deeper understanding of how learning emerges from interaction.\nWhen playing games against itself, for each move it makes during a game, we store:\nThe current board state The move distribution from MCTS (how often each move was visited) The final result of the game (win/loss/draw) This gives us training examples in the form:\n(state, mcts_probs, final_result) and we then batch these to become\n(batch_size, state, mcts_probs, final_result) We can feed this data into a trianing loop after generating a batch of games. We train both the policy network and the value network using gradient descent.\nfor epoch in range(num_epochs): for batch in replay_buffer: policy_pred = policy_net(state) value_pred = value_net(state) policy_loss = cross_entropy(policy_pred, mcts_probs) value_loss = mse_loss(value_pred, final_result) loss = policy_loss + value_loss loss.backward() optimizer.step() In my opinion, this is honestly kind of straightforward. Predict policy, predict value, calculate loss with cross entropy \u0026 mse and update your weights.\nThe overall loop continues to:\nGenerate self-play games Batch data from those games Train on that data Update the policy and value networks Repeat Over time, the agent improves not by memorizing moves but by learning which states are promising and which moves are strong, all from experience.\nBefore we finish up this section I want to touch on 2 things: the loss functions for policy_loss and value_loss.\nIn the value network we are predicting a single number at the end. This screams regression because we want the model to fit it’s prediction to a certain range. Using MSE encourages our predictions to be numerically close to the actual result.\nFor the policy loss, we are not predicting the correct move - we are trying to predict a probability distribution over 9 moves. The target is also a distribution, it’s the one generated by MCTS.\nSince we’re doing distribution matching, cross-entropy is the standard for that.\nIf we tried using MSE here instead, it would try to numerically match each probability. On the other hand, cross-entropy measures how surprised your prediction is by the true distribution.\nThe target distribution comes from MCTS visit counts, reflecting which moves were explored the most during search. Instead of learning how to predict “one correct move”, the policy network is learning to imitate the move distribution that MCTS created.\nDid it work? I haven’t fully coded and trained the agent yet, but we have everything so far to go ahead and get started.\nThere are some clear signs that we can see to know if the model is training and generalizing well.\nFrom random to strategic At first, the agent will make dumb, random moves. Over time, through self-play and training, I expect it to converge toward winning strategies like blocking opponents, going for forks, and always taking center if possible.\nPolicy get sharper The policy network should start off nearly uniform but as it trains on MCTS-guided targets, its output should begin concentrating around stronger moves and weaker moves should tend towards 0. This would match what an optimal player would do.\nValue network improves intuition We expect the value network to be very noisy early on. With enough games however, it should begin to accurately assign high scores to strong positions (like 2 in a row with an open third) and low scores to losing ones (like when opponent has 2 in a row and has a chance to close in).\nPerformance Metrics One implemented, I’d like to track:\nWin rate vs. random agent Win rate vs. greedy agent Value estimation error (MSE) Policy prediction accuracy (Cross-Entropy) This section wouldn’t be about flexing our results but rather about setting the expection. If everything above is done right, the model should learn to play well entirely from scratch.\nThese would be also be fun to track but a little harder to implement:\n% of times a fatal move is played (i.e., a move that directly allows the opponent to win on the next turn) % of games where agent creates a fork (shows strategic depth because forks are subtle and strong moves in TicTacToe) Average branching factor explored during MCTS (How many child nodes are explored on average — reflects how focused the agent’s search is) Conclusion \u0026 Further thoughts Let’s step back and look at what we have.\nWe built a clean TicTacToe environment from scratch for agents to play in. Designed an agent that could play the game and make decisions. Equipped it with a basic Monte Carlo Tree Search engine. Let two clones of the agent play thousands of games against each other. Used the data from self-play to train a policy and a value network. Watched our agent go from clueless to competent — learning how to win and stay sharp entirely through experience, without a single labeled example. I’ll wrap up by nailing how the self-play / training / inference pipelines work.\nDuring self-play Agent observes current board state s Run MCTS from s to choose a move MCTS uses the policy network to guide exploration MCTS optionally usese the value netwrok to evaluate leaf states MCTS performs N simulations, builds a search tree, and collects visit counts for each move Get move probabilities from MCTS Argmax or sample from that move distribution to pick your action Play the move and update the game state Store tuple (state, move_probs, final_result) for training later Repeat until game ends If game won: assign +1 to every state If game lost: assign -1 to every state If game draw: assign 0 to every state We assign this same value to every state because the value network is trained to predict “what is the expected outcome from this board state, assuming optimal play?” Also, all those states sequentially led to the win and that’s why they all get the same label.\nDuring training After a batch of self-play games you have a dataset\n(batch_size, state, mcts_probs, game_result) For each training example, we feed:\nInput: board state Target for policy network: MCTS move probabilities Target for value network: final outcome of the game (1, 0, -1) We feed in multiple batches at once (just like training any other model).\nDuring inference Once training is complete and our policy \u0026 value networks are strong, we can play games using them directly. There are two main options:\nOption 1: MCTS + Networks\nWe continue using MCTS at inference, but now guided by the trained networks.\nThe policy network initializes move priors at each node The value network replaces rollouts with learned estimates This is exactly how AlphaZero plays: every move still runs simulations, but they’re smarter and faster.\nOption 2: Policy Network only\nIn smaller environments like TicTacToe (or latency-sensitive apps), we can skip MCTS and just do:\nmove = argmax(policy_net(state)) This is less strategic but it’s extremely fast.\nAlphaTicTacToe (and even AlphaGo) is essentially just MCTS but guided by deep learning (policy and value nets).\nMCTS handles the planning (tree-based lookahead) and neural networks handle the learning generalize from past games.\nIf you think about it, there’s a lot of concepts shared between different fields within ML. MCTS guided by value \u0026 policy networks? That reminds me of tokens attending to others will generating text. Argmax or sampling from the probaility distribution after the policy network? Does that not remind you of beam search / argmax while generating text?\nCoding it out Whenever I get the time, I’m going to code this up and show the process, the agent getting better over time, and what struggles I had. Hopefully the next long weekend or whenever I have some time.\n",
  "wordCount" : "4188",
  "inLanguage": "en",
  "datePublished": "2025-05-02T00:00:00Z",
  "dateModified": "2025-05-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "ML Theory"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/reimplementations/" title="Reimplementations">
                    <span>Reimplementations</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Designing AlphaTicTacToe (and AlphaTicTacToeZero)
    </h1>
    <div class="post-meta"><span title='2025-05-02 00:00:00 +0000 UTC'>May 2, 2025</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;ML Theory

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: Even though RL feels kind of quiet nowadays outside of RLHF, it still blows my mind that agents can learn purely from experience — no labels, just trial and error. It’s messy, unstable, and hard to debug, but there’s something so raw and human-like about it. That’s what I’m chasing by building AlphaTicTacToe from scratch.</p>
<p>[UPDATE]: The repsitory is completed and the corresponding PyTorch implementation is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<h2 id="thoughts-on-rl">Thoughts on RL<a hidden class="anchor" aria-hidden="true" href="#thoughts-on-rl">#</a></h2>
<p>Recently, I don&rsquo;t see reinforcement learning being used that much outside of things like RLHF. Before LLMs took off, RL was the frontier and showed us how powerful scaling up machine learning problems can be.</p>
<p>RL is one of those fields that feels like magic when you first encounter it. Give an agent some rewards, throw it into an environment, and somehow, over time, it figures out how to win by minimizing loss.</p>
<p>Behind that magic is a rigorous and delicate balance between trial and error learning, exploration, value estimation, and function approximation.</p>
<p>What&rsquo;s always fascinated me about reinforcement learning is how well it learns without labels. Learning purely from experience helps the model come up with it&rsquo;s own features in a cool way, since it finds a structure in space that minimizes loss.</p>
<p>In some ways RL feels way more <em>human-like</em>. But learning from no labels and setting up the environment makes it hard to debug, unstable to train, and overly sensitive to design decisions. By designing and building AlphaTicTacToe I hope to learn more about the training and learning process in reinforcment learning.</p>
<div align="center">
  <img src="/images/rl1.png" alt="Skip-gram architecture" width="300"/>
  <p></p>
</div>
<h2 id="why-tictactoe-how-does-it-compare-to-go">Why TicTacToe? How does it compare to Go?<a hidden class="anchor" aria-hidden="true" href="#why-tictactoe-how-does-it-compare-to-go">#</a></h2>
<p>TicTacToe might seem like a joke compared to Go, and it kind of is. Go has more board configurations than atoms in the universe whereas TicTacToe has just 765 unique board states. But that simplicity is what I want to take advantage of.</p>
<p>AlphaGo combined deep learning, reinforcement learning, and tree search to beat world champions. My goal wasn’t to replicate that scale, it was to replicate the ideas. TicTacToe gives me a controlled, fully observable world where I can implement:</p>
<ul>
<li>Self-play training</li>
<li>A policy network</li>
<li>A value network</li>
<li>A touch of MCTS</li>
</ul>
<p>In other words, a tiny lab to test the same techniques that powered AlphaGo and AlphaZero but in minutes instead of months.</p>
<h2 id="core-concepts-well-use">Core concepts we&rsquo;ll use<a hidden class="anchor" aria-hidden="true" href="#core-concepts-well-use">#</a></h2>
<p>Before we dive into architecture and code, here are the main concepts this project draws from:</p>
<h3 id="policy-network">Policy Network<a hidden class="anchor" aria-hidden="true" href="#policy-network">#</a></h3>
<p>A neural net that maps board states → probabilities over next moves. This is how the agent decides what to do given the current input.</p>
<p>Here&rsquo;s a simple example for our case:</p>
<ul>
<li>Policy Network input: Current board (a 3x3 grid encoded as a 1D tensor)</li>
<li>Policy Network output: Probability distribution over all 9 positions</li>
</ul>
<p>If we had a 3x3 board like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[ <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><p>where</p>
<ul>
<li>1 = our move (X)</li>
<li>-1 = opponent move (O)</li>
<li>0 = empty cell</li>
</ul>
<p>Our policy network would return</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[<span style="color:#ae81ff">0.05</span>, <span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">0.00</span>, <span style="color:#ae81ff">0.20</span>, <span style="color:#ae81ff">0.00</span>, <span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">0.40</span>, <span style="color:#ae81ff">0.03</span>, <span style="color:#ae81ff">0.30</span>]
</span></span></code></pre></div><p>which is a probability distribution over the whole input (what we usually see after a softmax). The above means that the network thinks</p>
<ul>
<li>Move 6 (index 6) is most likely ot be the best (40% chance)</li>
<li>Move 8 is the next best (30% chance)</li>
<li>Moves like 1, 2, 4, 5, 6 are either illegal (the board isn&rsquo;t empty there) or bad (game losing moves) hence the 0 - 0.01%</li>
</ul>
<p>The agent would sample from this distribution or choose the argmax depending on the setting. Here&rsquo;s an NLP analogy: does this kind of remind you of picking the right token after scaled attention (argmax, beam search, etc.)? It&rsquo;s the same concept but now applied to moves in a game instead of picking the next best token.</p>
<h3 id="value-network">Value Network<a hidden class="anchor" aria-hidden="true" href="#value-network">#</a></h3>
<p>This is another neural net that maps board states → estimated outcome (win/loss/draw). It helps the agent understand which states are &ldquo;good&rdquo; and which states are &ldquo;bad&rdquo; (moves that will most likely to hurt your chances of winning).</p>
<p>Think of this like asking:</p>
<p>&ldquo;If I&rsquo;m in this position and play optimally from here, how likely am I to win?&rdquo;</p>
<p>Let&rsquo;s dive deeper</p>
<ul>
<li>Value Network input: Current board (a 3x3 grid encoded as a tensor, just like what we saw above)</li>
<li>Value Network output: A single scalar value between -1 and 1 describing winning likelihood of winning</li>
</ul>
<p>The scalar result of a value network is actually pretty easy to understand. A value of 1.0 means guaranteed win, a vaue of -1.0 means we&rsquo;re definitely going to lose, and 0.0 means a draw or unclear outcome.</p>
<p>These two (policy and value networks) work together really well. We can look at them like the following.</p>
<p>If you&rsquo;re standing at a fork in the road, the policy network says &ldquo;turn left, I think it leads to the best destination&rdquo; and the value network says &ldquo;this spot you&rsquo;re in right now? I&rsquo;d rate it a 7/10. Not bad but not amazing either.&rdquo; You would know to go left because of what your policy network says and given the fact that your value network thinks you can get to a better position (you want to get to a 10, not a 7).</p>
<blockquote>
<p>This combination of <em>knowing what to choose</em> and instantly <em>evaluating your current state</em> helps agents look ahead smartly like humans do when thinking &ldquo;if I go here, what&rsquo;s likely to happen?&rdquo;</p></blockquote>
<div align="center">
  <img src="/images/rl2.png" alt="Skip-gram architecture" width="600"/>
  <p>This picture shows how the policy network and value network work in Go, but the same concept applies in tic-tac-toe.</p>
</div>
<h3 id="self-play-for-tictactoezero">Self-Play (for TicTacToeZero)<a hidden class="anchor" aria-hidden="true" href="#self-play-for-tictactoezero">#</a></h3>
<p>In self play, the agent learns by playing against itself, without any human data. This is what made AlphaZero groundbreaking because it kind of &ldquo;taught itself the game&rdquo;. I&rsquo;ll go more into this when we prepare for training.</p>
<h3 id="reward-signal">Reward Signal<a hidden class="anchor" aria-hidden="true" href="#reward-signal">#</a></h3>
<p>This is super simple. For every game, we can assign</p>
<ul>
<li>+1 for a win</li>
<li>-1 for a loss</li>
<li>0 for a draw</li>
</ul>
<p>This reward signal is sparse and very delayed (only received at the end of the game) but it encodes everything that matters (win, lose, draw).</p>
<p>Though this is such a weak signal, the agent learns a lot through self-play, MCTS, and policy/value networks. This teaches the agent to evaluate board states and select promising moves, even when intermediate rewards aren&rsquo;t available.</p>
<p>For these games:</p>
<ul>
<li>Winning is the only thing that matters</li>
<li>MCTS provides intermediate guidance via simulation</li>
<li>The networks generalize from outcomes to board positions</li>
</ul>
<blockquote>
<p>You don&rsquo;t need dense rewards if your agent is powerful enough to learn from sparse ones.</p></blockquote>
<h3 id="exploration-strategy">Exploration Strategy<a hidden class="anchor" aria-hidden="true" href="#exploration-strategy">#</a></h3>
<p>Early on, the agent needs randomness (like ε-greedy) to discover better moves before we move on to doing MCTS (Monte Carlo Tree Search, more on this later). This randomness counts as <em>exploration noise</em> and is crucial early in training.</p>
<p>In the beginning,</p>
<ul>
<li>Your networks are terrible (random weights)</li>
<li>Terrible networks won&rsquo;t explore less obvious (but potentially better) strategies</li>
</ul>
<p>You end up in a toxic feedback loop:</p>
<p>Bad network → bad MCTS → bad self-play → no learning</p>
<p>With ε-greedy, you pick a random move instead of the best one with probability <code>ε</code>. This forces the agent to try moves that currently look suboptimal, but might actually</p>
<ul>
<li>Be super helpful down the line</li>
<li>Help MCTS explore deeper, richer branches of the tree</li>
<li>Expose the neural net to more diverse set of board states</li>
</ul>
<p>To use a chess analogy: this is like giving the agent a chance to learn castling. At first, it looks like a wasted move. But over time, the agent realizes it’s actually super powerful for long-term king safety and control.</p>
<h3 id="monte-carlo-tree-search-mcts">Monte Carlo Tree Search (MCTS)<a hidden class="anchor" aria-hidden="true" href="#monte-carlo-tree-search-mcts">#</a></h3>
<p>I&rsquo;ve been touching on MCTS a lot but haven&rsquo;t explained it well yet. Here&rsquo;s a proper breakdown.</p>
<p>MCTS is a search algorithm that helps an agent choose the best move by running <em>lots of simulations</em>, keeping track of which actions led to wins, and gradually growing a <strong>search tree</strong> based on experience, not hardcoded rules. Think of it as the scalable alternative to Q-tables used in tabular RL — but for massive action spaces.</p>
<p>The core idea is:</p>
<ul>
<li>From the current board, simulate random (or guided) games to the end</li>
<li>Record which moves led to wins more often</li>
<li>Keep exploring promising branches of the tree</li>
</ul>
<p>Over time, we form a biased tree that favors strong moves.</p>
<div align="center">
  <img src="/images/rl3.png" alt="Skip-gram architecture" width="600"/>
  <p>By repeating these steps thousands of times, MCTS builds a tree that implicitly “learns” which branches are worth pursuing — without ever training a model.</p>
</div>
<p>However, in AlphaZero, MCTS is used not just to pick moves, but to generate training data for the neural networks as well.</p>
<p>Let&rsquo;s take a look at one whole MCTS loop.</p>
<ol>
<li><strong>Selection</strong></li>
</ol>
<p>We start at the root (current board), and move down the tree by picking the most promising child using something like the <a href="https://www.geeksforgeeks.org/machine-learning/upper-confidence-bound-algorithm-in-reinforcement-learning/">UCB (Upper Confidence Bound)</a> formula.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">select</span>(node):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> node<span style="color:#f92672">.</span>is_fully_expanded():
</span></span><span style="display:flex;"><span>        node <span style="color:#f92672">=</span> max(node<span style="color:#f92672">.</span>children, key<span style="color:#f92672">=</span>ucb_score)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> node
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ucb_score</span>(child):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ucb1: exploitation + exploration</span>
</span></span><span style="display:flex;"><span>    C <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.4</span>  <span style="color:#75715e"># exploration constant</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> (child<span style="color:#f92672">.</span>wins <span style="color:#f92672">/</span> child<span style="color:#f92672">.</span>visits) <span style="color:#f92672">+</span> C <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>sqrt(math<span style="color:#f92672">.</span>log(child<span style="color:#f92672">.</span>parent<span style="color:#f92672">.</span>visits) <span style="color:#f92672">/</span> child<span style="color:#f92672">.</span>visits)
</span></span></code></pre></div><ol start="2">
<li><strong>Expansion</strong>
When you reach a node that isn’t fully explored, try a new move from that state and add a child node.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">expand</span>(node):
</span></span><span style="display:flex;"><span>    untried_moves <span style="color:#f92672">=</span> node<span style="color:#f92672">.</span>get_untried_moves()
</span></span><span style="display:flex;"><span>    move <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(untried_moves)
</span></span><span style="display:flex;"><span>    next_state <span style="color:#f92672">=</span> node<span style="color:#f92672">.</span>state<span style="color:#f92672">.</span>play(move)
</span></span><span style="display:flex;"><span>    child_node <span style="color:#f92672">=</span> TreeNode(state<span style="color:#f92672">=</span>next_state, parent<span style="color:#f92672">=</span>node, move<span style="color:#f92672">=</span>move)
</span></span><span style="display:flex;"><span>    node<span style="color:#f92672">.</span>children<span style="color:#f92672">.</span>append(child_node)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> child_node
</span></span></code></pre></div><ol start="3">
<li><strong>Simulation (Rollout)</strong>
From that child node, play random moves until the game ends.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">simulate</span>(state):
</span></span><span style="display:flex;"><span>    current_state <span style="color:#f92672">=</span> state<span style="color:#f92672">.</span>clone()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> current_state<span style="color:#f92672">.</span>is_terminal():
</span></span><span style="display:flex;"><span>        move <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(current_state<span style="color:#f92672">.</span>get_legal_moves())
</span></span><span style="display:flex;"><span>        current_state <span style="color:#f92672">=</span> current_state<span style="color:#f92672">.</span>play(move)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> current_state<span style="color:#f92672">.</span>get_result()  <span style="color:#75715e"># +1, 0, or -1</span>
</span></span></code></pre></div><ol start="4">
<li><strong>Backpropagation</strong></li>
</ol>
<p>Propagate the result up the tree, updating the win/visit stats for every node along the way. <strong>This is not backprop in the regular sense</strong> - there’s no gradients, no weight updates, no chain rule. We’re just logging outcomes like a scoreboard. This is enough for us because MCTS just needs to show us <em>viable paths</em>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backpropagate</span>(node, result):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> node <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        node<span style="color:#f92672">.</span>visits <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        node<span style="color:#f92672">.</span>wins <span style="color:#f92672">+=</span> result
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">*=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>  <span style="color:#75715e"># flip result for opponent&#39;s perspective</span>
</span></span><span style="display:flex;"><span>        node <span style="color:#f92672">=</span> node<span style="color:#f92672">.</span>parent
</span></span></code></pre></div><p>These 4 steps combined become one loop (or &ldquo;epoch&rdquo; if you want to think about it in more of a DL sense). Repeating this thousands of times and we end up with a smart tree that recommends the best next move. This essentially becomes the &ldquo;brain&rdquo; of the whole model.</p>
<p>Here&rsquo;s what it looks like put together</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mcts_search</span>(root, num_simulations):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(num_simulations):
</span></span><span style="display:flex;"><span>        node <span style="color:#f92672">=</span> select(root)
</span></span><span style="display:flex;"><span>        child <span style="color:#f92672">=</span> expand(node)
</span></span><span style="display:flex;"><span>        result <span style="color:#f92672">=</span> simulate(child<span style="color:#f92672">.</span>state)
</span></span><span style="display:flex;"><span>        backpropagate(child, result)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> max(root<span style="color:#f92672">.</span>children, key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> c: c<span style="color:#f92672">.</span>visits)<span style="color:#f92672">.</span>move
</span></span></code></pre></div><p>not too bad right?</p>
<p>Run this thousands of times and you get a <strong>smart tree</strong> that recommends the best next move based on experience, not brute force. This becomes the agent’s “thinking loop.”</p>
<p>In Go, there are ~10^170 board states, making brute force search absolutely useless. MCTS changed the game by allowing us to selectively explore only promising lines, guided by past outcomes instead of expanding every possible branch like in BFS.</p>
<p>In TicTacToe, the board is tiny. We can do a full-depth BFS style search if we reallly wanted to. I still wanted to see if I could use MCTS + neural networks in this smaller world to learn like AlphaGo did, instead of hardcoding perfect moves with exhaustive search algorithms.</p>
<p>Because of how different Go and TicTacToe are, I&rsquo;ll have to use a differenet version of MCTS.</p>
<ul>
<li>Instead of a deep tree, I ran 50 simulations per move.</li>
<li>I used random rollouts (no value network at first)</li>
</ul>
<p>This is definitley still overkill for TicTacToe (which can be done with an exhaustive search) but I wanted to model a network that could <em>think and reason</em> like AlphaGo on a smaller scale.</p>
<h2 id="design-choices-for-tictactoe">Design Choices for TicTacToe<a hidden class="anchor" aria-hidden="true" href="#design-choices-for-tictactoe">#</a></h2>
<p>TicTacToe is very simple but designing a learning agent still means making real decisions around how to represent the game, structure the model, and define the environment. Here’s how I approached it</p>
<h3 id="state-representation">State Representation<a hidden class="anchor" aria-hidden="true" href="#state-representation">#</a></h3>
<p>I touched on this earlier - a simple 9 element 1D tensor works well here in my opinion.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[ <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, 
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">1</span>,  <span style="color:#ae81ff">0</span>,  <span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><p>When I was thinking about different representations, ChatGPT recommended me to use one-hot encoding (3 channels: player, opponent, empty) but I wanted to keep this as simple as possible since the board is already tiny.</p>
<h3 id="action-space">Action Space<a hidden class="anchor" aria-hidden="true" href="#action-space">#</a></h3>
<p>There are 9 possible moves (indexes 0-8) where each corresponds to placing your symbol in one of the 9 cells.</p>
<ul>
<li>If the cell is already occupied → the move is illegal</li>
<li>Policy network gives us a probability distribution over 9 cells</li>
<li>During argmax (or sampling) selection, we mask the illegal moves so the agent doesn&rsquo;t try to play in filled spaces</li>
</ul>
<p>This way, the agent doesn&rsquo;t get punished for illegal moves, it just learns not to choose them with masking.</p>
<h3 id="network-architecture">Network Architecture<a hidden class="anchor" aria-hidden="true" href="#network-architecture">#</a></h3>
<p>Both the policy network and the value network would become small MLPs (multi-layer perceptrons).</p>
<p>Policy Network</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">64</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">9</span>)  <span style="color:#75715e"># raw logits → softmax</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Let&rsquo;s try to make sense of the hidden layer sizes. We first have an input of 9 because our input tensor is of size (9,) based on what we decided earlier and only has values of 1, -1, or 0.</p>
<p>The hidden layer size is 64. I chose this number as a sweet spot. It&rsquo;s large enough to allow the network to learn useful patterns and combinations but small enough to avoid overfitting or unnecessary complexity for such a tiny input space.</p>
<p>Applying a ReLU gives us non linearity instead of just linear transformations.</p>
<p>We end with squeezing it back down to an output of 9. The resulting 9 logits are going to be your probabilities as we talked about that the policy network return, one per board cell.</p>
<blockquote>
<p>The policy network&rsquo;s job is simple: take in the current board and return a probability distribution over all 9 moves.</p></blockquote>
<p>Value Network</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">9</span>, <span style="color:#ae81ff">64</span>),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>ReLU(),
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">1</span>),  <span style="color:#75715e"># scalar value</span>
</span></span><span style="display:flex;"><span>    nn<span style="color:#f92672">.</span>Tanh()  <span style="color:#75715e"># outputs belong to [-1, 1] (tanh applied on resulting logit for &#34;squashing&#34; to the -1, 1 range)</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Just like the policy network, we have an input size of 9 and I still went with 64 as a sweet spot. The main change is the last two lines.</p>
<p>Our output layer is now just a single scalar value that estimates how favorable the current state is for the agent. This final output is then passed through a <code>tanh</code> squeezing it into the range [-1, 1] (as explained before). +1 would be a very confident win and -1 would be a certain loss.</p>
<p>This output is used to help guide MCTS and inform learning. It acts like a learned evaluation function that replaces brute-force rollout results.</p>
<blockquote>
<p>The value network&rsquo;s job is simpler: take in the current board and return a score for how likely you think this will win the game in the range [-1, 1] (tanh helps with this).</p></blockquote>
<h3 id="game-logic">Game Logic<a hidden class="anchor" aria-hidden="true" href="#game-logic">#</a></h3>
<p>Before we can train an agent to play TicTacToe, we need to build the environment it lives in - the rules, state transitions, and reward mechanics. Here’s how I structured the game engine that powers both self-play and MCTS.</p>
<h4 id="applying-moves">Applying Moves<a hidden class="anchor" aria-hidden="true" href="#applying-moves">#</a></h4>
<p>Each player takes turns selecting a move (an index from 0 to 8). We check if the move is legal and update the board accordingly.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">apply_move</span>(self, move):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>board[move] <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">raise</span> <span style="color:#a6e22e">ValueError</span>(<span style="color:#e6db74">&#34;Illegal move!&#34;</span>)
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>board[move] <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>current_player
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>current_player <span style="color:#f92672">*=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>  <span style="color:#75715e"># switch turns</span>
</span></span></code></pre></div><h4 id="terminal-check">Terminal Check<a hidden class="anchor" aria-hidden="true" href="#terminal-check">#</a></h4>
<p>After every move I check for a win, loss, or draw. If the game is over, I return the final result</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_winner</span>(self):
</span></span><span style="display:flex;"><span>    winning_lines <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">5</span>], [<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">8</span>],  <span style="color:#75715e"># rows</span>
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">6</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">7</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">8</span>],  <span style="color:#75715e"># cols</span>
</span></span><span style="display:flex;"><span>        [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">8</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">6</span>]              <span style="color:#75715e"># diagonals</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> line <span style="color:#f92672">in</span> winning_lines:
</span></span><span style="display:flex;"><span>        values <span style="color:#f92672">=</span> [self<span style="color:#f92672">.</span>board[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> line]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> sum(values) <span style="color:#f92672">==</span> <span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span>   <span style="color:#75715e"># X wins</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">elif</span> sum(values) <span style="color:#f92672">==</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>  <span style="color:#75715e"># O wins</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>board:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">0</span>  <span style="color:#75715e"># Draw</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#66d9ef">None</span>  <span style="color:#75715e"># Game continues</span>
</span></span></code></pre></div><h4 id="resetting-the-game">Resetting the game<a hidden class="anchor" aria-hidden="true" href="#resetting-the-game">#</a></h4>
<p>Each new game starts with a clean board and the first player to move</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset</span>(self):
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>board <span style="color:#f92672">=</span> [<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">9</span>
</span></span><span style="display:flex;"><span>    self<span style="color:#f92672">.</span>current_player <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</span></span></code></pre></div><p>This reset function is essential for:</p>
<ul>
<li>Self-play, where each game is an independent episode</li>
<li>Training loops, where consistency across episodes matters</li>
</ul>
<p>Without resetting, the leftover board state or player turn bugs could corrupt learning.</p>
<h4 id="cloning-for-mcts">Cloning for MCTS<a hidden class="anchor" aria-hidden="true" href="#cloning-for-mcts">#</a></h4>
<p>MCTS doesn&rsquo;t need to reset the board, it needs to <em>duplicate</em> the current game state to run thousands of rollouts from it.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clone</span>(self):
</span></span><span style="display:flex;"><span>    cloned <span style="color:#f92672">=</span> TicTacToe()
</span></span><span style="display:flex;"><span>    cloned<span style="color:#f92672">.</span>board <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>board[:]
</span></span><span style="display:flex;"><span>    cloned<span style="color:#f92672">.</span>current_player <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>current_player
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cloned
</span></span></code></pre></div><p>Between <code>.reset()</code> and <code>.clone()</code> we&rsquo;ve got a clean way to simulate and learn without state leaks or bugs.</p>
<h3 id="integration-with-agent">Integration with Agent<a hidden class="anchor" aria-hidden="true" href="#integration-with-agent">#</a></h3>
<p>At each step</p>
<ol>
<li>
<p>The agent observes the current board state</p>
</li>
<li>
<p>It runs MCTS, using the policy network to guide exploration</p>
</li>
<li>
<p>MCTS outputs a move distribution (based on visit counts)</p>
</li>
<li>
<p>The agent samples a move from that distribution (or just takes argmax)</p>
</li>
<li>
<p>The selected move is applied to the board</p>
</li>
<li>
<p>The tuple <code>(state, move_probs, final_result)</code> is saved for training</p>
</li>
</ol>
<p>Here&rsquo;s what I optimized for:</p>
<ul>
<li>Keeping everything small, fast, and interpretable</li>
<li>Using self-play, policy/value, and MCTS to mimic AlphaGo logic</li>
<li>Letting the agent learn from scratch rather than relying on hardcoded strategies</li>
</ul>
<h2 id="training-the-agent">Training the Agent<a hidden class="anchor" aria-hidden="true" href="#training-the-agent">#</a></h2>
<p>There are two main paradigms for training a game playing agent like this:</p>
<ul>
<li>Learn via human gameplay (supervised learning)</li>
<li>Learn from scratch via self-play (reinforcement learning)</li>
</ul>
<p>The first is what AlphaGo did and the second is what AlphaZero did.</p>
<p>In this project, I chose to go with self-play. It’s slower and more complex than imitation learning, but it gives me a more scalable setup, an agent that can discover novel strategies on its own, and a deeper understanding of how learning emerges from interaction.</p>
<p>When playing games against itself, for each move it makes during a game, we store:</p>
<ul>
<li>The current board state</li>
<li>The move distribution from MCTS (how often each move was visited)</li>
<li>The final result of the game (win/loss/draw)</li>
</ul>
<p>This gives us training examples in the form:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(state, mcts_probs, final_result)
</span></span></code></pre></div><p>and we then batch these to become</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(batch_size, state, mcts_probs, final_result)
</span></span></code></pre></div><p>We can feed this data into a trianing loop after generating a batch of games. We train both the policy network and the value network using gradient descent.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> replay_buffer:
</span></span><span style="display:flex;"><span>        policy_pred <span style="color:#f92672">=</span> policy_net(state)
</span></span><span style="display:flex;"><span>        value_pred <span style="color:#f92672">=</span> value_net(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        policy_loss <span style="color:#f92672">=</span> cross_entropy(policy_pred, mcts_probs)
</span></span><span style="display:flex;"><span>        value_loss <span style="color:#f92672">=</span> mse_loss(value_pred, final_result)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> policy_loss <span style="color:#f92672">+</span> value_loss
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p>In my opinion, this is honestly kind of straightforward. Predict policy, predict value, calculate loss with cross entropy &amp; mse and update your weights.</p>
<p>The overall loop continues to:</p>
<ul>
<li>Generate self-play games</li>
<li>Batch data from those games</li>
<li>Train on that data</li>
<li>Update the policy and value networks</li>
<li>Repeat</li>
</ul>
<p>Over time, the agent improves not by memorizing moves but by learning which states are promising and which moves are strong, all from experience.</p>
<p>Before we finish up this section I want to touch  on 2 things: the loss functions for <code>policy_loss</code> and <code>value_loss</code>.</p>
<p>In the value network we are predicting a <em>single number</em> at the end. This screams regression because we want the model to fit it&rsquo;s prediction to a certain range. Using MSE encourages our predictions to be numerically close to the actual result.</p>
<p>For the policy loss, we are not predicting the correct move - we are trying to predict a probability distribution over 9 moves. The target is also a distribution, it&rsquo;s the one generated by MCTS.</p>
<p>Since we&rsquo;re doing distribution matching, cross-entropy is the standard for that.</p>
<p>If we tried using MSE here instead, it would try to numerically match each probability. On the other hand, cross-entropy measures how surprised your prediction is by the true distribution.</p>
<p>The target distribution comes from MCTS visit counts, reflecting which moves were explored the most during search. Instead of learning how to predict &ldquo;one correct move&rdquo;, the policy network is learning to imitate the move distribution that MCTS created.</p>
<h2 id="did-it-work">Did it work?<a hidden class="anchor" aria-hidden="true" href="#did-it-work">#</a></h2>
<p>I haven&rsquo;t fully coded and trained the agent yet, but we have everything so far to go ahead and get started.</p>
<p>There are some clear signs that we can see to know if the model is training and generalizing well.</p>
<ol>
<li><strong>From random to strategic</strong></li>
</ol>
<p>At first, the agent will make dumb, random moves. Over time, through self-play and training, I expect it to converge toward winning strategies like blocking opponents, going for forks, and always taking center if possible.</p>
<ol start="2">
<li><strong>Policy get sharper</strong></li>
</ol>
<p>The policy network should start off nearly uniform but as it trains on MCTS-guided targets, its output should begin concentrating around stronger moves and weaker moves should tend towards 0. This would match what an optimal player would do.</p>
<ol start="3">
<li><strong>Value network improves intuition</strong></li>
</ol>
<p>We expect the value network to be very noisy early on. With enough games however, it should begin to accurately assign high scores to strong positions (like 2 in a row with an open third) and low scores to losing ones (like when opponent has 2 in a row and has a chance to close in).</p>
<ol start="4">
<li><strong>Performance Metrics</strong></li>
</ol>
<p>One implemented, I&rsquo;d like to track:</p>
<ul>
<li>Win rate vs. random agent</li>
<li>Win rate vs. greedy agent</li>
<li>Value estimation error (MSE)</li>
<li>Policy prediction accuracy (Cross-Entropy)</li>
</ul>
<p>This section wouldn&rsquo;t be about flexing our results but rather about setting the expection. If everything above is done right, the model should learn to play well entirely from scratch.</p>
<p>These would be also be fun to track but a little harder to implement:</p>
<ul>
<li>% of times a fatal move is played (i.e., a move that directly allows the opponent to win on the next turn)</li>
<li>% of games where agent creates a fork
(shows strategic depth because forks are subtle and strong moves in TicTacToe)</li>
<li>Average branching factor explored during MCTS
(How many child nodes are explored on average — reflects how focused the agent’s search is)</li>
</ul>
<h2 id="conclusion--further-thoughts">Conclusion &amp; Further thoughts<a hidden class="anchor" aria-hidden="true" href="#conclusion--further-thoughts">#</a></h2>
<p>Let’s step back and look at what we have.</p>
<ul>
<li>We built a clean TicTacToe environment from scratch for agents to play in.</li>
<li>Designed an agent that could play the game and make decisions.</li>
<li>Equipped it with a basic Monte Carlo Tree Search engine.</li>
<li>Let two clones of the agent play thousands of games against each other.</li>
<li>Used the data from self-play to train a policy and a value network.</li>
<li>Watched our agent go from clueless to competent — learning how to win and stay sharp entirely through experience, without a single labeled example.</li>
</ul>
<p>I&rsquo;ll wrap up by nailing how the self-play / training / inference pipelines work.</p>
<h3 id="during-self-play">During self-play<a hidden class="anchor" aria-hidden="true" href="#during-self-play">#</a></h3>
<ol>
<li>Agent observes current board state <code>s</code></li>
<li>Run MCTS from <code>s</code> to choose a move</li>
</ol>
<ul>
<li>MCTS uses the policy network to guide exploration</li>
<li>MCTS optionally usese the value netwrok to evaluate leaf states</li>
<li>MCTS performs N simulations, builds a search tree, and collects visit counts for each move</li>
</ul>
<ol start="3">
<li>Get move probabilities from MCTS</li>
<li>Argmax or sample from that move distribution to pick your action</li>
<li>Play the move and update the game state</li>
<li>Store tuple <code>(state, move_probs, final_result)</code> for training later</li>
<li>Repeat until game ends</li>
</ol>
<ul>
<li>If game won: assign +1 to every state</li>
<li>If game lost: assign -1 to every state</li>
<li>If game draw: assign 0 to every state</li>
</ul>
<p>We assign this same value to <em>every state</em> because the value network is trained to predict &ldquo;what is the expected outcome from this board state, assuming optimal play?&rdquo; Also, all those states sequentially led to the win and that&rsquo;s why they all get the same label.</p>
<h3 id="during-training">During training<a hidden class="anchor" aria-hidden="true" href="#during-training">#</a></h3>
<p>After a batch of self-play games you have a dataset</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>(batch_size, state, mcts_probs, game_result)
</span></span></code></pre></div><p>For each training example, we feed:</p>
<ul>
<li>Input: board state</li>
<li>Target for policy network: MCTS move probabilities</li>
<li>Target for value network: final outcome of the game (1, 0, -1)</li>
</ul>
<p>We feed in multiple batches at once (just like training any other model).</p>
<h3 id="during-inference">During inference<a hidden class="anchor" aria-hidden="true" href="#during-inference">#</a></h3>
<p>Once training is complete and our policy &amp; value networks are strong, we can play games using them directly. There are two main options:</p>
<p><strong>Option 1:</strong> MCTS + Networks</p>
<p>We continue using MCTS at inference, but now guided by the trained networks.</p>
<ul>
<li>The policy network initializes move priors at each node</li>
<li>The value network replaces rollouts with learned estimates</li>
</ul>
<p>This is exactly how AlphaZero plays: every move still runs simulations, but they’re smarter and faster.</p>
<p><strong>Option 2:</strong> Policy Network only</p>
<p>In smaller environments like TicTacToe (or latency-sensitive apps), we can skip MCTS and just do:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>move <span style="color:#f92672">=</span> argmax(policy_net(state))
</span></span></code></pre></div><p>This is less strategic but it&rsquo;s extremely fast.</p>
<hr>
<blockquote>
<p>AlphaTicTacToe (and even AlphaGo) is essentially just MCTS but guided by deep learning (policy and value nets).</p></blockquote>
<p>MCTS handles the <em>planning</em> (tree-based lookahead) and neural networks handle the <em>learning</em> generalize from past games.</p>
<p>If you think about it, there&rsquo;s a lot of concepts shared between different fields within ML. MCTS guided by value &amp; policy networks? That reminds me of tokens <em>attending</em> to others will generating text. Argmax or sampling from the probaility distribution after the policy network? Does that not remind you of beam search / argmax while generating text?</p>
<h2 id="coding-it-out">Coding it out<a hidden class="anchor" aria-hidden="true" href="#coding-it-out">#</a></h2>
<p>Whenever I get the time, I&rsquo;m going to code this up and show the process, the agent getting better over time, and what struggles I had. Hopefully the next long weekend or whenever I have some time.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
