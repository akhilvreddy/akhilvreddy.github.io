<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.">
<meta name="author" content="ML Theory &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/attention-from-scratch/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/attention-from-scratch/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/attention-from-scratch/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT">
  <meta property="og:description" content="I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-03-13T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-03-13T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT">
<meta name="twitter:description" content="I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT",
      "item": "https://akhilvreddy.github.io/posts/attention-from-scratch/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT",
  "name": "Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT",
  "description": "I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: This post picks up where my last one ended. I dive deep into how attention evolved from a helpful tool to the backbone of modern deep learning. I covered multi-head self-attention, positional encodings, and the overall Transformer architecture while breaking down the math, intuition, along with some PyTorch code snippets.\nThe repsitory with the transformer implemented in PyTorch is available here.\nIn my last post, I walked through the path that led us from rule based and symbolic systems all the way to attention-based seq2seq models. We ended with a paradigm shift: letting the decoder look back at the encoder’s hidden states instead of bottlenecking everything into a single context vector.\nBut that was just the beginning of attention. In this post, I want to do two things:\nShow how researchers experimented with attention between 2015-2017. Stacking it, tweaking it, and realizing that we could do way more than guided decoding. Build a Vaswani style transformer model from scratch while discussing the design choices they made along the way. (Part 1) Attention between 2015-2017 What did Bahdanau discover? Bahdanau et al. came up with a powerful idea back in 2014. When struggling with decoding RNNs, they thought:\n“Instead of compressing the entire input sequence into a single context vector, let the decoder dynamically attend to different parts of the input at every step.”\nThis was a foundational shift from the way people were thinking at that time. They introduced:\nThe idea of storing intermediary hidden states from the encoder Additive attention - a scoring function that compares decoder states with encoder states The idea of computing alignment scores for each input token The context vector as a weighted sum of encoder hidden states Here’s how the architecutres changed\nVanilla seq2seq: input_seq ➡️ encoder ➡️ final hidden ➡️ decoder Bahdanau-style: input_seq ➡️ encoder ➡️ all hidden states ➡️ attention scores ➡️ decoder looks where it wants each step We are now giving the decoder the ability of where to look at each step, which fixed a bunch of issues:\nNo more fixed-size context vector It helped translation with longer sentences Showed that attention can be learned through training, not hardcoded This was very useful but it was a little limited in its abilities.\nRNN with attention: This is where we start to see the use of many encoder hidden states, all feeding into attention based context vectors used in the decoder.\nBuilding on Bahdanau: The Attention arms race After Bahdanau’s attention mechanism gained popularity, it became clear that attention wasn’t patching some issues, it was a new method of expressivity. More researchers started experimenting by pushing attention in different ways than what we had. In this section, we’ll walk thorugh those early innovations that paved the way for Transformers.\nScore Function Variants Researchers quickly started exploring alternatives to the way we compute alignment scores between encoder and decoder hidden states. Bahdanau’s additive attention was a feedforward network that took the decoder hidden state and each hidden state, concatenated them, passed them through a linear layer + tanh activation, and finally reduced it to a scalar score.\n$$ score(s_t, h_i) = v^\\top \\tanh(W_1 h_i + W_2 s_t) $$\nThis worked, but it wasn’t the most efficient. In 2015 some alternatives were introduced to make this process simpler, faster to compute, and easier to parallelize.\nDot Product scoring This is the simplest scoring function. No parameters and no extra computation was required for this.\n$$ score(s_t, h_i) = s_t^\\top h_i $$\nThe only assumption we have here is that both vectors are in the same space and comparable which works well when your hidden states are the same dimension.\nGeneral This adds a learnable weight matrix W to transform the encoder hidden state before scoring.\n$$ score(s_t, h_i) = s_t^\\top W h_i $$\nThis should remind you of Bahdanau’s attention because it kind of resembles it. However, it is way more lightweight with fewer parameters and better speed.\nBy 2017, dot product attention had become the default because of it’s speed. At it’s core it’s just matrix multiplication and that parallelizes well on GPUs. With scaling, it performs just as well (sometimes even better than) additive attention.\nMulti-Headed Attention Once dot product attnetion became the go-to, researchers started to ask a new question:\n“What if one attention head isn’t enough?”\nIf we think about it, this is actually a very reasonable question to ask. A single attention head can only learn one way to distribute focus. However, real language (or any real world data) is messy. Some words carry meaning, other words define structure, and a combination of words might bring out sentiment, syntax, or intent. Why are we forcing a single head to capture all of that? It kind of felt like a self-induced bottleneck.\nAsking this question brought us another huge upgrade.\nMulti-head attention = multiple parallel attention heads, each with its own learned projection of the input\nEach head learns a different view of the input, attends to different tokens, and operates in its own subspace. Stitching together many of these attention heads, they give the model a richer, more nuanced representation of the sequence.\nEach attention head in the diagram above learns to focus on different parts of the input sentence. Maybe one head focuses on the subject (“How”), another on verb tense (“are doing”), and another on sentence boundaries (”?”). These heads operate in parallel, each with their own learned projection of the input, creating separate “views” of the same sentence.\nLet’s take a simple example\nJohn gave his dog a bath because he was dirty. and let’s work through this in two different ways: Bahdanau-style single head attention and multi head attention.\nWith single head attention, that single head needs to:\nGuide every single word Caputre grammar Resolve ambiguity Identify semantic relationships Stay rich as input length grows This is like trying to listen to every voice in a room and summarize the conversation using only one ear. It might work fien when there aren’t many conversations going or the conversation lengths are short, but you will crumble under pressure.\nWith multi-head attention, different heads can specialize:\nHead 1 could learn coreference and know that “he” refers to “John” Head 2 could learn syntax and grammar and know that “his dog” is the more recent noun and might be the real referent (instead of John, the named entity) Head 3 could learn semantic roles and context and know to link “dirty” to “his dog” Each head builds its own interpretation, giving us a more complete understanding of the input with a distributed perspective.\nThis is again like trying to listen to every voice in a room and summarize the conversation but now with n ears. And we are going to let each ear specialize in what it is hearing. Then combining the information from all of these gives you a great understanding.\nSelf-Attention After dot product attention was working well and multi-head attention was starting to scale up, researchers asked one more question.\nWhat if we let tokens attend to themselves?\nThis was intra-sequence attention. Every token in a sentence was looking at every other token to decide what’s relevant and what isn’t. This became known as self-attention and it became the backbone of the Transformer.\nThe thickness of the lines corresponds to the scalar value of the attention score that we would get after doing self-attention.\nHere’s an example\nThe trophy doesn't fit in the suitcase because it is too big To understand what “it” refers to, the model needs to:\nRelate “it” to both words “trophy” and “suitcase” Understand the verb “fit” Use context to figure out that “trophy” is the thing that is too big With self attention, when the model is processing the word “it”, it has the ability to:\nLook back at “trophy” Glance at “suitcase” Consider the verb “fit” And it can weigh all of htem differently based on learned relevance. The best part? This can all happen in one pass. No recurrence required and no waiting. The math does get a little bit harder though.\nHow self-attention works Each token now becomes:\nA query (Q): “What am I looking for?” A key (K): “How relevant am I to others?” A value (V): “What information do I carry?” We compute attention scores by comparing the query for a token with the keys of all other tokens in the sentence.\nLet’s take one more example to see what Q, K, and V really mean.\nI poured water into the cup because it was empty Say suppose we got to the word “it”. The model needs to figure out what “it” refers to. Ideally, we need “it” to point to the “cup” not the “water”. Here’s how self attention resolves that:\n“it” becomes a query (Q) - “I’m trying to understand myself. Who should I pay attention to?” Every other word in the sentence (including “it” itself) has a key (K) and a value (v) We compute:\nscore(it, I) = Q_it ⋅ K_I score(it, poured) = Q_it ⋅ K_poured score(it, water) = Q_it ⋅ K_water score(it, into) = Q_it ⋅ K_into score(it, the) = Q_it ⋅ K_the score(it, cup) = Q_it ⋅ K_cup ← probably highest score(it, because) = Q_it ⋅ K_because score(it, was) = Q_it ⋅ K_was score(it, empty) = Q_it ⋅ K_empty score(it, it) = Q_it ⋅ K_it Softmaxing over all these scores gives us our attention weights (same thing what Badhandhu was doing). We then multiply each encoder value (V) by its weight. Summing this up, we now get a new context vector, supercharged with a lot more information. That context vector is what helps the model actually understand what “it” is referring to.\nIn this case,\n“cup” might have a key that signals it’s a container “empty” might help reinforce that interpretation semantically the softmax attention mechanism allows “it” to lean more on “cup”, “empty”, and “into” while simulatenously leaning less on “water” or “poured” This is attention in action. We compute attention weights by dot products of Q and K and and scale them down and apply softmax. We weight the values V by these attention scores to get our final output `z`: a context aware representation used by the model. The self-attention here is that this happens for every single token.\nI’ll rephrase it once again so that it hits home.\nThe model computes dot products between Q_it and all the keys (K_I, K_poured, etc.) and softmaxes them into attention weights. Each attention weight is then multiplied with the value (V) of the respective token. Essentially, we are blending the meaning of each token with how much it matters to the current token, where each value contributes proportionally based on how relevant its key was.\nQ, K, and V are not fixed vectors — they’re created by passing the input through three separate learnable linear layers. These layers are initialized (typically with Kaiming/He), and their weights are updated through training via backpropagation.\nDuring training, the model learns how to project the input tokens into query, key, and value spaces in a way that helps minimize loss. In multi-head attention, each head learns to focus on different patterns — like syntax, semantics, or positional relationships — by shaping its own Q, K, and V transformations over time.\nHence, in the case of self attention for the word “it”, if trained well, “cup” and “empty” will dominate that vector which helps the model correctly resolve the pronoun.\nIt does this for every single word in the sequence at the same time - that’s the power of self attention.\nThis is really useful because of multiple reasons:\nNo recurrence gives us a massive speed boost. Every token sees global context from the start. It’s easy to stack (just like conv layers in CNNs). And unlike RNNs/LSTMs, there’s nothing to forget. There’s no hidden state being passed forward and decaying over time. At every layer, the entire sequence is open game.\n(Part 2) Vaswani’s Transformer (from scratch) Dot product scoring, multi-head attention, and self-attention really started to change the way we think about things. A group of researchers at Google Brain were keen to combine these tools intelligently. Instead of passing state left to right like an RNN, they proposed a radically simple idea.\nLet every token look at every other token, at every layer, using attention — and stack these layers deep.\nIt was fast, parallelizable, and composable. More importantly, it performed insanely well — not just on translation, but on everything.\nIn this section, we’ll walk through how the Transformer is built, block by block, and implement it ourselves from scratch using functional PyTorch.\nHowever, let’s first start with the one thing RNNs had that attention didn’t: position.\nPositional Encoding (I personally struggled to understand this for a while, so I’ll try my best to explain it in a way that’s easily digestible.)\nIt’s funny that the things that RNNs do best is what attention didn’t have.\nAttention can look all over the input but it doesn’t know where anything is - self-attention is completely orderless. That’s pretty dangerous because “the cat sat on the mat” is very different from “the mat sat on the cat” — but attention alone wouldn’t know that.\nWe fix this by inlcuding positional information into the input embeddings of each token so the model can learn order as well.\nGive each position in the sequence a vector, and add it to the input embeddings.\nNow, token embeddings carry both what the word is and where it is.\nVaswani et al. didn’t just assign integers like 1, 2, 3, …\nThey used sinusoids — functions that encode position with multiple frequencies.\nWhy sinusoids?\nSmooth variation Generalizes to unseen sequence lengths No new parameters to learn The formula:\n$$ PE_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right), \\quad PE_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right) $$\nWhere:\npos = token position (0, 1, 2, …) i = dimension index d_model = embedding size (e.g., 512) Each dimension of the positional vector uses a different frequency: lower dims capture coarse position and higher dims capture fine-grained offsets.\nWe’re not just going around one circle. We’re going around many circles of different radii and frequencies at the same time.\nTo be totally honest, sinusoidal positional encoding made my head spin at first, especially that it’s deterministic. I learned to think of it like GPS coordinates.\nIf we train a system to understand GPS coordinates by learning from satellite signals, the system learns that\nIf signals A and B are 80% in phase, that usually means I’m 2 miles away If A and C are 30% out of phase, that usually means I’m 10 miles away Now even if it receives a new combination of signals from a place it’s never been, because the signal system is continuous and smooth, the system can extrapolate and say,\n“I know what this kind of signal means, even if I’ve never seen this exact combo before” which is called generalization from fixed sinusoids.\nIt can recognize positions it’s never seen before because the sin and cos waves continue smoothly for higher positions. So even if we input a sequence longer than anything during training, the positional pattern still makes sense. This part was Vaswani’s cherry on top in my opinion.\nThese encodings are deterministic because they just are sin and cos values. We can precompute a matrix of positional vectors ahead of time. And by adding them to the token embeddings before computing Q, K, and V, we allow the model to infer both content and position jointly.\nWhile the model doesn’t directly compare position vectors using dot products, it learns to interpret the combined patterns in a way that reflects how close or far other tokens are — across short and long ranges.\nHere’s the code behind it\nStart with token embeddings token_embeddings = embedding_layer(input_tokens) # shape: (seq_len, d_model) Add positional encodings to token embeddings input_with_pos = token_embeddings + positional_encoding # shape: (seq_len, d_model) Compute Q, K, and V Q = W_Q @ input_with_pos K = W_K @ input_with_pos V = W_V @ input_with_pos Positional encoding is added before any Q/K/V projection, so it directly affects all three. The position info is baked into the input that generates Q, K, and V — meaning attention scores and the final value aggregation both depend on where a token is.\nScaled Dot-Product Attention Now, every token has been transformed into a query, key, and value vector thanks to those learned linear projections. But given those, we need to be able to get our attention values.\nLet’s take a look at this same sentence again\nI poured water into the cup because it was empty Step 1 We first step by computing dot products between Q_it and every K_*.\nIf those vectors are similar, the dot product will be high. If it’s not similar the scores will be low.\nThis results in a vector or raw attention scores: one attention score per token in a sequence. Just like what we had earlier with Badhandhu attention.\nStep 2 Vaswani then introduced\n$$ \\text{attention score} = \\frac{Q K^T}{\\sqrt{d_k}} $$\nwhich scaled these attention scores down. The core reason is about the dot products becoming too huge, and softmax becomes super sharp (peaks in some parts and gradients die).\nScaling it down by the square root of d_k makes is so that softmax (and future gradients) behaves well.\nStep 3 Then, we apply softwax to get attention weights.\nWith the noramlized scores across all tokens, the softmax clearly tells us how much to attend to that token.\nStep 4 We need to put back meaning into each attention weight.\nWe take a weighted sum of all the values - each one scaled by how much attention we are paying to it:\n$$ \\text{output i} = \\sum_j \\text{weight i j} \\cdot V_j $$\nThis is the new representaiton for token i, contextualized by the other tokens.\nIf we had to put all these together in one life of code:\nscores = Q @ K.T / sqrt(d_k) # shape: (seq_len, seq_len) weights = softmax(scores, dim=-1) # shape: (seq_len, seq_len) output = weights @ V # shape: (seq_len, d_model) Multi-Head Attention We’ve gotten a good grasp on how attention works. But as we’ve seen before, one attention head can only focus on one “aspect” of the input. We tackle this by using multiple heads in parallel.\nEach head has its own learned projection of Q, K and, V which translate to different perspectives of the input. As we saw before, one head could capture coreference, another could capture syntax, and another could capture positional relationships.\nEach head\nQ = X @ W_q^head_i K = X @ W_k^head_i V = X @ W_v^head_i then attention happens per head\nsoftmax(Q @ K.T / sqrt(d_k)) @ V and finally we concatenate the outputs\nconcat(head_1, ..., head_h) @ W_o This lets the model process many relationships in parallel instead of being bottlenecked by one perspective.\nFeedforward Network After mutli-head attention, we now have a contextualized summary of your input. We pass this to a feedforward network\nFFN(x) = ReLU(x @ W1 + b1) @ W2 + b2 This applies to each position independently and also adds more expressiveness and nonlinearity.\nWe can think of this as a tiny MLP that proceses every token on its own, after it’s been contextually enriched by attention.\nResidual Connections + LayerNorm To help with gradient flow (and to avoid vanishing/exploding gradients) they added residuals and layernorm after every sublayer. This would try to restrict the variance of each layer.\nx = LayerNorm(x + Sublayer(x)) This gives some really good benefits:\nResidual helps preserve information LayerNorm stabilizes training And this patter is applied after both attention and FFN blocks - it helps in making the model sturdy in general.\nEncoder Layer Each encoder block has:\nMulti-Head self-attention Add input \u0026 LayerNorm Feedforward Network Add input \u0026 LayerNorm once again This transforms a sequence of tokens into context-rich hidden states that carry meaning and relationships.\nThis is stacked a various amout of times in practice (6 times in the original paper).\nHere’s what this layer would look like\n# x: (batch_size, seq_len, d_model) def encoder_layer(x, mask, d_model, num_heads): # multi-head self-attention attn_output = multi_head_attention(x, x, x, mask, d_model, num_heads) x = layer_norm(x + attn_output) # feedforward network ff_output = feed_forward(x, d_model) x = layer_norm(x + ff_output) return x Decoder Layer The decoder is autoregressive - it only generates next tokens based on what is has seen so far. It would be cheating to let the decoder look at future tokens.\nEach decoder block has:\nMasked Multi-Head Self-Attnetion Only attends to previous tokens (called a casual mask) Multi-head cross attention Decoder queries attend to encoder outputs Feedforwrad Network Add residual \u0026 LayerNorm after every block This gives the decoder the ability to look back at its own generated tokens (self attention), simulating how ChatGPT writes text. It can also peek at the encoder’s outputs (cross attention) and make predictions one token at a time.\nHere’s what this layer would look like\n# x: (batch, tgt_seq_len, d_model) def decoder_layer(x, enc_output, src_mask, tgt_mask, d_model, num_heads): # enc_output: (batch, src_seq_len, d_model) # masked multi-head self-attention (causal mask) attn1 = multi_head_attention(x, x, x, tgt_mask, d_model, num_heads) x = layer_norm(x + attn1) # cross attention (Q = decoder, K/V = encoder output) attn2 = multi_head_attention(x, enc_output, enc_output, src_mask, d_model, num_heads) x = layer_norm(x + attn2) # feedforward network ff_output = feed_forward(x, d_model) x = layer_norm(x + ff_output) return x Here are the key supporting functions that we would need to use\ndef multi_head_attention(q, k, v, mask, d_model, num_heads): head_dim = d_model // num_heads # linear projections for all heads Q = linear(q, d_model, num_heads) # shape: (batch, heads, seq_len, head_dim) K = linear(k, d_model, num_heads) V = linear(v, d_model, num_heads) # scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) weights = F.softmax(scores, dim=-1) output = torch.matmul(weights, V) # concatenate heads + final linear output = combine_heads(output, d_model) return final_linear(output, d_model) def feed_forward(x, d_model): hidden = F.relu(torch.matmul(x, W1) + b1) return torch.matmul(hidden, W2) + b2 And with that, there’s your transformer.\nThe power of the transformer With just these blocks, we can now design:\nAn encoder-only model like BERT A decoder-only model like GPT A full encoder-decoder setup for translation Because it was modular by design, the Transformer wasn’t just a better model. It became a platform.\nConclusion Transformers unlocked massive flexibility and scalability.\nStacking = Depth We could now layer Transformer blocks like LEGOs. We can easily add more layers which is what helped us to go from GPT-2 to GPT-4.\nSwapping blocks = Differnet model family Encoder only blocks gave us BERT-style masked language modeling. Decoder only blocks gave us GPT-style autoregressive text generation. Encoder blocks \u0026 Decoder blocks combined gave us S-tier translation models.\nTask-agnostic The transformer turned out to be general purpose as people pushed it’s limits. We can use it for all sorts of generation:\ntext ➝ text text ➝ image image ➝ caption audio ➝ text code ➝ docstring If you really think about it, it can do:\nanything ➝ anything because of how flexible it is.\nHardware efficiency Everything runs in parallel, from training to inference. With no recurrence, training and inference on transformers makes it highly parallelizable giving us the need for GPUs/TPUs.\nThis meant that we could scale up training massively without worrying about time bottlenecks from sequential operations.\nThat same architecture (attention + feedforward + layernorm) is now what powers everything from ChatGPT to DALL·E and Sora. Different modalities but they have the same brain.\nAnd that’s why we had an AI revolution in the 2020s.\n",
  "wordCount" : "3947",
  "inLanguage": "en",
  "datePublished": "2025-03-13T00:00:00Z",
  "dateModified": "2025-03-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "ML Theory + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/attention-from-scratch/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
          onload="renderMathInElement(document.body, {
            delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '\\(', right: '\\)', display: false},
              {left: '$', right: '$', display: false}
            ]
          });">
  </script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/reimplementations/" title="Reimplementations">
                    <span>Reimplementations</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT
    </h1>
    <div class="post-meta"><span title='2025-03-13 00:00:00 +0000 UTC'>March 13, 2025</span>&nbsp;·&nbsp;19 min&nbsp;·&nbsp;ML Theory &#43; Code

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: This post picks up where my last one ended. I dive deep into how attention evolved from a helpful tool to the backbone of modern deep learning. I covered multi-head self-attention, positional encodings, and the overall Transformer architecture while breaking down the math, intuition, along with some PyTorch code snippets.</p>
<p>The repsitory with the transformer implemented in PyTorch is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<p>In my <a href="https://akhilvreddy.com/posts/attention-beginnings/">last post</a>, I walked through the path that led us from rule based and symbolic systems all the way to attention-based seq2seq models. We ended with a paradigm shift: letting the decoder look back at the encoder’s hidden states instead of bottlenecking everything into a single context vector.</p>
<p>But that was just the beginning of attention. In this post, I want to do two things:</p>
<ol>
<li>Show how researchers experimented with attention between 2015-2017. Stacking it, tweaking it, and realizing that we could do way more than guided decoding.</li>
<li>Build a Vaswani style transformer model from scratch while discussing the design choices they made along the way.</li>
</ol>
<hr>
<h1 id="part-1-attention-between-2015-2017">(Part 1) Attention between 2015-2017<a hidden class="anchor" aria-hidden="true" href="#part-1-attention-between-2015-2017">#</a></h1>
<h2 id="what-did-bahdanau-discover">What did Bahdanau discover?<a hidden class="anchor" aria-hidden="true" href="#what-did-bahdanau-discover">#</a></h2>
<p>Bahdanau et al. came up with a powerful idea back in 2014. When struggling with decoding RNNs, they thought:</p>
<p>&ldquo;Instead of compressing the entire input sequence into a single context vector, let the decoder dynamically attend to different parts of the input at every step.&rdquo;</p>
<p>This was a foundational shift from the way people were thinking at that time. They introduced:</p>
<ul>
<li>The idea of storing intermediary hidden states from the encoder</li>
<li>Additive attention - a scoring function  that compares decoder states with encoder states</li>
<li>The idea of computing alignment scores for each input token</li>
<li>The context vector as a weighted sum of encoder hidden states</li>
</ul>
<p>Here&rsquo;s how the architecutres changed</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Vanilla seq2seq: 
</span></span><span style="display:flex;"><span>    input_seq ➡️ encoder ➡️ final hidden ➡️ decoder
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Bahdanau-style:
</span></span><span style="display:flex;"><span>    input_seq ➡️ encoder ➡️ all hidden states ➡️ attention scores ➡️ decoder looks where it wants each step
</span></span></code></pre></div><p>We are now giving the decoder the ability of <em>where to look at each step</em>, which fixed a bunch of issues:</p>
<ol>
<li>No more <em>fixed-size context vector</em></li>
<li>It helped translation with longer sentences</li>
<li>Showed that attention can be learned through training, not hardcoded</li>
</ol>
<p>This was very useful but it was a little limited in its abilities.</p>
<div align="center">
  <img src="/images/transformer1.png" alt="Skip-gram architecture" width="800"/>
  <p>RNN with attention: This is where we start to see the use of many encoder hidden states, all feeding into attention based context vectors used in the decoder.</p>
</div>
<h2 id="building-on-bahdanau-the-attention-arms-race">Building on Bahdanau: The Attention arms race<a hidden class="anchor" aria-hidden="true" href="#building-on-bahdanau-the-attention-arms-race">#</a></h2>
<p>After Bahdanau&rsquo;s attention mechanism gained popularity, it became clear that attention wasn&rsquo;t patching some issues, it was a new method of expressivity. More researchers started experimenting by pushing attention in different ways than what we had. In this section, we&rsquo;ll walk thorugh those early innovations that paved the way for Transformers.</p>
<h3 id="score-function-variants">Score Function Variants<a hidden class="anchor" aria-hidden="true" href="#score-function-variants">#</a></h3>
<p>Researchers quickly started exploring alternatives to the way we compute alignment scores between encoder and decoder hidden states. Bahdanau&rsquo;s additive attention was a feedforward network that took the decoder hidden state and each hidden state, concatenated them, passed them through a linear layer + tanh activation, and finally reduced it to a scalar score.</p>
<p>$$ score(s_t, h_i) = v^\top \tanh(W_1 h_i + W_2 s_t) $$</p>
<p>This worked, but it wasn&rsquo;t the most efficient. In 2015 some alternatives were introduced to make this process simpler, faster to compute, and easier to parallelize.</p>
<ol>
<li><strong>Dot Product scoring</strong></li>
</ol>
<p>This is the simplest scoring function. No parameters and no extra computation was required for this.</p>
<p>$$ score(s_t, h_i) = s_t^\top h_i $$</p>
<p>The only assumption we have here is that both vectors are in the same space and comparable which works well when your hidden states are the same dimension.</p>
<ol start="2">
<li><strong>General</strong></li>
</ol>
<p>This adds a learnable weight matrix <code>W</code> to transform the encoder hidden state before scoring.</p>
<p>$$ score(s_t, h_i) = s_t^\top W h_i $$</p>
<p>This should remind you of Bahdanau&rsquo;s attention because it kind of resembles it. However, it is way more lightweight with fewer parameters and better speed.</p>
<p>By 2017, dot product attention had become the default because of it&rsquo;s speed. At it&rsquo;s core it&rsquo;s just matrix multiplication and that parallelizes well on GPUs. With scaling, it performs just as well (sometimes even better than) additive attention.</p>
<h3 id="multi-headed-attention">Multi-Headed Attention<a hidden class="anchor" aria-hidden="true" href="#multi-headed-attention">#</a></h3>
<p>Once dot product attnetion became the go-to, researchers started to ask a new question:</p>
<p>&ldquo;What if one attention head isn&rsquo;t enough?&rdquo;</p>
<p>If we think about it, this is actually a very reasonable question to ask. A single attention head can only learn one way to distribute focus. However, real language (or any real world data) is messy. Some words carry meaning, other words define structure, and a combination of words might bring out sentiment, syntax, or intent. Why are we forcing a single head to capture all of that? It kind of felt like a self-induced bottleneck.</p>
<p>Asking this question brought us another huge upgrade.</p>
<blockquote>
<p>Multi-head attention = multiple parallel attention heads, each with its own learned projection of the input</p></blockquote>
<p>Each head learns a different view of the input, attends to different tokens, and operates in its own subspace. Stitching together many of these attention heads, they give the model a richer, more nuanced representation of the sequence.</p>
<div align="center">
  <img src="/images/transformer2.png" alt="Skip-gram architecture" width="800"/>
  <p>Each attention head in the diagram above learns to focus on different parts of the input sentence. Maybe one head focuses on the subject (“How”), another on verb tense (“are doing”), and another on sentence boundaries (”?”). These heads operate in parallel, each with their own learned projection of the input, creating separate “views” of the same sentence.</p>
</div>
<p>Let&rsquo;s take a simple example</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>John gave his dog a bath because he was dirty.
</span></span></code></pre></div><p>and let&rsquo;s work through this in two different ways: Bahdanau-style single head attention and multi head attention.</p>
<p>With single head attention, that single head needs to:</p>
<ul>
<li>Guide every single word</li>
<li>Caputre grammar</li>
<li>Resolve ambiguity</li>
<li>Identify semantic relationships</li>
<li>Stay rich as input length grows</li>
</ul>
<p>This is like trying to listen to every voice in a room and summarize the conversation using only one ear. It might work fien when there aren&rsquo;t many conversations going or the conversation lengths are short, but you will crumble under pressure.</p>
<p>With multi-head attention, different heads can specialize:</p>
<ul>
<li>Head 1 could learn coreference and know that &ldquo;he&rdquo; refers to &ldquo;John&rdquo;</li>
<li>Head 2 could learn syntax and grammar and know that &ldquo;his dog&rdquo; is the more recent noun and might be the real referent (instead of John, the named entity)</li>
<li>Head 3 could learn semantic roles and context and know to link &ldquo;dirty&rdquo; to &ldquo;his dog&rdquo;</li>
</ul>
<p>Each head builds its own interpretation, giving us a more complete understanding of the input with a distributed perspective.</p>
<p>This is again like trying to listen to every voice in a room and summarize the conversation but now with <code>n</code> ears. And we are going to let each ear specialize in what it is hearing. Then combining the information from all of these gives you a great understanding.</p>
<h3 id="self-attention">Self-Attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">#</a></h3>
<p>After dot product attention was working well and multi-head attention was starting to scale up, researchers asked one more question.</p>
<blockquote>
<p>What if we let tokens attend to themselves?</p></blockquote>
<p>This was intra-sequence attention. Every token in a sentence was looking at every <em>other</em> token to decide what&rsquo;s relevant and what isn&rsquo;t. This became known as self-attention and it became the backbone of the Transformer.</p>
<div align="center">
  <img src="/images/transformer3.png" alt="Skip-gram architecture" width="400"/>
  <p>The thickness of the lines corresponds to the scalar value of the attention score that we would get after doing self-attention.</p>
</div>
<p>Here&rsquo;s an example</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>The trophy doesn&#39;t fit in the suitcase because it is too big
</span></span></code></pre></div><p>To understand what &ldquo;it&rdquo; refers to, the model needs to:</p>
<ul>
<li>Relate &ldquo;it&rdquo; to both words &ldquo;trophy&rdquo; and &ldquo;suitcase&rdquo;</li>
<li>Understand the verb &ldquo;fit&rdquo;</li>
<li>Use context to figure out that &ldquo;trophy&rdquo; is the thing that is too big</li>
</ul>
<p>With self attention, when the model is processing the word &ldquo;it&rdquo;, it has the ability to:</p>
<ul>
<li>Look back at &ldquo;trophy&rdquo;</li>
<li>Glance at &ldquo;suitcase&rdquo;</li>
<li>Consider the verb &ldquo;fit&rdquo;</li>
</ul>
<p>And it can weigh all of htem differently based on learned relevance. The best part? This can all happen in one pass. No recurrence required and no waiting. The math does get a little bit harder though.</p>
<h4 id="how-self-attention-works">How self-attention works<a hidden class="anchor" aria-hidden="true" href="#how-self-attention-works">#</a></h4>
<p>Each token now becomes:</p>
<ul>
<li>A query (Q): &ldquo;What am I looking for?&rdquo;</li>
<li>A key (K): &ldquo;How relevant am I to others?&rdquo;</li>
<li>A value (V): &ldquo;What information do I carry?&rdquo;</li>
</ul>
<p>We compute attention scores by comparing the query for a token with the keys of all other tokens in the sentence.</p>
<p>Let&rsquo;s take one more example to see what Q, K, and V really mean.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>I poured water into the cup because it was empty
</span></span></code></pre></div><p>Say suppose we got to the word &ldquo;it&rdquo;. The model needs to figure out what &ldquo;it&rdquo; refers to. Ideally, we need &ldquo;it&rdquo; to point to the &ldquo;cup&rdquo; not the &ldquo;water&rdquo;. Here&rsquo;s how self attention resolves that:</p>
<ol>
<li>&ldquo;it&rdquo; becomes a query (Q) - &ldquo;I&rsquo;m trying to understand myself. Who should I pay attention to?&rdquo;</li>
<li>Every other word in the sentence (including &ldquo;it&rdquo; itself) has a key (K) and a value (v)</li>
</ol>
<p>We compute:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>score(it, I)       = Q_it ⋅ K_I
</span></span><span style="display:flex;"><span>score(it, poured)  = Q_it ⋅ K_poured
</span></span><span style="display:flex;"><span>score(it, water)   = Q_it ⋅ K_water
</span></span><span style="display:flex;"><span>score(it, into)    = Q_it ⋅ K_into
</span></span><span style="display:flex;"><span>score(it, the)     = Q_it ⋅ K_the
</span></span><span style="display:flex;"><span>score(it, cup)     = Q_it ⋅ K_cup ← probably highest
</span></span><span style="display:flex;"><span>score(it, because) = Q_it ⋅ K_because
</span></span><span style="display:flex;"><span>score(it, was)     = Q_it ⋅ K_was
</span></span><span style="display:flex;"><span>score(it, empty)   = Q_it ⋅ K_empty
</span></span><span style="display:flex;"><span>score(it, it)      = Q_it ⋅ K_it
</span></span></code></pre></div><p>Softmaxing over all these scores gives us our attention weights (same thing what Badhandhu was doing). We then multiply each encoder value (V) by its weight. Summing this up, we now get a new <strong>context vector</strong>, supercharged with a lot more information. That context vector is what helps the model actually understand what &ldquo;it&rdquo; is referring to.</p>
<p>In this case,</p>
<ul>
<li>&ldquo;cup&rdquo; might have a key that signals it&rsquo;s a container</li>
<li>&ldquo;empty&rdquo; might help reinforce that interpretation semantically</li>
<li>the softmax attention mechanism allows &ldquo;it&rdquo; to lean more on &ldquo;cup&rdquo;, &ldquo;empty&rdquo;, and &ldquo;into&rdquo; while simulatenously leaning less on &ldquo;water&rdquo; or &ldquo;poured&rdquo;</li>
</ul>
<div align="center">
  <img src="/images/transformer4.png" alt="Skip-gram architecture" width="600"/>
  <p>This is attention in action. We compute attention weights by dot products of Q and K and and scale them down and apply softmax. We weight the values V by these attention scores to get our final output `z`: a context aware representation used by the model. The <em>self-attention</em> here is that this happens <em>for every single token</em>.</p>
</div>
<p>I&rsquo;ll rephrase it once again so that it hits home.</p>
<p>The model computes dot products between <code>Q_it</code> and all the keys (<code>K_I</code>, <code>K_poured</code>, etc.) and softmaxes them into attention weights. Each attention weight is then multiplied with the value (V) of the respective token. Essentially, we are blending the meaning of each token with how much it matters to the current token, where each value contributes proportionally based on how relevant its key was.</p>
<p>Q, K, and V are not fixed vectors — they’re created by passing the input through three separate learnable linear layers. These layers are initialized (typically with Kaiming/He), and their weights are updated through training via backpropagation.</p>
<p>During training, the model learns how to project the input tokens into query, key, and value spaces in a way that helps minimize loss. In multi-head attention, each head learns to focus on different patterns — like syntax, semantics, or positional relationships — by shaping its own Q, K, and V transformations over time.</p>
<p>Hence, in the case of self attention for the word &ldquo;it&rdquo;, if trained well, &ldquo;cup&rdquo; and &ldquo;empty&rdquo; will dominate that vector which helps the model correctly resolve the pronoun.</p>
<p>It does this for every single word in the sequence <em>at the same time</em> - that&rsquo;s the power of self attention.</p>
<p>This is really useful because of multiple reasons:</p>
<ol>
<li>No recurrence gives us a massive speed boost.</li>
<li>Every token sees global context from the start.</li>
<li>It&rsquo;s easy to stack (just like conv layers in CNNs).</li>
</ol>
<p>And unlike RNNs/LSTMs, there&rsquo;s nothing to forget. There&rsquo;s no hidden state being passed forward and decaying over time. At every layer, the entire sequence is open game.</p>
<hr>
<h1 id="part-2-vaswanis-transformer-from-scratch">(Part 2) Vaswani&rsquo;s Transformer (from scratch)<a hidden class="anchor" aria-hidden="true" href="#part-2-vaswanis-transformer-from-scratch">#</a></h1>
<p>Dot product scoring, multi-head attention, and self-attention really started to change the way we think about things. A group of researchers at Google Brain were keen to combine these tools intelligently. Instead of passing state left to right like an RNN, they proposed a radically simple idea.</p>
<blockquote>
<p><strong>Let every token look at every other token, at every layer, using attention — and stack these layers deep.</strong></p></blockquote>
<p>It was fast, parallelizable, and composable. More importantly, it performed insanely well — not just on translation, but on everything.</p>
<p>In this section, we&rsquo;ll walk through how the Transformer is built, block by block, and implement it ourselves from scratch using functional PyTorch.</p>
<p>However, let&rsquo;s first start with the one thing RNNs had that attention didn’t: position.</p>
<h2 id="positional-encoding">Positional Encoding<a hidden class="anchor" aria-hidden="true" href="#positional-encoding">#</a></h2>
<p>(I personally struggled to understand this for a while, so I&rsquo;ll try my best to explain it in a way that&rsquo;s easily digestible.)</p>
<p>It&rsquo;s funny that the things that RNNs do best is what attention didn&rsquo;t have.</p>
<p>Attention can look all over the input but it doesn&rsquo;t know where anything is - self-attention is completely orderless. That’s pretty dangerous because “the cat sat on the mat” is very different from “the mat sat on the cat” — but attention alone wouldn’t know that.</p>
<p>We fix this by inlcuding positional information into the input embeddings of each token so the model can learn order as well.</p>
<blockquote>
<p>Give each position in the sequence a vector, and add it to the input embeddings.</p></blockquote>
<p>Now, token embeddings carry both <em>what the word is</em> and <em>where it is</em>.</p>
<p>Vaswani et al. didn’t just assign integers like 1, 2, 3, &hellip;<br>
They used sinusoids — functions that encode position with multiple frequencies.</p>
<p>Why sinusoids?</p>
<ul>
<li>Smooth variation</li>
<li>Generalizes to unseen sequence lengths</li>
<li>No new parameters to learn</li>
</ul>
<p>The formula:</p>
<p>$$
PE_{pos, 2i} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right), \quad
PE_{pos, 2i+1} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
$$</p>
<p>Where:</p>
<ul>
<li><code>pos</code> = token position (0, 1, 2, …)</li>
<li><code>i</code> = dimension index</li>
<li><code>d_model</code> = embedding size (e.g., 512)</li>
</ul>
<p>Each dimension of the positional vector uses a different frequency: lower dims capture coarse position and higher dims capture fine-grained offsets.</p>
<div align="center">
  <img src="/images/transformer5.png" alt="Skip-gram architecture" width="600"/>
  <p>We’re not just going around one circle. We’re going around many circles of different radii and frequencies at the same time.</p>
</div>
<p>To be totally honest, sinusoidal positional encoding made my head spin at first, especially that it&rsquo;s deterministic. I learned to think of it like GPS coordinates.</p>
<p>If we train a system to understand GPS coordinates by learning from satellite signals, the system learns that</p>
<ul>
<li>If signals A and B are 80% in phase, that usually means I’m 2 miles away</li>
<li>If A and C are 30% out of phase, that usually means I’m 10 miles away</li>
</ul>
<p>Now even if it receives a new combination of signals from a place it’s never been, because the signal system is continuous and smooth, the system can extrapolate and say,</p>
<p>“I know what this kind of signal means, even if I’ve never seen this exact combo before” which is called generalization from fixed sinusoids.</p>
<p>It can recognize positions it&rsquo;s never seen before because the sin and cos waves continue smoothly for higher positions. So even if we input a sequence longer than anything during training, the positional pattern still makes sense. This part was Vaswani&rsquo;s cherry on top in my opinion.</p>
<p>These encodings are deterministic because they just are sin and cos values. We can precompute a matrix of positional vectors ahead of time. And by adding them to the token embeddings before computing Q, K, and V, we allow the model to infer both content and position jointly.</p>
<p>While the model doesn’t directly compare position vectors using dot products, it learns to interpret the combined patterns in a way that reflects how close or far other tokens are — across short and long ranges.</p>
<p>Here&rsquo;s the code behind it</p>
<ol>
<li>Start with token embeddings</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>token_embeddings <span style="color:#f92672">=</span> embedding_layer(input_tokens)  <span style="color:#75715e"># shape: (seq_len, d_model)</span>
</span></span></code></pre></div><ol start="2">
<li>Add positional encodings to token embeddings</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>input_with_pos <span style="color:#f92672">=</span> token_embeddings <span style="color:#f92672">+</span> positional_encoding  <span style="color:#75715e"># shape: (seq_len, d_model)</span>
</span></span></code></pre></div><ol start="3">
<li>Compute Q, K, and V</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> W_Q <span style="color:#f92672">@</span> input_with_pos
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> W_K <span style="color:#f92672">@</span> input_with_pos
</span></span><span style="display:flex;"><span>V <span style="color:#f92672">=</span> W_V <span style="color:#f92672">@</span> input_with_pos
</span></span></code></pre></div><p>Positional encoding is added before any Q/K/V projection, so it directly affects all three. The position info is baked into the input that generates Q, K, and V — meaning attention scores and the final value aggregation both depend on where a token is.</p>
<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h2>
<p>Now, every token has been transformed into a query, key, and value vector thanks to those learned linear projections. But given those, we need to be able to get our attention values.</p>
<p>Let&rsquo;s take a look at this same sentence again</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>I poured water into the cup because it was empty
</span></span></code></pre></div><h4 id="step-1">Step 1<a hidden class="anchor" aria-hidden="true" href="#step-1">#</a></h4>
<p>We first step by computing dot products between Q_it and every K_*.</p>
<p>If those vectors are similar, the dot product will be high. If it&rsquo;s not similar the scores will be low.</p>
<p>This results in a vector or raw attention scores: one attention score per token in a sequence. Just like what we had earlier with Badhandhu attention.</p>
<h4 id="step-2">Step 2<a hidden class="anchor" aria-hidden="true" href="#step-2">#</a></h4>
<p>Vaswani then introduced</p>
<p>$$
\text{attention score} = \frac{Q K^T}{\sqrt{d_k}}
$$</p>
<p>which scaled these attention scores down. The core reason is about the dot products becoming too huge, and softmax becomes super sharp (peaks in some parts and gradients die).</p>
<p>Scaling it down by the square root of <code>d_k</code> makes is so that softmax (and future gradients) behaves well.</p>
<h4 id="step-3">Step 3<a hidden class="anchor" aria-hidden="true" href="#step-3">#</a></h4>
<p>Then, we apply softwax to get attention weights.</p>
<p>With the noramlized scores across all tokens, the softmax clearly tells us <strong>how much to attend to that token</strong>.</p>
<h4 id="step-4">Step 4<a hidden class="anchor" aria-hidden="true" href="#step-4">#</a></h4>
<p>We need to put back meaning into each attention weight.</p>
<p>We take a weighted sum of all the values - each one scaled by how much attention we are paying to it:</p>
<p>$$
\text{output i} = \sum_j \text{weight i j} \cdot V_j
$$</p>
<p>This is the new representaiton for token <code>i</code>, contextualized by the other tokens.</p>
<p>If we had to put all these together in one life of code:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>scores <span style="color:#f92672">=</span> Q <span style="color:#f92672">@</span> K<span style="color:#f92672">.</span>T <span style="color:#f92672">/</span> sqrt(d_k)      <span style="color:#75715e"># shape: (seq_len, seq_len)</span>
</span></span><span style="display:flex;"><span>weights <span style="color:#f92672">=</span> softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>) <span style="color:#75715e"># shape: (seq_len, seq_len)</span>
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> weights <span style="color:#f92672">@</span> V              <span style="color:#75715e"># shape: (seq_len, d_model)</span>
</span></span></code></pre></div><h2 id="multi-head-attention">Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h2>
<p>We&rsquo;ve gotten a good grasp on how attention works. But as we&rsquo;ve seen before, one attention head can only focus on one &ldquo;aspect&rdquo; of the input. We tackle this by using multiple heads in parallel.</p>
<p>Each head has its own <em>learned projection</em> of Q, K and, V which translate to different perspectives of the input. As we saw before, one head could capture coreference, another could capture syntax, and another could capture positional relationships.</p>
<p>Each head</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W_q<span style="color:#f92672">^</span>head_i
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W_k<span style="color:#f92672">^</span>head_i
</span></span><span style="display:flex;"><span>V <span style="color:#f92672">=</span> X <span style="color:#f92672">@</span> W_v<span style="color:#f92672">^</span>head_i
</span></span></code></pre></div><p>then attention happens <strong>per head</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>softmax(Q <span style="color:#f92672">@</span> K<span style="color:#f92672">.</span>T <span style="color:#f92672">/</span> sqrt(d_k)) <span style="color:#f92672">@</span> V
</span></span></code></pre></div><p>and finally we concatenate the outputs</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>concat(head_1, <span style="color:#f92672">...</span>, head_h) <span style="color:#f92672">@</span> W_o
</span></span></code></pre></div><p>This lets the model process many relationships in parallel instead of being bottlenecked by one perspective.</p>
<h2 id="feedforward-network">Feedforward Network<a hidden class="anchor" aria-hidden="true" href="#feedforward-network">#</a></h2>
<p>After mutli-head attention, we now have a contextualized summary of your input. We pass this to a feedforward network</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>FFN(x) <span style="color:#f92672">=</span> ReLU(x <span style="color:#f92672">@</span> W1 <span style="color:#f92672">+</span> b1) <span style="color:#f92672">@</span> W2 <span style="color:#f92672">+</span> b2
</span></span></code></pre></div><p>This applies to each position independently and also adds more expressiveness and nonlinearity.</p>
<p>We can think of this as a tiny MLP that proceses every token on its own, after it&rsquo;s been contextually enriched by attention.</p>
<h2 id="residual-connections--layernorm">Residual Connections + LayerNorm<a hidden class="anchor" aria-hidden="true" href="#residual-connections--layernorm">#</a></h2>
<p>To help with gradient flow (and to avoid vanishing/exploding gradients) they added residuals and layernorm after every sublayer. This would try to restrict the variance of each layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> LayerNorm(x <span style="color:#f92672">+</span> Sublayer(x))
</span></span></code></pre></div><p>This gives some really good benefits:</p>
<ul>
<li>Residual helps preserve information</li>
<li>LayerNorm stabilizes training</li>
</ul>
<p>And this patter is applied after both attention and FFN blocks - it helps in making the model sturdy in general.</p>
<h2 id="encoder-layer">Encoder Layer<a hidden class="anchor" aria-hidden="true" href="#encoder-layer">#</a></h2>
<p>Each encoder block has:</p>
<ul>
<li>Multi-Head self-attention</li>
<li>Add input &amp; LayerNorm</li>
<li>Feedforward Network</li>
<li>Add input &amp; LayerNorm once again</li>
</ul>
<p>This transforms a sequence of tokens into context-rich hidden states that carry meaning and relationships.</p>
<p>This is stacked a various amout of times in practice (6 times in the original paper).</p>
<p>Here&rsquo;s what this layer would look like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># x: (batch_size, seq_len, d_model)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encoder_layer</span>(x, mask, d_model, num_heads):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># multi-head self-attention</span>
</span></span><span style="display:flex;"><span>    attn_output <span style="color:#f92672">=</span> multi_head_attention(x, x, x, mask, d_model, num_heads)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> layer_norm(x <span style="color:#f92672">+</span> attn_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># feedforward network</span>
</span></span><span style="display:flex;"><span>    ff_output <span style="color:#f92672">=</span> feed_forward(x, d_model)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> layer_norm(x <span style="color:#f92672">+</span> ff_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><h2 id="decoder-layer">Decoder Layer<a hidden class="anchor" aria-hidden="true" href="#decoder-layer">#</a></h2>
<p>The decoder is autoregressive - it only generates next tokens based on what is has seen so far. It would be cheating to let the decoder look at future tokens.</p>
<p>Each decoder block has:</p>
<ol>
<li>Masked Multi-Head Self-Attnetion
Only attends to previous tokens (called a casual mask)</li>
<li>Multi-head cross attention
Decoder queries attend to encoder outputs</li>
<li>Feedforwrad Network</li>
<li>Add residual &amp; LayerNorm after every block</li>
</ol>
<p>This gives the decoder the ability to look back at its own generated tokens (self attention), simulating how ChatGPT writes text. It can also peek at the encoder&rsquo;s outputs (cross attention) and make predictions one token at a time.</p>
<p>Here&rsquo;s what this layer would look like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># x: (batch, tgt_seq_len, d_model)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decoder_layer</span>(x, enc_output, src_mask, tgt_mask, d_model, num_heads):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># enc_output: (batch, src_seq_len, d_model)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># masked multi-head self-attention (causal mask)</span>
</span></span><span style="display:flex;"><span>    attn1 <span style="color:#f92672">=</span> multi_head_attention(x, x, x, tgt_mask, d_model, num_heads)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> layer_norm(x <span style="color:#f92672">+</span> attn1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># cross attention (Q = decoder, K/V = encoder output)</span>
</span></span><span style="display:flex;"><span>    attn2 <span style="color:#f92672">=</span> multi_head_attention(x, enc_output, enc_output, src_mask, d_model, num_heads)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> layer_norm(x <span style="color:#f92672">+</span> attn2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># feedforward network</span>
</span></span><span style="display:flex;"><span>    ff_output <span style="color:#f92672">=</span> feed_forward(x, d_model)
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> layer_norm(x <span style="color:#f92672">+</span> ff_output)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> x
</span></span></code></pre></div><p>Here are the key supporting functions that we would need to use</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">multi_head_attention</span>(q, k, v, mask, d_model, num_heads):
</span></span><span style="display:flex;"><span>    head_dim <span style="color:#f92672">=</span> d_model <span style="color:#f92672">//</span> num_heads
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># linear projections for all heads</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> linear(q, d_model, num_heads)  <span style="color:#75715e"># shape: (batch, heads, seq_len, head_dim)</span>
</span></span><span style="display:flex;"><span>    K <span style="color:#f92672">=</span> linear(k, d_model, num_heads)
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> linear(v, d_model, num_heads)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># scaled dot-product attention</span>
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(Q, K<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(head_dim)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
</span></span><span style="display:flex;"><span>    weights <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(scores, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(weights, V)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># concatenate heads + final linear</span>
</span></span><span style="display:flex;"><span>    output <span style="color:#f92672">=</span> combine_heads(output, d_model)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> final_linear(output, d_model)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">feed_forward</span>(x, d_model):
</span></span><span style="display:flex;"><span>    hidden <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(torch<span style="color:#f92672">.</span>matmul(x, W1) <span style="color:#f92672">+</span> b1)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>matmul(hidden, W2) <span style="color:#f92672">+</span> b2
</span></span></code></pre></div><p>And with that, there&rsquo;s your transformer.</p>
<h2 id="the-power-of-the-transformer">The power of the transformer<a hidden class="anchor" aria-hidden="true" href="#the-power-of-the-transformer">#</a></h2>
<p>With just these blocks, we can now design:</p>
<ul>
<li>An encoder-only model like BERT</li>
<li>A decoder-only model like GPT</li>
<li>A full encoder-decoder setup for translation</li>
</ul>
<p>Because it was modular by design, the Transformer wasn&rsquo;t just a better model. It became a platform.</p>
<hr>
<h1 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h1>
<p>Transformers unlocked massive flexibility and scalability.</p>
<ol>
<li><strong>Stacking = Depth</strong></li>
</ol>
<p>We could now layer Transformer blocks like LEGOs. We can easily add more layers which is what helped us to go from GPT-2 to GPT-4.</p>
<ol start="2">
<li><strong>Swapping blocks = Differnet model family</strong></li>
</ol>
<p>Encoder only blocks gave us BERT-style masked language modeling.
Decoder only blocks gave us GPT-style autoregressive text generation.
Encoder blocks &amp; Decoder blocks combined gave us S-tier translation models.</p>
<ol start="3">
<li><strong>Task-agnostic</strong></li>
</ol>
<p>The transformer turned out to be general purpose as people pushed it&rsquo;s limits.
We can use it for all sorts of generation:</p>
<ul>
<li>text ➝ text</li>
<li>text ➝ image</li>
<li>image ➝ caption</li>
<li>audio ➝ text</li>
<li>code ➝ docstring</li>
</ul>
<p>If you really think about it, it can do:</p>
<ul>
<li>anything ➝ anything</li>
</ul>
<p>because of how flexible it is.</p>
<ol start="4">
<li><strong>Hardware efficiency</strong></li>
</ol>
<p><strong>Everything runs in parallel, from training to inference</strong>. With no recurrence, training and inference on transformers makes it highly parallelizable giving us the need for GPUs/TPUs.</p>
<p>This meant that we could scale up training massively without worrying about time bottlenecks from sequential operations.</p>
<blockquote>
<p>That same architecture (<em>attention + feedforward + layernorm</em>) is now what powers everything from ChatGPT to DALL·E and Sora. Different modalities but they have the same brain.</p></blockquote>
<p>And that&rsquo;s why we had an AI revolution in the 2020s.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
