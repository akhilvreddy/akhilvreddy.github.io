<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Deploying a toy ML model to production | Akhil‚Äôs Blog</title>
<meta name="keywords" content="">
<meta name="description" content="I&rsquo;ve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens after training.">
<meta name="author" content="ML Systems &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/deploying-model/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/deploying-model/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/deploying-model/">
  <meta property="og:site_name" content="Akhil‚Äôs Blog">
  <meta property="og:title" content="Deploying a toy ML model to production">
  <meta property="og:description" content="I‚Äôve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens after training.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-14T18:12:56-04:00">
    <meta property="article:modified_time" content="2025-06-14T18:12:56-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deploying a toy ML model to production">
<meta name="twitter:description" content="I&rsquo;ve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens after training.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Deploying a toy ML model to production",
      "item": "https://akhilvreddy.github.io/posts/deploying-model/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Deploying a toy ML model to production",
  "name": "Deploying a toy ML model to production",
  "description": "I\u0026rsquo;ve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens after training.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: This post walks through the full lifecycle of deploying a profanity detection model from training with sklearn to serving with FastAPI and BentoML, containerizing with Docker, automating CI/CD with GitHub Actions, and monitoring with Prometheus, Grafana, and Evidently. It‚Äôs a hands-on guide to turning a simple ML model into a production-grade service.\nThe repsitory with the corresponding PyTorch implementation is available here.\nPeople forget that in machine learning engineering, a good chunk of the work happens before modeling (cleaning, pre-processing, etc.) but also a good amount happens after training as well. Serving and deploying the model lets users touch your model safely (instead of you having to send the .pth file, someone loading it, and then finally evaluating with torch.no_grad()).\nIn this blog post I want to focus on what happens after we do machine learning and want to focus on the part that actually lets users touch ML models at scale.\nHere‚Äôs the high-level game plan:\nTrying to attack a real-problem The simplest model that I could think of which still has some use is profanity detection: it‚Äôs not that hard to train and anyone can hit it the API with any string. We would also only need a tiny corpus and having binary labels keeps the training loop pretty simple so that we can actually focus on the production part instead of drowning in things like feature engineering or other deep learning nuances.\nThe dataset I want to use is the Davidson 2017 ‚ÄúHate/Offensive‚Äù Tweets set which has around ~25000 balanced, single-line tweets.\nSince it‚Äôs binary we can map:\nhate + offensive speech ‚Üí 1 anything else ‚Üí 0 Train \u0026 lock the artifact We‚Äôll run a simple training loop (we can aim for a decent F1 but that‚Äôs not the main priority here) and save the model as a .joblib.\nWrapping it in an API This would be the most important part. I want to use FastAPI and BentoML; we‚Äôll use FastAPI for the clean routing and request handling and BentoML for model management, packaging, and scalable deployment.\nI‚Äôve seen people only use FastAPI but I feel that you don‚Äôt get full model versioning support or multi-model serving if you skip BentoML.\nPackage it I‚Äôll show the example of using both (dockerfile and bento build) to create an image in a registry.\nShipping We have a couple of options: Fly.io (heroku but for docker), Northflank (PaaS built for containers \u0026 microservices), ECS (reliable but I don‚Äôt want to pay), and K8s (overkill for this)\nFly.io seems the most viable one here because it has the least amount of overhead.\nWire CI/CD We‚Äôll use GitHub actions to build our image and deploy on every main push.\nObservability We‚Äôll implement logs, latency, traffic, model-drift hooks\nClosing steps A/B rollout, feature flags, retraining loop\nNow that we have a clear idea of what we are going to do, let‚Äôs get started.\nPretraining \u0026 Training Deep learning is definitely overkill here so I wanted to use TFIDF (Term Frequency - Inverse Document Frequency) along with Logistic Regression using sklearn. Essentially, TF-IDF gives high scores to words that are common in a document but rare across the dataset, which usually carry the most signal.\nLet‚Äôs take a quick example with movie reviews:\nLet‚Äôs say suppose we have 1000 movie reviews with 100 words each (very hypothetical). The word ‚Äúamazing‚Äù appears 10 times in one review, and in only 20 of the 1000 reviews. TF = 10 / total words in that review = 10 / 500 = 0.1 IDF = log(1000 / 20) = log(50) ‚âà 1.7 TF-IDF = 0.1 * 1.7 = 0.17 so ‚Äúamazing‚Äù gets a decently high score. But if the word was ‚Äúthe‚Äù (which is in basically every review), IDF would be near 0, so its TF-IDF score would be ~0.0005 ‚Üí not meaningful.\nThis method is great for spam detection, product categorization based on descriptions, basic sentiment analysis, and in our case, profanity/toxic comment classification.\nHere is what our loop would look like after we import our data:\ntrain.py\nimport pandas as pd, joblib from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline df = pd.read_csv(\"tweets.csv\") # columns: text, label X_train, X_val, y_train, y_val = train_test_split( df[\"text\"], df[\"label\"], test_size=0.2, random_state=42 ) pipeline = Pipeline([ (\"tfidf\", TfidfVectorizer(min_df=3, ngram_range=(1,2))), (\"clf\", LogisticRegression(max_iter=1000)) ]) pipeline.fit(X_train, y_train) print(\"val-acc:\", pipeline.score(X_val, y_val)) joblib.dump(pipeline, \"profanity.joblib\") So we now have our model saved as profanity.joblib and we can proceed to putting this to production now. In the repository I‚Äôve added eval metrics like F1 and confusion matrix but I don‚Äôt think they are that important in this case.\n2) Serving I‚Äôll start with showing just how to use FastAPI and then will introduce BentoML to show how it supercharges your workflow.\nFastAPI We would have to write a python script that exposes our API and then a dockerfile to serve.\nservice.py\nfrom fastapi import FastAPI from pydantic import BaseModel import joblib model = joblib.load(\"profanity.joblib\") app = FastAPI() class Req(BaseModel): text: str @app.post(\"/predict\") def predict(r: Req): proba = model.predict_proba([r.text])[0][1] return {\"is_profane\": proba \u003e 0.5, \"confidence\": proba} dockerfile\nFROM python:3.11-slim WORKDIR /app COPY requirements.txt . \u0026\u0026 pip install -r requirements.txt COPY . . CMD [\"uvicorn\", \"service:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"] Not bad right? Here‚Äôs how BentoML simplifies it though:\nservice.py\nimport bentoml from bentoml.io import JSON import joblib; pipe = joblib.load(\"profanity.joblib\") model_ref = bentoml.sklearn.save_model(\"profanity_detector\", pipe) svc = bentoml.Service( \"profanity_api\", runners=[model_ref.to_runner()], ) @svc.api(input=JSON(), output=JSON()) def predict(payload): text = payload[\"text\"] proba = svc.runners.profanity_detector.predict_proba.run([text])[0][1] return {\"is_profane\": proba \u003e 0.5, \"confidence\": proba} Serving locally:\nbentoml serve service.py:svc Packaging:\nbentoml build bentoml containerize profanity_api:latest docker run -p 3000:3000 profanity_api:latest Bento gives you healthz and metrics endpoints out the box. healthz is useful for when load balancers can ping to check if your service is healthy or needs to be restarted. metrics is a prometheus-compatible endpoint which exposes internal stats like request counts, response times, errors, memory usage, and latency histograms. This is really useful if you want to host your own observability serivce.\nWithout BentoML, you‚Äôd have to manually implement these endpoints which can be gruesome. Bento takes care of all of these and you instantly have them production-ready.\n3) Continuous Integration \u0026 Continous Deployment Here‚Äôs the pipeline flow:\nTrigger on push to main (from wherever you are developing from, or a PR gets merged) Checkout your current repository GitHub builds Bento Container gets built + pushed to GHRC flyctl deploy pulls from GHRC and deploys globally name: Deploy ML endpoint on: push: branches: [main] workflow_dispatch: {} jobs: build-push: runs-on: ubuntu-latest permissions: contents: read packages: write steps: - uses: actions/checkout@v4 - uses: bentoml/setup-bentoml-action@v1 with: python-version: \"3.11\" cache: pip - name: Build Bento run: bentoml build - name: Login to GHCR uses: docker/login-action@v3 with: registry: ghcr.io username: ${{ github.repository_owner }} password: ${{ secrets.GITHUB_TOKEN }} - name: Containerize \u0026 push uses: bentoml/containerize-push-action@v1 with: bento-tag: profanity_api:latest push: true tags: ghcr.io/${{ github.repository_owner }}/profanity-api:latest - name: Install flyctl uses: superfly/flyctl-actions/setup-flyctl@master - name: Deploy to Fly.io run: flyctl deploy --image ghcr.io/${{ github.repository_owner }}/profanity-api:latest --remote-only env: FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }} 4) Observability With BentoML, every service already exposes /metrics in Prometheus format without any extra code (which is extremely useful).\nWe can easily see what it is returning with the following:\ncurl http://localhost:3000/metrics | head # HELP http_request_duration_seconds ... # TYPE http_request_duration_seconds histogram All this data is already in prometheus format, which makes it super easy down the line.\nWe can quickly spin up Prom + Grafana:\ndocker compose up -d prometheus grafana And now we have observability baked in ü§ë\nHere‚Äôs the overall flow:\nBentoML exposes /metrics Prometheus scrapes + stores time-series data Grafana queries Prom + visualizes insights This obviously changes with the scale of your application - I would assume Youtube‚Äôs RecSys and other tools would have more sophisticated versions of observability. At it‚Äôs core they are measuring the same thing:\nhow their infrastructure is doing (monitoring CPU, memory, p95 latency) how the application is behaving (2xx/4xx/5xx counts, request time, queue depth) how the inputs to the model are (mean prob, class skew, confidence histograms) 5) Model \u0026 Data-Drift Monitoring Have you ever heard the term that ML models can silently fail in production? Unlike a software issue that is easy to spot, ML issues happen with the model producing incorrect (or even slightly incorrect) results. This happens when the input data changes and our model isn‚Äôt prepared for that.\nIn our case of the profanity checking model, that could happen if people could start saying new words / slurs that our model just has never seen before. In our case, we used a dataset from 2017. Every following year after that, the profane words and slurs that people say probably change slightly. Over years, this effect can compound and essentially break the model even if everything else (infra, APIs, etc.) is working well.\nA nightly cron + Evidently AI gives you guardrails against this happening.\nnightly_check.py\nfrom evidently import ColumnMapping from evidently.report import Report from evidently.metric_preset import DataDriftPreset baseline = pd.read_parquet(\"2025-06-01.parquet\") latest = pd.read_parquet(\"2025-06-28.parquet\") rpt = Report(metrics=[DataDriftPreset()]) rpt.run(reference_data=baseline, current_data=latest, column_mapping=ColumnMapping(target=None)) rpt.save_html(\"drift.html\") and for scheduling we can do\nimport schedule schedule.every().day.at(\"03:00\").do(run_data_drift_check) A lot of people then take this drift.html and store in in S3 which Grafana then embeds so product managers / non-technical people can see without having to ssh into the specific machine.\n6) Versioning, Rollbacks, Deployment Strategies This part is very similar to it‚Äôs software counterpart.\n1. Model Versioning Every time a new model is trained, it should be treated like a software artifact. We should go ahead and save it with a unique version ID along with some other metadata (training date, dataset version, training script hash, val \u0026 test set metrics (accuracy, F1, drift, etc.)).\nFor version tracking: MLflow, Weights \u0026 Biases, BentoML For storage: S3, GCS, MinIO This is very useful because we always know which model is in production, we have the ability to compare and debug past verisons, and reproduce behavior in production from months ago.\n2. Rollbacks Even if a new model looks promising in UAT with decent F1 scores, it can suddenly perform poorly in prod (drops in accuracy, increased latency, or user complaints).\nTo be able to roll back a model instantly, we can just rebind the production endpoint to an older model or roll back the image tag if it was hosted as a Docker container.\nThis can easily be automated by setting up triggers from Prometheus + Grafana + AlertManager which can go and trigger previous versions through CI/CD pipelines.\nHere‚Äôs how simple it can be\n# assume previous model version was \"profanity_api:v2\" and v3 is failing in prod # first pull the stable model docker pull ghcr.io/your-org/profanity_api:v2 # then stop the current container docker stop profanity-api docker rm profanity-api # finally re-run with previous version (v2) docker run -d --name profanity-api -p 3000:3000 ghcr.io/your-org/profanity_api:v2 3. Deployment Strategies Blue-Green\nDeploy the new model to a parallel env while the old one is still serving. After validation, switch all traffic over.\nCanary\nSlowly roll out the new model to a small subset of users (5-10%) while monitoring for issues. If things look good, gradually increase traffic.\nA/B testing\nRoute users randomly to different model versions (A vs B) and compare performance (CTR, conversions, etc.) Best for evaluating new models based on real user outcomes.\nMulti-Armed Bandit (MAB)\nA smarter version of A/B testing; we are going to route more traffic to the better-performing model as results come in. Dynamically allocating traffic based on current performance.\nShadow Deployment (my most favorite, tradeoff is that you are wasting compute)\nSpin up the new model but send real production traffic to both the models (old and new) without returning results from the new model to users. It shadows the main model, allowing you to compare logs and behavior.\n7) Automated Retraining Automated retraining is how we keep our model fresh as the world changes (data drift, concept drift, new patterns, etc.).\nWe can setup a pipeline to run on a schedule to:\nPull new data Clean + process it Retrain the model Evaluate it Upload to registry Deploy it (using one of the above strategies, if it beats current prod model) A potential stack that we could use is\nScheduler: Airflow (or cron if it‚Äôs simple) Storage: S3, GCS, or BigQuery Eval: sklearn.metrics or W\u0026B Registry: GHRC or MLflow Given this, we form a feedback loop\nData drift ‚Üí Model degrades ‚Üí Pipeline retrains ‚Üí Deploy an updated model All this keeps your system continuously learning without human intervention.\nThis ‚Äúself-updating model factory‚Äù is essential as you scale your model. At tech companies like Google, Meta, or Snap, this loop can run daily or weekly. Some teams even have a ‚Äúauto-promote‚Äù trigger (similar to the rollback that we saw above) that deploys the new model if it beats the current prod in eval tests.\nDeploying machine learning models isn‚Äôt really about beating your previous F1 or just writing highly optimized fused CUDA kernels to make inference as fast as possible. It‚Äôs about building systems around your model that make it work reliably in the real world. I personally feel that the fact that these models can have ‚Äúsilent failures‚Äù make it such that we need twice the effort to support an ML model compared to a software feature.\nTake a look at this repository to see the full code.\n",
  "wordCount" : "2218",
  "inLanguage": "en",
  "datePublished": "2025-06-14T18:12:56-04:00",
  "dateModified": "2025-06-14T18:12:56-04:00",
  "author":{
    "@type": "Person",
    "name": "ML Systems + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/deploying-model/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil‚Äôs Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil‚Äôs Blog (Alt + H)">Akhil‚Äôs Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Deploying a toy ML model to production
    </h1>
    <div class="post-meta"><span title='2025-06-14 18:12:56 -0400 EDT'>June 14, 2025</span>&nbsp;¬∑&nbsp;11 min&nbsp;¬∑&nbsp;ML Systems &#43; Code

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: This post walks through the full lifecycle of deploying a profanity detection model from training with sklearn to serving with FastAPI and BentoML, containerizing with Docker, automating CI/CD with GitHub Actions, and monitoring with Prometheus, Grafana, and Evidently. It‚Äôs a hands-on guide to turning a simple ML model into a production-grade service.</p>
<p>The repsitory with the corresponding PyTorch implementation is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<p>People forget that in machine learning engineering, a good chunk of the work happens before modeling (cleaning, pre-processing, etc.) but also a good amount happens after training as well. Serving and deploying the model lets users touch your model safely (instead of you having to send the .pth file, someone loading it, and then finally evaluating with <code>torch.no_grad()</code>).</p>
<p>In this blog post I want to focus on what happens <em>after</em> we do machine learning and want to focus on the part that actually lets users touch ML models at scale.</p>
<p>Here&rsquo;s the high-level game plan:</p>
<ol>
<li><strong>Trying to attack a real-problem</strong></li>
</ol>
<p>The simplest model that I could think of which still has some use is <strong>profanity detection</strong>: it&rsquo;s not that hard to train and anyone can hit it the API with any string. We would also only need a tiny corpus and having binary labels keeps the training loop pretty simple so that we can actually focus on the production part instead of drowning in things like feature engineering or other deep learning nuances.</p>
<p>The dataset I want to use is the <em>Davidson 2017 &ldquo;Hate/Offensive&rdquo; Tweets</em> set which has around ~25000 balanced, single-line tweets.</p>
<p>Since it&rsquo;s binary we can map:</p>
<ul>
<li>hate + offensive speech ‚Üí 1</li>
<li>anything else ‚Üí 0</li>
</ul>
<ol start="2">
<li><strong>Train &amp; lock the artifact</strong></li>
</ol>
<p>We&rsquo;ll run a simple training loop (we can aim for a decent F1 but that&rsquo;s not the main priority here) and save the model as a <code>.joblib</code>.</p>
<ol start="3">
<li><strong>Wrapping it in an API</strong></li>
</ol>
<p>This would be the most important part. I want to use FastAPI and BentoML; we&rsquo;ll use FastAPI for the clean routing and request handling and BentoML for model management, packaging, and scalable deployment.</p>
<p>I&rsquo;ve seen people only use FastAPI but I feel that you don&rsquo;t get full model versioning support or multi-model serving if you skip BentoML.</p>
<ol start="4">
<li><strong>Package it</strong></li>
</ol>
<p>I&rsquo;ll show the example of using both (dockerfile and bento build) to create an image in a registry.</p>
<ol start="5">
<li><strong>Shipping</strong></li>
</ol>
<p>We have a couple of options: Fly.io (heroku but for docker), Northflank (PaaS built for containers &amp; microservices), ECS (reliable but I don&rsquo;t want to pay), and K8s (overkill for this)</p>
<p>Fly.io seems the most viable one here because it has the least amount of overhead.</p>
<ol start="6">
<li><strong>Wire CI/CD</strong></li>
</ol>
<p>We&rsquo;ll use GitHub actions to build our image and deploy on every <code>main</code> push.</p>
<ol start="7">
<li><strong>Observability</strong></li>
</ol>
<p>We&rsquo;ll implement logs, latency, traffic, model-drift hooks</p>
<ol start="8">
<li><strong>Closing steps</strong></li>
</ol>
<p>A/B rollout, feature flags, retraining loop</p>
<p>Now that we have a clear idea of what we are going to do, let&rsquo;s get started.</p>
<hr>
<h3 id="pretraining--training">Pretraining &amp; Training<a hidden class="anchor" aria-hidden="true" href="#pretraining--training">#</a></h3>
<p>Deep learning is definitely overkill here so I wanted to use TFIDF (Term Frequency - Inverse Document Frequency) along with Logistic Regression using <code>sklearn</code>.  Essentially, TF-IDF gives high scores to words that are common in a document but rare across the dataset, which usually carry the most signal.</p>
<p>Let&rsquo;s take a quick example with movie reviews:</p>
<ul>
<li>Let&rsquo;s say suppose we have 1000 movie reviews with 100 words each (very hypothetical).</li>
<li>The word ‚Äúamazing‚Äù appears 10 times in one review, and in only 20 of the 1000 reviews.</li>
<li>TF = 10 / total words in that review = 10 / 500 = 0.1</li>
<li>IDF = log(1000 / 20) = log(50) ‚âà 1.7</li>
<li>TF-IDF = 0.1 * 1.7 = 0.17 so ‚Äúamazing‚Äù gets a decently high score.</li>
</ul>
<p>But if the word was ‚Äúthe‚Äù (which is in basically every review), IDF would be near 0, so its TF-IDF score would be ~0.0005 ‚Üí not meaningful.</p>
<p>This method is great for spam detection, product categorization based on descriptions, basic sentiment analysis, and in our case, profanity/toxic comment classification.</p>
<p>Here is what our loop would look like after we import our data:</p>
<p>train.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd<span style="color:#f92672">,</span> joblib
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.feature_extraction.text <span style="color:#f92672">import</span> TfidfVectorizer  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.pipeline <span style="color:#f92672">import</span> Pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;tweets.csv&#34;</span>) <span style="color:#75715e"># columns: text, label</span>
</span></span><span style="display:flex;"><span>X_train, X_val, y_train, y_val <span style="color:#f92672">=</span> train_test_split(
</span></span><span style="display:flex;"><span>    df[<span style="color:#e6db74">&#34;text&#34;</span>], df[<span style="color:#e6db74">&#34;label&#34;</span>], test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pipeline <span style="color:#f92672">=</span> Pipeline([
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;tfidf&#34;</span>, TfidfVectorizer(min_df<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, ngram_range<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>))),
</span></span><span style="display:flex;"><span>    (<span style="color:#e6db74">&#34;clf&#34;</span>,  LogisticRegression(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>))
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pipeline<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;val-acc:&#34;</span>, pipeline<span style="color:#f92672">.</span>score(X_val, y_val))
</span></span><span style="display:flex;"><span>joblib<span style="color:#f92672">.</span>dump(pipeline, <span style="color:#e6db74">&#34;profanity.joblib&#34;</span>)
</span></span></code></pre></div><p>So we now have our model saved as <code>profanity.joblib</code> and we can proceed to putting this to production now. In the repository I&rsquo;ve added eval metrics like F1 and confusion matrix but I don&rsquo;t think they are that important in this case.</p>
<h3 id="2-serving">2) Serving<a hidden class="anchor" aria-hidden="true" href="#2-serving">#</a></h3>
<p>I&rsquo;ll start with showing just how to use FastAPI and then will introduce BentoML to show how it supercharges your workflow.</p>
<h4 id="fastapi">FastAPI<a hidden class="anchor" aria-hidden="true" href="#fastapi">#</a></h4>
<p>We would have to write a python script that exposes our API and then a dockerfile to serve.</p>
<p>service.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> fastapi <span style="color:#f92672">import</span> FastAPI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> pydantic <span style="color:#f92672">import</span> BaseModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> joblib
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> joblib<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;profanity.joblib&#34;</span>)
</span></span><span style="display:flex;"><span>app <span style="color:#f92672">=</span> FastAPI()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Req</span>(BaseModel):
</span></span><span style="display:flex;"><span>    text: str
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.post</span>(<span style="color:#e6db74">&#34;/predict&#34;</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(r: Req):
</span></span><span style="display:flex;"><span>    proba <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict_proba([r<span style="color:#f92672">.</span>text])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;is_profane&#34;</span>: proba <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;confidence&#34;</span>: proba}
</span></span></code></pre></div><p>dockerfile</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-docker" data-lang="docker"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> python:3.11-slim</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">WORKDIR</span><span style="color:#e6db74"> /app</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> requirements.txt . <span style="color:#f92672">&amp;&amp;</span> pip install -r requirements.txt<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> . .<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">CMD</span> [<span style="color:#e6db74">&#34;uvicorn&#34;</span>, <span style="color:#e6db74">&#34;service:app&#34;</span>, <span style="color:#e6db74">&#34;--host&#34;</span>, <span style="color:#e6db74">&#34;0.0.0.0&#34;</span>, <span style="color:#e6db74">&#34;--port&#34;</span>, <span style="color:#e6db74">&#34;8000&#34;</span>]<span style="color:#960050;background-color:#1e0010">
</span></span></span></code></pre></div><p>Not bad right? Here&rsquo;s how BentoML simplifies it though:</p>
<p>service.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> bentoml
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> bentoml.io <span style="color:#f92672">import</span> JSON
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> joblib; pipe <span style="color:#f92672">=</span> joblib<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;profanity.joblib&#34;</span>)
</span></span><span style="display:flex;"><span>model_ref <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>sklearn<span style="color:#f92672">.</span>save_model(<span style="color:#e6db74">&#34;profanity_detector&#34;</span>, pipe)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>svc <span style="color:#f92672">=</span> bentoml<span style="color:#f92672">.</span>Service(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;profanity_api&#34;</span>,
</span></span><span style="display:flex;"><span>    runners<span style="color:#f92672">=</span>[model_ref<span style="color:#f92672">.</span>to_runner()],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@svc.api</span>(input<span style="color:#f92672">=</span>JSON(), output<span style="color:#f92672">=</span>JSON())
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>(payload):
</span></span><span style="display:flex;"><span>    text <span style="color:#f92672">=</span> payload[<span style="color:#e6db74">&#34;text&#34;</span>]
</span></span><span style="display:flex;"><span>    proba <span style="color:#f92672">=</span> svc<span style="color:#f92672">.</span>runners<span style="color:#f92672">.</span>profanity_detector<span style="color:#f92672">.</span>predict_proba<span style="color:#f92672">.</span>run([text])[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;is_profane&#34;</span>: proba <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#e6db74">&#34;confidence&#34;</span>: proba}
</span></span></code></pre></div><p>Serving locally:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>bentoml serve service.py:svc
</span></span></code></pre></div><p>Packaging:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>bentoml build
</span></span><span style="display:flex;"><span>bentoml containerize profanity_api:latest
</span></span><span style="display:flex;"><span>docker run -p 3000:3000 profanity_api:latest
</span></span></code></pre></div><p>Bento gives you <code>healthz</code> and <code>metrics</code> endpoints out the box. <code>healthz</code> is useful for when load balancers can ping to check if your service is healthy or needs to be restarted. <code>metrics</code> is a prometheus-compatible endpoint which exposes internal stats like request counts, response times, errors, memory usage, and latency histograms. This is really useful if you want to host your own observability serivce.</p>
<p>Without BentoML, you&rsquo;d have to manually implement these endpoints which can be gruesome. Bento takes care of all of these and you instantly have them production-ready.</p>
<h3 id="3-continuous-integration--continous-deployment">3) Continuous Integration &amp; Continous Deployment<a hidden class="anchor" aria-hidden="true" href="#3-continuous-integration--continous-deployment">#</a></h3>
<p>Here&rsquo;s the pipeline flow:</p>
<ol>
<li>Trigger on push to main (from wherever you are developing from, or a PR gets merged)</li>
<li>Checkout your current repository</li>
<li>GitHub builds Bento</li>
<li>Container gets built + pushed to GHRC</li>
<li><code>flyctl deploy</code> pulls from GHRC and deploys globally</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">Deploy ML endpoint</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">on</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">push</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">branches</span>: [<span style="color:#ae81ff">main]</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">workflow_dispatch</span>: {}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">jobs</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">build-push</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">runs-on</span>: <span style="color:#ae81ff">ubuntu-latest</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">permissions</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">contents</span>: <span style="color:#ae81ff">read</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">packages</span>: <span style="color:#ae81ff">write</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">steps</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">actions/checkout@v4</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">bentoml/setup-bentoml-action@v1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">python-version</span>: <span style="color:#e6db74">&#34;3.11&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">cache</span>: <span style="color:#ae81ff">pip</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Build Bento</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">run</span>: <span style="color:#ae81ff">bentoml build</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Login to GHCR</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">docker/login-action@v3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">registry</span>: <span style="color:#ae81ff">ghcr.io</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">username</span>: <span style="color:#ae81ff">${{ github.repository_owner }}</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">password</span>: <span style="color:#ae81ff">${{ secrets.GITHUB_TOKEN }}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Containerize &amp; push</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">bentoml/containerize-push-action@v1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">bento-tag</span>: <span style="color:#ae81ff">profanity_api:latest</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">push</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">tags</span>: <span style="color:#ae81ff">ghcr.io/${{ github.repository_owner }}/profanity-api:latest</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Install flyctl</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">superfly/flyctl-actions/setup-flyctl@master</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Deploy to Fly.io</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">run</span>: <span style="color:#ae81ff">flyctl deploy --image ghcr.io/${{ github.repository_owner }}/profanity-api:latest --remote-only</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">FLY_API_TOKEN</span>: <span style="color:#ae81ff">${{ secrets.FLY_API_TOKEN }}</span>
</span></span></code></pre></div><h3 id="4-observability">4) Observability<a hidden class="anchor" aria-hidden="true" href="#4-observability">#</a></h3>
<p>With BentoML, every service already exposes <code>/metrics</code> in Prometheus format without any extra code (which is extremely useful).</p>
<p>We can easily see what it is returning with the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl http://localhost:3000/metrics | head
</span></span><span style="display:flex;"><span><span style="color:#75715e"># HELP http_request_duration_seconds ...</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># TYPE http_request_duration_seconds histogram</span>
</span></span></code></pre></div><p>All this data is already in prometheus format, which makes it super easy down the line.</p>
<p>We can quickly spin up Prom + Grafana:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker compose up -d prometheus grafana
</span></span></code></pre></div><p>And now we have observability baked in ü§ë</p>
<p>Here&rsquo;s the overall flow:</p>
<ol>
<li>BentoML exposes <code>/metrics</code></li>
<li>Prometheus scrapes + stores time-series data</li>
<li>Grafana queries Prom + visualizes insights</li>
</ol>
<p>This obviously changes with the scale of your application - I would assume Youtube&rsquo;s RecSys and other tools would have more sophisticated versions of observability. At it&rsquo;s core they are measuring the same thing:</p>
<ul>
<li>how their infrastructure is doing (monitoring CPU, memory, p95 latency)</li>
<li>how the application is behaving (2xx/4xx/5xx counts, request time, queue depth)</li>
<li>how the <strong>inputs to the model are</strong> (mean prob, class skew, confidence histograms)</li>
</ul>
<h3 id="5-model--data-drift-monitoring">5) Model &amp; Data-Drift Monitoring<a hidden class="anchor" aria-hidden="true" href="#5-model--data-drift-monitoring">#</a></h3>
<p>Have you ever heard the term that ML models can silently fail in production? Unlike a software issue that is easy to spot, ML issues happen with the model producing incorrect (or even slightly incorrect) results. This happens when the input data changes and our model isn&rsquo;t prepared for that.</p>
<p>In our case of the profanity checking model, that could happen if people could start saying new words / slurs that our model just has never seen before. In our case, we used a dataset from 2017. Every following year after that, the profane words and slurs that people say probably change slightly. Over years, this effect can compound and essentially break the model even if everything else (infra, APIs, etc.) is working well.</p>
<p>A nightly cron + Evidently AI gives you guardrails against this happening.</p>
<p>nightly_check.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> evidently <span style="color:#f92672">import</span> ColumnMapping
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> evidently.report <span style="color:#f92672">import</span> Report
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> evidently.metric_preset <span style="color:#f92672">import</span> DataDriftPreset
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>baseline <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_parquet(<span style="color:#e6db74">&#34;2025-06-01.parquet&#34;</span>)
</span></span><span style="display:flex;"><span>latest <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_parquet(<span style="color:#e6db74">&#34;2025-06-28.parquet&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>rpt <span style="color:#f92672">=</span> Report(metrics<span style="color:#f92672">=</span>[DataDriftPreset()])
</span></span><span style="display:flex;"><span>rpt<span style="color:#f92672">.</span>run(reference_data<span style="color:#f92672">=</span>baseline, current_data<span style="color:#f92672">=</span>latest, column_mapping<span style="color:#f92672">=</span>ColumnMapping(target<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>))
</span></span><span style="display:flex;"><span>rpt<span style="color:#f92672">.</span>save_html(<span style="color:#e6db74">&#34;drift.html&#34;</span>)
</span></span></code></pre></div><p>and for scheduling we can do</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> schedule
</span></span><span style="display:flex;"><span>schedule<span style="color:#f92672">.</span>every()<span style="color:#f92672">.</span>day<span style="color:#f92672">.</span>at(<span style="color:#e6db74">&#34;03:00&#34;</span>)<span style="color:#f92672">.</span>do(run_data_drift_check)
</span></span></code></pre></div><p>A lot of people then take this <code>drift.html</code> and store in in S3 which Grafana then embeds so product managers / non-technical people can see without having to ssh into the specific machine.</p>
<h3 id="6-versioning-rollbacks-deployment-strategies">6) Versioning, Rollbacks, Deployment Strategies<a hidden class="anchor" aria-hidden="true" href="#6-versioning-rollbacks-deployment-strategies">#</a></h3>
<p>This part is very similar to it&rsquo;s software counterpart.</p>
<h4 id="1-model-versioning">1. Model Versioning<a hidden class="anchor" aria-hidden="true" href="#1-model-versioning">#</a></h4>
<p>Every time a new model is trained, it should be treated like a software artifact. We should go ahead and save it with a unique version ID along with some other metadata (training date, dataset version, training script hash, val &amp; test set metrics (accuracy, F1, drift, etc.)).</p>
<ul>
<li>For version tracking: MLflow, Weights &amp; Biases, BentoML</li>
<li>For storage: S3, GCS, MinIO</li>
</ul>
<p>This is very useful because we always know which model is in production, we have the ability to compare and debug past verisons, and reproduce behavior in production from months ago.</p>
<h4 id="2-rollbacks">2. Rollbacks<a hidden class="anchor" aria-hidden="true" href="#2-rollbacks">#</a></h4>
<p>Even if a new model looks promising in UAT with decent F1 scores, it can suddenly perform poorly in prod (drops in accuracy, increased latency, or user complaints).</p>
<p>To be able to roll back a model instantly, we can just rebind the production endpoint to an older model or roll back the image tag if it was hosted as a Docker container.</p>
<p>This can easily be automated by setting up triggers from Prometheus + Grafana + AlertManager which can go and trigger previous versions through CI/CD pipelines.</p>
<p>Here&rsquo;s how simple it can be</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># assume previous model version was &#34;profanity_api:v2&#34; and v3 is failing in prod</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># first pull the stable model</span>
</span></span><span style="display:flex;"><span>docker pull ghcr.io/your-org/profanity_api:v2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># then stop the current container</span>
</span></span><span style="display:flex;"><span>docker stop profanity-api
</span></span><span style="display:flex;"><span>docker rm profanity-api
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># finally re-run with previous version (v2)</span>
</span></span><span style="display:flex;"><span>docker run -d --name profanity-api -p 3000:3000 ghcr.io/your-org/profanity_api:v2
</span></span></code></pre></div><h4 id="3-deployment-strategies">3. Deployment Strategies<a hidden class="anchor" aria-hidden="true" href="#3-deployment-strategies">#</a></h4>
<p><strong>Blue-Green</strong></p>
<p>Deploy the new model to a parallel env while the old one is still serving. After validation, switch all traffic over.</p>
<p><strong>Canary</strong></p>
<p>Slowly roll out the new model to a small subset of users (5-10%) while monitoring for issues. If things look good, gradually increase traffic.</p>
<p><strong>A/B testing</strong></p>
<p>Route users randomly to different model versions (A vs B) and compare performance (CTR, conversions, etc.) Best for evaluating new models based on real user outcomes.</p>
<p><strong>Multi-Armed Bandit (MAB)</strong></p>
<p>A smarter version of A/B testing; we are going to route more traffic to the better-performing model as <em>results come in</em>. Dynamically allocating traffic based on current performance.</p>
<p><strong>Shadow Deployment</strong> (my most favorite, tradeoff is that you are wasting compute)</p>
<p>Spin up the new model but send real production traffic to both the models (old and new) <em>without returning results from the new model to users</em>. It shadows the main model, allowing you to compare logs and behavior.</p>
<h3 id="7-automated-retraining">7) Automated Retraining<a hidden class="anchor" aria-hidden="true" href="#7-automated-retraining">#</a></h3>
<p>Automated retraining is how we keep our model fresh as the world changes (data drift, concept drift, new patterns, etc.).</p>
<p>We can setup a pipeline to run on a schedule to:</p>
<ol>
<li>Pull new data</li>
<li>Clean + process it</li>
<li>Retrain the model</li>
<li>Evaluate it</li>
<li>Upload to registry</li>
<li>Deploy it (using one of the above strategies, if it beats current prod model)</li>
</ol>
<p>A potential stack that we could use is</p>
<ul>
<li>Scheduler: Airflow (or cron if it&rsquo;s simple)</li>
<li>Storage: S3, GCS, or BigQuery</li>
<li>Eval: sklearn.metrics or W&amp;B</li>
<li>Registry: GHRC or MLflow</li>
</ul>
<p>Given this, we form a feedback loop</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Data drift ‚Üí Model degrades ‚Üí Pipeline retrains ‚Üí Deploy an updated model
</span></span></code></pre></div><p>All this keeps your system continuously learning without human intervention.</p>
<p>This &ldquo;self-updating model factory&rdquo; is essential as you scale your model. At tech companies like Google, Meta, or Snap, this loop can run daily or weekly. Some teams even have a &ldquo;auto-promote&rdquo; trigger (similar to the rollback that we saw above) that deploys the new model if it beats the current prod in eval tests.</p>
<hr>
<p>Deploying machine learning models isn&rsquo;t really about beating your previous F1 or just writing highly optimized fused CUDA kernels to make inference as fast as possible. It&rsquo;s about building systems around your model that make it work reliably in the real world. I personally feel that the fact that these models can have &ldquo;silent failures&rdquo; make it such that we need twice the effort to support an ML model compared to a software feature.</p>
<p>Take a look at <a href="https://github.com/akhilvreddy/profanity-to-prod">this repository</a> to see the full code.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
