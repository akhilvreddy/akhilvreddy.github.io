<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Why I think Tesla&#39;s Robotaxi will do laps on Waymo | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="In the coming years, I have a feeling that Robotaxi is going to scale like Uber whereas Waymo will scale like Lyft (if even). The comparison becomes lean deep neural networks vs. redundant vision &amp; sensors.">
<meta name="author" content="Computer Vision &amp; Personal Opinions">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="Why I think Tesla&#39;s Robotaxi will do laps on Waymo">
  <meta property="og:description" content="In the coming years, I have a feeling that Robotaxi is going to scale like Uber whereas Waymo will scale like Lyft (if even). The comparison becomes lean deep neural networks vs. redundant vision &amp; sensors.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-24T18:12:56-04:00">
    <meta property="article:modified_time" content="2025-06-24T18:12:56-04:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Why I think Tesla&#39;s Robotaxi will do laps on Waymo">
<meta name="twitter:description" content="In the coming years, I have a feeling that Robotaxi is going to scale like Uber whereas Waymo will scale like Lyft (if even). The comparison becomes lean deep neural networks vs. redundant vision &amp; sensors.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Why I think Tesla's Robotaxi will do laps on Waymo",
      "item": "https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Why I think Tesla's Robotaxi will do laps on Waymo",
  "name": "Why I think Tesla\u0027s Robotaxi will do laps on Waymo",
  "description": "In the coming years, I have a feeling that Robotaxi is going to scale like Uber whereas Waymo will scale like Lyft (if even). The comparison becomes lean deep neural networks vs. redundant vision \u0026amp; sensors.",
  "keywords": [
    
  ],
  "articleBody": "Tesla’s Robotaxis and Google’s Waymo solve the same problem in two vastly different ways but I believe that the approach that tesla is taking is going to out-scale Waymo’s approach in a few years. In my opinion, tesla not only picked the better approach, they have the greatest secret weapon of all time (espeically in machine learning): data.\nRobotaxi uses an end-to-end neural network driver with 8 cameras around the car. Essentially they are betting on the fact that if the neural network understands the current driving conditions, cheap hardware lets them scale up to millions.\nWaymo leans on a sensor fusion stack: lidar, radar, cameras, HD maps—prioritizing redundancy, and measurable safety.\nIn essance, this is the same trade-off we see all the time in Computer Science but now wrapped in self driving cars: simplicity \u0026 scale vs. redundancy \u0026 robustness.\nLet’s dive into their individual software stacks:\nSensor Philosophy: Monocular vs. Multispectral Tesla: “Eyes Only, like a human”\n8 × HDR cameras (plus a front “tri‑cam” setup) give a 360° bubble.\nBeta units are regaining a slim HD radar to assist in rain/fog, but lidar is a four‑letter word in Palo Alto.\nGoal: keep sensor BOM ≈ $400 so every Model Y can moonlight as a robotaxi.\n–\nWaymo: “See Everything, Twice”\n29 cameras, 5 lidars, 6 imaging radars per car.\n360° lidar range to 300 m, down‑to‑5 cm depth resolution.\nOnboard heaters + cleaning nozzles because hardware this fancy hates LA/Phoenix dust.\nTesla’s network must infer depth \u0026 occupancy from pure images whereas Waymo just measures it in point clouds, bringing out the cheaper vs. easier argument. However, as technology like Neural Effective Field Radiance improves, inferring depth from plain 2 dimensional images is not that diffcult for these nets. Also, humans don’t have lidar attached to them - we take 2 separate 2D images (from each eye) slightly spaced apart and then stitch them together and our brain is the one that makes thigns have depth. This is the exact thing that tesla is betting on. If humans only drive by seeing 2D images of the world, they think that their car can do it as well. And to be honest, Robotaxis do drive pretty well just by seeing images of the world.\nIf we look at Waymo, it is definitely overkill. They not only capture images from 29 cameras, they have LiDAR senesor across multiple parts of the vehicle, making it super easy to sense how close / far objects are to you. That’s useful, but if tesla is inferring depth without external LiDARs on top, that method will scale much easier.\nPerception Pipeline: From Pixels/Points → Drivable Space Stage Tesla (FSD v12) Waymo (5th-Gen Stack) Model Backbone HydraNet is tesla’s multi-task CNN -\u003e Occupancy Network voxelizes (fancy word to say map to 3D) the world around the car Sensor-specific CNNs + BEV transformer -\u003e features are fused into a unified grid Tasks Object, lane, traffic lights, freespace predicted in one shot Separate output heads for detection, velocity, and intent prediction Planner End-to-end policy network which outputs steering and acceleration Rule based layer based on the flow Perception, Prediction, then Planning If we look at it like this, I really like the backbone models for both. Waymo’s Bird’s Eye View Transformer (BEVFormer) projects multiple camera views into a top-down feature map. Cross-attention then goes and collects spatial cues like object boundaries and lane markings by querying relevant regions from different camera perspectives. It then uses temporal self-attention by attending to past BEV frames to capture motion cues and ensure consistency across predictions.\nHere’s some pseudo-code I found online representing what the BEVFormer architecture looks like:\nB, V, C, H, W = 2, 6, 128, 256, 256 # batch, views (cams), channels, height, width # 1) Backbone encodes each camera frame cam_feats = backbone(frames.view(B*V, 3, H, W)) # -\u003e [B*V, C, H/4, W/4] cam_feats = cam_feats.view(B, V, C, H//4, W//4) # 2) Lift‑Splat to bird’s‑eye bev_feats = lift_splat(cam_feats, intrinsics, extrinsics) # [B, C, X, Y] # 3) Fuse lidar voxels via cross‑attention lidar_vox = voxelize(lidar_points) # [B, C, X, Y] bev_fused = bev_xattn(bev_feats, lidar_vox) # transformer cross‑attn # 4) Detection queries (DETR‑style) queries = torch.zeros(B, 900, 256) # learnable object queries obj_feats = transformer_decoder(queries, bev_fused) # 5) Heads → 3‑D boxes and class probs boxes_3d, cls_logits = box_head(obj_feats) Tesla’s HydraNet has some similarities but also some differences. It combines a CNN-based feature extraction with transofrmer-based fusion layers and temporal modules. Tesla uses a RegNet architecture (a CNN super similar to ResNet) for initial feature extraction. ConvLSTM units build short-term temporal awareness with a final Feature Pyramind Network for multi-scale processing. They then feed these features into transformer-based cross-attention blocks which learn to combine information across all cameras to build a multi view representation.\nHere’s some pseudocode of how the single, shared backbone HydraNet works:\n# Tiny pseudo-HydraNet head in PyTorch x = shared_backbone(frames) # [B, C, H, W] lanes = lane_head(x) # [B, 4, H/4, W/4] objects = obj_head(x) # [B, 256, 7] # 3-D boxes voxels = occ_decoder(x) # [B, 80, 200, 200] policy = policy_mlp(torch.cat([voxels.flatten(1), ego_state], dim=-1)) Main tradeoffs:\nTesla trades modularity (learning various features about your current driving status) for latency \u0026 data efficiency. All features are learned jointly in their network.\nWaymo has a very modular stack, letting the engineers patch the prediction layer without re-training perception.\nTesla camera frames become a 3-D voxel occupancy grid consumed by the planner.\nBEVFormer fuses six camera views into a single bird’s-eye representation.\nData Engine Tesla Fleet Learning\nThis is the main power behind Tesla in my opinion. If you ask anyone what you need to make a good machine learning model, they are almost always going to say it’s the data you have to feed into the model.\nTesla gets billions of fresh frames per day. Every single person who drives a tesla is quietly feeding in data back to their HQ where they probably run auto-labeling with minimal human intervention (probably only for edge cases).\nWith time and as more teslas are on the road, you can only imagine how well they are going to fine tune their models.\nWaymo’s Curation\nWaymo actually open sourced their initial dataset, full of multi-sensor, hand-labeled data. They then ran multiple Carla-style simulations that generate rare crashes, rain, snow and then feed these into their lidar and camera networks.\n^^ fix please ^^\nHere, we see that Waymo’s edge is high quality labels and diverse sensor modalities. On the flip side, Tesla’s edge is it’s scale. And that scale is going to rise exponentially with more robotaxis and personal teslas on the road.\nTraining Muscle Tesla Waymo Silicon Dojo D1 ASCICs + 50K NVIDIA H100 Google Cloud TPU v5p pods Frameworks PyTorch, custom graph compiler JAX + Flax Batch Sizes Up to 600K frames per step (mixed from all cameras) 256 x 12-sensor slices per step Essentially, we can see that:\nTesla’s strategy is to drown the net in cheap, plentiful video -\u003e needs ultra-large batches.\nWaymo’s strategy is to feed the net richer examples (video + depth + doppler (same as doppler effect)).\nI don’t think there is a clear winner here, they just have differnet training methods because of differnet network architectures.\nEdge-Case handling This might be the most important section because edge cases are 80% of the difficulty of autonomous driving.\nLet’s take 3 examples:\nBlinding Sun at Dawn - Tesla applies temporal HDR stacking: producing high quality dynamic range images by combining multiple frames over time, not just multiple exposures at once.\nWaymo downgrades from vision to radar/lidar depth if cams wash out. Safety built in.\nSnow-Covered Lane Lines - Tesla leans on priors from it’s occupancy network.\nWaymo still “sees” road edges in lidar retruns.\nSandstorm in Nevada - Tesla hopes radar + optical flow catch moving blobs.\nWaymo’s lidar can punch through this entirely.\nRobustness today favors Waymo but Tesla’s curve is rising fast as data piles in.\nLocalization Tesla uses real-time visual SLAM (Simultaneous Localization and Mapping). It means the car is figuring out where it is while also building a map of the environment at the same time. We can think of it like watching the world through a camera feed and just from that, figuring out the 3D space you are moving through. With the fleet of teslas on the road, each one would help add and refine the “neural map” of cubrs, signs, etc without a pre-survey of the area.\nWaymo uses centimeter-grade HD maps built before service launch. It offloads perception workload but pins expansion to survey trucks.\nAdding one new U.S. city:\nTesla: ~4 weeks Waymo: ~8-12 months I think it’s pretty easy to see which one is more scalable and efficient.\nConclusion Once tesla is out of it’s pilot phase, they aim to charge $0.20 per mile.\nWaymo is currently charging anywhere between $0.50 - $1.20 per mile in Phoenix right now.\nFor tesla, they already produce their cars at a pretty low price (~$38000) and easily have the ability to tap into the retail car pipeline to build out these robotaxis. If they nail full stack vision, the cost curve and vision only ability is going to make this a monopoly.\nFor waymo, they definitely have a leg up on edge cases because of the redundancy baked into it. However, it is not easy to scale at all (~$200000 per vehicle) and the geo-fencing makes it feel more like a rule-based system rather than an actual autonomous driver.\nAt the end of the day, we don’t know which one will actually take off. Tesla seems promising but if it keeps failing on edge cases, the world will quickly realize that Waymo was right about having lidar as redudancy.\nIt’s the age old trade off but now wrapped in cars that drive themselves: simplicity \u0026 scalability vs. durability \u0026 redundancy.\n",
  "wordCount" : "1645",
  "inLanguage": "en",
  "datePublished": "2025-06-24T18:12:56-04:00",
  "dateModified": "2025-06-24T18:12:56-04:00",
  "author":{
    "@type": "Person",
    "name": "Computer Vision \u0026 Personal Opinions"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Why I think Tesla&#39;s Robotaxi will do laps on Waymo
    </h1>
    <div class="post-meta"><span title='2025-06-24 18:12:56 -0400 EDT'>June 24, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Computer Vision &amp; Personal Opinions

</div>
  </header> 
  <div class="post-content"><p>Tesla&rsquo;s Robotaxis and Google&rsquo;s Waymo solve the same problem in two vastly different ways but I believe that the approach that tesla is taking is going to out-scale Waymo&rsquo;s approach in a few years. In my opinion, tesla not only picked the better approach, they have the greatest secret weapon of all time (espeically in machine learning): <strong>data</strong>.</p>
<p>Robotaxi uses an end-to-end neural network driver with 8 cameras around the car. Essentially they are betting on the fact that if the neural network understands the current driving conditions, cheap hardware lets them scale up to millions.</p>
<p>Waymo leans on a sensor fusion stack: lidar, radar, cameras, HD maps—prioritizing redundancy, and measurable safety.</p>
<p>In essance, this is the same trade-off we see all the time in Computer Science but now wrapped in self driving cars: <strong>simplicity &amp; scale</strong> vs. <strong>redundancy &amp; robustness</strong>.</p>
<hr>
<p>Let&rsquo;s dive into their individual software stacks:</p>
<ol>
<li>Sensor Philosophy: Monocular vs. Multispectral</li>
</ol>
<p>Tesla: &ldquo;Eyes Only, like a human&rdquo;</p>
<p>8 × HDR cameras (plus a front “tri‑cam” setup) give a 360° bubble.</p>
<p>Beta units are regaining a slim HD radar to assist in rain/fog, but lidar is a four‑letter word in Palo Alto.</p>
<p>Goal: keep sensor BOM ≈ $400 so every Model Y can moonlight as a robotaxi.</p>
<p>&ndash;</p>
<p>Waymo: &ldquo;See Everything, Twice&rdquo;</p>
<p>29 cameras, 5 lidars, 6 imaging radars per car.</p>
<p>360° lidar range to 300 m, down‑to‑5 cm depth resolution.</p>
<p>Onboard heaters + cleaning nozzles because hardware this fancy hates LA/Phoenix dust.</p>
<p>Tesla’s network must infer depth &amp; occupancy from pure images whereas Waymo just measures it in point clouds, bringing out the cheaper vs. easier argument. However, as technology like <em>Neural Effective Field Radiance</em> improves, inferring depth from plain 2 dimensional images is not that diffcult for these nets. Also, humans don&rsquo;t have lidar attached to them - we take 2 separate 2D images (from each eye) slightly spaced apart and then stitch them together and our brain is the one that makes thigns have depth. This is the exact thing that tesla is betting on. If humans only drive by seeing 2D images of the world, they think that their car can do it as well. And to be honest, Robotaxis do drive pretty well just by seeing images of the world.</p>
<p>If we look at Waymo, it is definitely overkill. They not only capture images from 29 cameras, they have LiDAR senesor across multiple parts of the vehicle, making it super easy to sense how close / far objects are to you. That&rsquo;s useful, but if tesla is inferring depth without external LiDARs on top, that method will scale much easier.</p>
<hr>
<ol start="2">
<li>Perception Pipeline: From Pixels/Points → Drivable Space</li>
</ol>
<table>
  <thead>
      <tr>
          <th>Stage</th>
          <th>Tesla (FSD v12)</th>
          <th>Waymo (5th-Gen Stack)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Model Backbone</td>
          <td><code>HydraNet</code> is tesla&rsquo;s multi-task CNN -&gt; Occupancy Network voxelizes (fancy word to say map to 3D) the world around the car</td>
          <td>Sensor-specific CNNs + BEV transformer -&gt; features are fused into a unified grid</td>
      </tr>
      <tr>
          <td>Tasks</td>
          <td>Object, lane, traffic lights, freespace predicted in <em>one shot</em></td>
          <td>Separate output heads for detection, velocity, and intent prediction</td>
      </tr>
      <tr>
          <td>Planner</td>
          <td>End-to-end policy network which outputs steering and acceleration</td>
          <td>Rule based layer based on the flow Perception, Prediction, then Planning</td>
      </tr>
  </tbody>
</table>
<p>If we look at it like this, I really like the backbone models for both. Waymo&rsquo;s <strong>Bird&rsquo;s Eye View Transformer</strong> (BEVFormer) projects multiple camera views into a top-down feature map. Cross-attention then goes and collects spatial cues like object boundaries and lane markings by querying relevant regions from different camera perspectives. It then uses temporal self-attention by attending to past BEV frames to capture motion cues and ensure consistency across predictions.</p>
<p>Here&rsquo;s some pseudo-code I found online representing what the BEVFormer architecture looks like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>B, V, C, H, W <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">256</span>  <span style="color:#75715e"># batch, views (cams), channels, height, width</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1) Backbone encodes each camera frame</span>
</span></span><span style="display:flex;"><span>cam_feats <span style="color:#f92672">=</span> backbone(frames<span style="color:#f92672">.</span>view(B<span style="color:#f92672">*</span>V, <span style="color:#ae81ff">3</span>, H, W))  <span style="color:#75715e"># -&gt; [B*V, C, H/4, W/4]</span>
</span></span><span style="display:flex;"><span>cam_feats <span style="color:#f92672">=</span> cam_feats<span style="color:#f92672">.</span>view(B, V, C, H<span style="color:#f92672">//</span><span style="color:#ae81ff">4</span>, W<span style="color:#f92672">//</span><span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 2) Lift‑Splat to bird’s‑eye</span>
</span></span><span style="display:flex;"><span>bev_feats <span style="color:#f92672">=</span> lift_splat(cam_feats, intrinsics, extrinsics)  <span style="color:#75715e"># [B, C, X, Y]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3) Fuse lidar voxels via cross‑attention</span>
</span></span><span style="display:flex;"><span>lidar_vox <span style="color:#f92672">=</span> voxelize(lidar_points)         <span style="color:#75715e"># [B, C, X, Y]</span>
</span></span><span style="display:flex;"><span>bev_fused <span style="color:#f92672">=</span> bev_xattn(bev_feats, lidar_vox) <span style="color:#75715e"># transformer cross‑attn</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4) Detection queries (DETR‑style)</span>
</span></span><span style="display:flex;"><span>queries <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(B, <span style="color:#ae81ff">900</span>, <span style="color:#ae81ff">256</span>)  <span style="color:#75715e"># learnable object queries</span>
</span></span><span style="display:flex;"><span>obj_feats <span style="color:#f92672">=</span> transformer_decoder(queries, bev_fused)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5) Heads → 3‑D boxes and class probs</span>
</span></span><span style="display:flex;"><span>boxes_3d, cls_logits <span style="color:#f92672">=</span> box_head(obj_feats)
</span></span></code></pre></div><p>Tesla&rsquo;s HydraNet has some similarities but also some differences. It combines a CNN-based feature extraction with transofrmer-based fusion layers and temporal modules. Tesla uses a RegNet architecture (a CNN super similar to ResNet) for initial feature extraction. ConvLSTM units build short-term temporal awareness with a final Feature Pyramind Network for multi-scale processing. They then feed these features into transformer-based cross-attention blocks which learn to combine information across all cameras to build a multi view representation.</p>
<p>Here&rsquo;s some pseudocode of how the single, shared backbone HydraNet works:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Tiny pseudo-HydraNet head in PyTorch</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> shared_backbone(frames)        <span style="color:#75715e"># [B, C, H, W]</span>
</span></span><span style="display:flex;"><span>lanes   <span style="color:#f92672">=</span> lane_head(x)             <span style="color:#75715e"># [B, 4, H/4, W/4]</span>
</span></span><span style="display:flex;"><span>objects <span style="color:#f92672">=</span> obj_head(x)              <span style="color:#75715e"># [B, 256, 7]  # 3-D boxes</span>
</span></span><span style="display:flex;"><span>voxels  <span style="color:#f92672">=</span> occ_decoder(x)           <span style="color:#75715e"># [B, 80, 200, 200]</span>
</span></span><span style="display:flex;"><span>policy  <span style="color:#f92672">=</span> policy_mlp(torch<span style="color:#f92672">.</span>cat([voxels<span style="color:#f92672">.</span>flatten(<span style="color:#ae81ff">1</span>), ego_state], dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>))
</span></span></code></pre></div><p>Main tradeoffs:</p>
<ul>
<li>
<p>Tesla trades modularity (learning various features about your current driving status) for latency &amp; data efficiency. All features are learned jointly in their network.</p>
</li>
<li>
<p>Waymo has a very modular stack, letting the engineers patch the prediction layer without re-training perception.</p>
</li>
</ul>
<p><img alt="HydraNet occupancy demo" loading="lazy" src=".static/images/me.jpg">
<em>Tesla camera frames become a 3-D voxel occupancy grid consumed by the planner.</em></p>
<p><img alt="BEVFormer cross-view attention" loading="lazy" src="./images/bevformer_attention.png">
<em>BEVFormer fuses six camera views into a single bird’s-eye representation.</em></p>
<hr>
<ol start="3">
<li>Data Engine</li>
</ol>
<p><strong>Tesla Fleet Learning</strong></p>
<p>This is the main power behind Tesla in my opinion. If you ask anyone what you need to make a good machine learning model, they are almost always going to say it&rsquo;s the data you have to feed into the model.</p>
<p>Tesla gets <em><strong>billions</strong></em> of fresh frames per day. Every single person who drives a tesla is quietly feeding in data back to their HQ where they probably run auto-labeling with minimal human intervention (probably only for edge cases).</p>
<p>With time and as more teslas are on the road, you can only imagine how well they are going to fine tune their models.</p>
<p><strong>Waymo&rsquo;s Curation</strong></p>
<p>Waymo actually open sourced their initial dataset, full of multi-sensor, hand-labeled data. They then ran multiple Carla-style simulations that generate rare crashes, rain, snow and then feed these into their lidar and camera networks.</p>
<p>^^ fix please ^^</p>
<p>Here, we see that Waymo&rsquo;s edge is high quality labels and diverse sensor modalities. On the flip side, Tesla&rsquo;s edge is it&rsquo;s scale. And that scale is going to rise exponentially with more robotaxis and personal teslas on the road.</p>
<hr>
<ol start="4">
<li>Training Muscle</li>
</ol>
<table>
  <thead>
      <tr>
          <th></th>
          <th>Tesla</th>
          <th>Waymo</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Silicon</td>
          <td>Dojo D1 ASCICs + 50K NVIDIA H100</td>
          <td>Google Cloud TPU v5p pods</td>
      </tr>
      <tr>
          <td>Frameworks</td>
          <td>PyTorch, custom graph compiler</td>
          <td>JAX + Flax</td>
      </tr>
      <tr>
          <td>Batch Sizes</td>
          <td>Up to 600K frames per step (mixed from all cameras)</td>
          <td>256 x 12-sensor slices per step</td>
      </tr>
  </tbody>
</table>
<p>Essentially, we can see that:</p>
<p>Tesla&rsquo;s strategy is to drown the net in cheap, plentiful video -&gt; needs ultra-large batches.</p>
<p>Waymo&rsquo;s strategy is to feed the net richer examples (video + depth + doppler (same as doppler effect)).</p>
<p>I don&rsquo;t think there is a clear winner here, they just have differnet training methods because of differnet network architectures.</p>
<hr>
<ol start="5">
<li>Edge-Case handling</li>
</ol>
<p>This might be the most important section because edge cases are 80% of the difficulty of autonomous driving.</p>
<p>Let&rsquo;s take 3 examples:</p>
<p><strong>Blinding Sun at Dawn</strong> -
Tesla applies temporal HDR stacking: producing high quality dynamic range images by combining multiple frames over time, not just multiple exposures at once.</p>
<p>Waymo downgrades from vision to radar/lidar depth if cams wash out. Safety built in.</p>
<p><strong>Snow-Covered Lane Lines</strong> -
Tesla leans on priors from it&rsquo;s occupancy network.</p>
<p>Waymo still &ldquo;sees&rdquo; road edges in lidar retruns.</p>
<p><strong>Sandstorm in Nevada</strong> -
Tesla <em>hopes</em> radar + optical flow catch moving blobs.</p>
<p>Waymo&rsquo;s lidar can punch through this entirely.</p>
<blockquote>
<p>Robustness today favors Waymo but Tesla&rsquo;s curve is rising fast as data piles in.</p></blockquote>
<hr>
<ol start="6">
<li>Localization</li>
</ol>
<p>Tesla uses real-time visual SLAM (Simultaneous Localization and Mapping). It means the car is figuring out where it is while also building a map of the environment at the same time. We can think of it like watching the world through a camera feed and just from that, figuring out the 3D space you are moving through. With the fleet of teslas on the road, each one would help add and refine the &ldquo;neural map&rdquo; of cubrs, signs, etc without a pre-survey of the area.</p>
<p>Waymo uses centimeter-grade HD maps built before service launch. It offloads perception workload but pins expansion to survey trucks.</p>
<p>Adding one new U.S. city:</p>
<ul>
<li>Tesla: ~4 weeks</li>
<li>Waymo: ~8-12 months</li>
</ul>
<p>I think it&rsquo;s pretty easy to see which one is more scalable and efficient.</p>
<hr>
<ol start="7">
<li>Conclusion</li>
</ol>
<p>Once tesla is out of it&rsquo;s pilot phase, they aim to charge $0.20 per mile.</p>
<p>Waymo is currently charging anywhere between $0.50 - $1.20 per mile in Phoenix right now.</p>
<p>For tesla, they already produce their cars at a pretty low price (~$38000) and easily have the ability to tap into the retail car pipeline to build out these robotaxis. If they nail full stack vision, the cost curve and vision only ability is going to make this a monopoly.</p>
<p>For waymo, they definitely have a leg up on edge cases because of the redundancy baked into it. However, it is not easy to scale at all (~$200000 per vehicle) and the geo-fencing makes it feel more like a rule-based system rather than an actual autonomous driver.</p>
<p>At the end of the day, we don&rsquo;t know which one will actually take off. Tesla seems promising but if it keeps failing on edge cases, the world will quickly realize that Waymo was right about having lidar as redudancy.</p>
<p>It&rsquo;s the age old trade off but now wrapped in cars that drive themselves: simplicity &amp; scalability vs. durability &amp; redundancy.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
