<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fun with LoRA: How low-rank can we go before adjacency matrices break down? | Akhil’s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.">
<meta name="author" content="ML Theory &#43; Code">
<link rel="canonical" href="https://akhilvreddy.github.io/posts/tiny-lora/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilvreddy.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilvreddy.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilvreddy.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilvreddy.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilvreddy.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilvreddy.github.io/posts/tiny-lora/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="https://akhilvreddy.github.io/posts/tiny-lora/">
  <meta property="og:site_name" content="Akhil’s Blog">
  <meta property="og:title" content="Fun with LoRA: How low-rank can we go before adjacency matrices break down?">
  <meta property="og:description" content="How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-27T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-05-27T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fun with LoRA: How low-rank can we go before adjacency matrices break down?">
<meta name="twitter:description" content="How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://akhilvreddy.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fun with LoRA: How low-rank can we go before adjacency matrices break down?",
      "item": "https://akhilvreddy.github.io/posts/tiny-lora/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fun with LoRA: How low-rank can we go before adjacency matrices break down?",
  "name": "Fun with LoRA: How low-rank can we go before adjacency matrices break down?",
  "description": "How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.",
  "keywords": [
    
  ],
  "articleBody": " TL;DR: LoRA lets us fine-tune massive models cheaply by injecting tiny low-rank adapters into frozen weights. This post stress-tests how far we can compress by reducing adapter rank and quantizing the base model before performance falls apart. Turns out, LoRA is surprisingly robust…until it’s not.\nThe repsitory with the corresponding PyTorch implementation is available here.\nWhenever I open up Hugging Face, a lot of it feels like:\n“I took an open source LLaMA / BERT / Mistral, slapped on LoRA, fine-tuned on a niche dataset (anime scripts, law documents), and boom - new model checkpoint which beats some random bench”\nIt’s kind of like the ImageNet transfer learning era but this time instead of freezing a CNN and retraining the head, people are freezing 99% of the LLM and training a few low-rank adapters.\nBut if these many people are relying on LoRA, it must be a very powerful architecture: easy to scale, cheap to train, and good enough to ship models.\nIn this post, I wanted to push LoRA to its limits.\nShrink the adapter ranks. Quantize the backbone (QLoRA). Compress everything as much as possible. Would LoRA still work? When does LoRA crumble?\nThat’s the question I want to explore in this post. Before we dive into that, what exactly is LoRA?\nWhat is LoRA? And, what is QLoRA? Why are they so effective?\nLet’s take a look at a trained, massive LLM like LLaMA or Mistral. It’s either open weight (LLaMA) or open source (Mistral) with a giant neural network with billions of parameters. It works fine for small chat, questions about common history tasks, and even some basic analytical tasks. Now, you want to fine-tune it on your own task which could be legal contracts, anime scripts, or cooking recipes. Here’s the big problem:\nFine-tuning the entire model is insanely expensive.\nThere are billions of parameters to update, which means high GPU memory usage, heavy compute requirements, and long training times.\nLoRA offers a smart workaround to this.\nThis shows LoRA in action, with the right side where and how the compression happens.\nThe brain behind LoRA We aim to freeze our LLM with LoRA. The base model gets frozen completely - no gradients and no weight changes. Essentially, that whole part is going to get evaluated with torch.no_grad (or more precisely, the base weights have requires_grad=False, so they don’t accumulate gradients or get updated). We then inject a tiny set of trainable matrices inside and update parameters on those. It’s like saying:\n“I’m going to lock the LLM in place, and instead of editing the whole brain, I’ll just train a few small knobs that tweak what is says.”\nThat’s where the low-rank adapter idea comes in.\nMost LLM layers have giant matrices: a single linear layer might be a 4096 x 4096 matrix. That’s 16 million parameters for just one layer.\nLoRA takes use of the fact that you don’t need a full-rank matrix to get meaningful updates. So instead of training a full matrix ΔW, we approximate it with two smaller matrices.\nMatrix A of shape (d x r) Matrix B of shape (r x d) where d is the full dim of a linear layer (in this case it’s 4096) and r is a small number (usually 4, 8, or 16).\nA quick calculation can show us that the size of the new matrix we are adding is the same dimension as the original layer:\nA @ B = (d x r) x (r x d) = (d x d)\nBut we now only have d x r x 2 parameters to update instead of d x d x 2. Doing this at every layer, this makes it way more manageable at scale.\nThe update now becomes ΔW = A @ B.\nThis saves us memory, vRAM in GPU, and training time while still giving enough capacity to adapt the model to new data.\nIf you are like me, this explanation should help you understand how LoRA helps (less params to update gives us a quicker, more stable updating). In my opinion, this doesn’t touch on why it really works (why can we approximate ΔW as A @ B).\nLet’s dive a little under the hood. But first, here’s an analogy.\nImagine W is a giant painting:\nYou’re not allowed to repaint it — just allowed to stick a thin sheet of transparent film (ΔW) over it You draw tiny adjustments on the film (via A @ B) The final image is still affected, but the original canvas never changes In standard fine-tuning we would have to update W directly.\nW_new = W - α * ∇L(W) With LoRA you freeze W and only train\nΔW = A @ B (A ∈ ℝ^{d×r}, B ∈ ℝ^{r×d}) and hence\nW_eff = W + α * ΔW During training and inference, we compute\noutput = x @ (W + α * (A @ B)) where x in this case is the input tensor and α is a scaling factor to make sure your low-rank update doesn’t dominate. A and B are initialized carefully:\nA is often random B is zero at initialization Given those initialization, ΔW = 0 at first. This is needed because you want your first forward pass to mimic the original LLM.\nQLoRA is essentially the same steps as the above, but we quantize the model before we run LoRA (more to come on that below).\nLet’s fine-tune a small model (2x LoRA implementation) There’s two ways to fine-tune with LoRA: using Hugging Face’s PEFT and injecting the matrices yourself (a DIY method).\nHugging Face PEFT PEFT stands for Performance Efficient Fine Tuning - it is the plug and play version which supports Hugging Face transformers. It helps us inject LoRA adapters into any supported model with just a few lines of code.\nLet’s take a small model to keep things simple and fast: bert-base-uncased and distilbert are decent for experiments and they don’t fry your laptop.\nUsing PEFT takes care of:\nFreezing the base model Inserting low-rank matrices into attention or linear layers Training only the LoRA parameters hf_lora_init.py\nfrom peft import get_peft_model, LoraConfig config = LoraConfig( r=8, lora_alpha=16, target_modules=[\"query\", \"value\"], # where to inject LoRA bias=\"none\", task_type=\"SEQ_CLS\" ) model = get_peft_model(base_model, config) This snippet adds low-rank trainable matrices to specific layers: in this case, the “query” and “value” projections inside each attention block. If you know the internals of a transformer (or you’ve read my previous blog post), you know that each self attention layer has components like\nself.query = nn.Linear(...) self.key = nn.Linear(...) self.value = nn.Linear(...) we’re essentially telling PEFT:\n“Insert LoRA into the query and value linear layers (the parts that project your input into Q and V vector during attention).”\nLoRA has been shown to be most effective for language models if you insert them here.\nOnce the model is LoRA-ified with get_peft_model(), it’s ready to train. The training process would look just like any other Hugging Face model.\nNow, we would need to\nLoad our dataset (sentiment, instrucitons, legal docs) Tokenize the inputs Feed those into a training loop This is the part where your text actually enters the model. We’re teaching the adapters how to nudge the base model’s outputs for our specific task.\nUsing Hugging Face Trainer, the setup is identical to a full-fine tuning case except our model now has low-rank matrices attached to it and the base weights are frozen.\npeft_train.py\nfrom transformers import Trainer, TrainingArguments trainer = Trainer( model=model, args=TrainingArguments( output_dir=\"./results\", num_train_epochs=3, per_device_train_batch_size=16, learning_rate=2e-4, evaluation_strategy=\"epoch\", fp16=True, logging_steps=10, save_strategy=\"epoch\" ), train_dataset=train_dataset, eval_dataset=eval_dataset, tokenizer=tokenizer ) trainer.train() the training looks exactly the same but we’re now updating only a handful of parameters (handful relative to the billions of base parameters).\nDIY LoRA Hugging Face makes it so simple and efficient it almost feels like cheating at times. Let’s do it ourselves to see how it train this under the hood. We can expect the code to be much more complicated here.\nWe first need a LoRA linear class\nimport torch import torch.nn as nn class LoRALinear(nn.Module): def __init__(self, in_features, out_features, r=8, alpha=16): super().__init__() self.W = nn.Linear(in_features, out_features, bias=False) self.r = r self.alpha = alpha self.scaling = self.alpha / self.r # LoRA parameters self.A = nn.Parameter(torch.randn(out_features, r) * 0.01) self.B = nn.Parameter(torch.zeros(r, in_features)) # init ΔW = 0 # Freeze W for param in self.W.parameters(): param.requires_grad = False def forward(self, x): delta_W = self.A @ self.B # shape: (out_features x in_features) W_eff = self.W.weight + self.scaling * delta_W return x @ W_eff.T # simulate nn.Linear with W_eff Next up, we have to do some model surgery. We need to traverse the transformer (via named_modules() usually) and manually replace the Linear layers inside query, value, and sometimes dense, output, etc. (if you are doing a different version of LoRA).\nThis is how it would look like:\nfrom transformers import BertModel def inject_lora_into_bert(model, r=8, alpha=16): for name, module in model.named_modules(): if 'attention' in name and isinstance(module, nn.Linear): if 'query' in name or 'value' in name: parent = get_parent_module(model, name) key = name.split('.')[-1] orig = getattr(parent, key) lora_layer = LoRALinear(orig.in_features, orig.out_features, r, alpha) setattr(parent, key, lora_layer) def get_parent_module(model, name): parts = name.split('.') for part in parts[:-1]: model = getattr(model, part) return model Now, we got the hard part out of the way. We can load the model and inject LoRA.\nfrom transformers import BertModel base_model = BertModel.from_pretrained(\"bert-base-uncased\") inject_lora_into_bert(base_model) Here, BERT is now frozen except for the tiny A and B matrices in the Q and V layers. These are the only things getting updated in backpropagation.\nThe rest kind of follows the same structure as a regular NLP pipeline. Let’s take an example where we are doing binary sentiment classification.\nOur model is patched so we can build a simple dataset and tokenizer setup for binary classification (IMDb).\ndata.py\nfrom datasets import load_dataset from transformers import BertTokenizer dataset = load_dataset(\"imdb\") tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") def tokenize(batch): return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256) tokenized = dataset.map(tokenize, batched=True) tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']) Since we’re using BERT (which is an encoder-only model) we need to add a classification head if we want to fine-tune it on a task that outputs a label (like sentiment or genre classification). BERT processes input and produces a hidden representation, but it doesn’t naturally generate text, so we map its [CLS] token output to a class via a small linear layer.\nOn the other hand, if we were fine-tuning a decoder-only model like Mistral or LLaMA, and the task involved generating text (like summarization, dialogue, or instruction tuning), we wouldn’t need a classification head at all. We could just use the model’s built-in autoregressive capabilities.\nHere’s a quick breakdown\nIf your labels are categories → classification head required If your labels are text → just use generation (no extra heads) If your labels are per token → use token classification heads Let’s continue with BERT classification. We would add the classification head and train.\nclass BertWithLoRAForClassification(nn.Module): def __init__(self, base_model, num_classes=2): super().__init__() self.bert = base_model self.classifier = nn.Linear(base_model.config.hidden_size, num_classes) def forward(self, input_ids, attention_mask, labels=None): outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) pooled = outputs.pooler_output # or use mean over last hidden state logits = self.classifier(pooled) loss = None if labels is not None: loss = nn.CrossEntropyLoss()(logits, labels) return {\"loss\": loss, \"logits\": logits} And here’s what a training loop would look like\nfrom torch.utils.data import DataLoader import torch.optim as optim model = BertWithLoRAForClassification(base_model) model.train() optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4) train_loader = DataLoader(tokenized[\"train\"], batch_size=8, shuffle=True) for epoch in range(3): for batch in train_loader: optimizer.zero_grad() output = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"]) output[\"loss\"].backward() optimizer.step() Let’s recap what we did here:\nWe first froze the base transformer Manually injected LoRA layers (into query, value) Trained only tiny matrices (A @ B) And we can expect to get real performance with a tiny parameter budget.\nHere’s the overall structure of what we just wrote\nText → [Tokenized] → [BERT (frozen)] → [LoRA-injected Q \u0026 V] → [CLS vector] → [Classification Head] → Prediction Before we move on to another part, I wanted to talk about why we decided to inject LoRA between Q and V. Here’s a quick refresher about Q, K, and V:\nQ (Query): What am I looking for? K (Key): What is available? V (Value): What information do I get if there’s a match? Changing Q changes what the model pays attention to. This is very important since we want our model to attend more to tokens similar to what is in our fine-tuned training set instead of the current way it is set up.\nChanging V is also important because it changes the core information that is passed on once attention is calculated. Fine-tuning takes advantage of this because it can change the meaning of the information that is being passed in.\nCombining these two, we steer both attention focus and content delivery which is extremely powerful for adapting the model.\nNow that we have a solid foundation of what LoRA is doing, let’s start pushing it.\nWhat happens if we compress too far? But before diving into that, let’s talk about how we can compress these models in the first place.\nWhen pushing LoRA to its limits, there are two major levers we can pull:\nQuantization: Compress model weights by reducing precision (like FB32 → INT8). We can do this in a couple of different ways Post training (static or dynamic quantization) During training (as in QLoRA which has gradient checkpointing and paged optimizers) Low-Rank Decomposition: Reduce the rank r of the LoRA adapters (make the trainable A and B matrices smaller). Smaller r gives us fewer trainable params and more compression If we compress too far, we lose expressive power The craziest part about these two techniques is that they are orthogonal. We can combine them to train small low-rank adapters on top of a quantized base model.\nThink of it like tuning a guitar: quantization is changing the thickness of your strings, and LoRA rank is how many fingers you have on the fretboard. Go too far either way and things start sounding off.\nAnd that’s exactly what we are going to do. But in this section I want to talk about the theoretical limits first.\nEssentially, I want to answer the question, “when do we cross the line from ’efficient’ to ‘destructive’?” from a theoretical standpoint.\nWhat do we lose in quantization? The tradeoff here is precision loss.\nSpecifically, we reduce the precision of the weights and activations used in the neural network. Instead of storing each weight as a 32-bit float (FP32) we might store them in different ways.\n16-bit float (FP16) → mild compression (used in mixed-precision training) 8-bit integer (INT8) → big compression (common in production) 4-bit integer (INT4) → maximum compression for LLM usage (used in ollama) 2-bit integer (INT2) → ultra compression (serious tradeoff in model quality) 1-bit integer (INT1) → a lost cause most of the time (these are binary weights used in tiny embedded models) Quantization is nice because smaller weights = smaller models which means less vRAM usage while training and inference. Also, integer math is faster on most hardware, giving us a compute advantage as well. But, we lose granularity which can affect model performance.\nLet’s take an example where a certain weight in a random layer is 0.123456789 in FP32.\nIf stored as INT8 (which only has 256 discrete values in total), it might get rounded to just 0.12 or even 0.1. For a single weight, that’s not bad. But if we do this for every single weight inside a massive matrix multiplication, we get a compounding eror.\nHere’s what can happen with that subtle shift:\nAttention weights might favor the wrong tokens LayerNorm might lose balance Gradients might vanish or explode In a real world model, in self-attention we could see this\ntoken A → 0.23891 token B → 0.23892 in high precision, B wins barely. In INT8, both get quantized to 0.23 meaning that the attention scores are technically wrong here because it has given equal value to both (which is going to hurt after softmax) even though token B should slightly beat out after softmax.\nWe see these effects in real life. Have you ever used ollama and seen how text is generated there? Those are most likely quantized as much as possible so they don’t have to pay for much compute. LLM giants like ChatGPT, Claude, and Gemini can afford to run full-precision models — and that’s exactly why their responses feel sharper, more nuanced, and more accurate.\nWhat do we lose in LoRA rank decay? The tradeoff here is capacity loss.\nLet’s think about what LoRA does exactly: it approximates ΔW = A @ B. And both matrices are of rank r. A larger value for r gives us more expressive power whereas lower r is a tighter bottleneck (just like layers in deep learning). Smaller r forces the model to “pack” more meaning into fewer degrees of freedom, which is exactly what we are going to struggle with.\nImagine trying to describe a 1000-pixel image using just 4 brush strokes. That’s what low-rank LoRA is doing at r=4.\nEmpiricallly, classification tasks hold up at r=8 and sometimes at r=4. Instruction-tuning may degrade below r=8.\nHere’s what I found online and with consulting with Claude:\nr=16 → near full-fine-tune performance r=8 → good for many tasks r=4 → tolerable r=2 → fragile r=1 → mostly breaks What do we lose when we take advantage of orthogonality? The tradeoff here is that we are compounding the damage. We’re squeezing both the model’s capacity and precision at the same time which puts us in a risky zone.\nQuantization distorts how the model sees the world (by rounding off detail) while low-rank LoRA limits what the model can express (by shrinking the update space). We’re essentially saying\n“Here’s a tiny canvas and we’re giving you blunt crayons to draw with.”\nThis is like giving a blurry camera (quantized model) to a painter with only 2 colors (LoRA r=2) and expecting a photorealistic portrait.\nSo in the next part, we’ll test just how far we can push until results crumble. We’ll try LoRA at ranks 16, 8, 4, 2, and 1 (both on quantized and unquantized models) and find out exactly where we start to lose meaning.\nExtreme LoRA experiments Just like before, we have to decide if we want to use the Hugging Face PEFT method or go with the DIY approach. I’m leaning toward the DIY method for these experiments so we can:\nControl exactly where LoRA is injected Run rank sweeps cleanly Visualize the effects of low-rank tuning more transparently The goal here is to implement that stress testing that I talked about in the previous section. Here’s what the sweep lists would look like\nrank_sweep = [32, 16, 8, 4, 3, 2, 1] and we would have different types of models\nmodels = [ \"bert-base-uncased (FP32)\", # Standard full-precision model \"bert-base-uncased (INT8)\", # Quantized using bitsandbytes (INT8) \"distilbert-base-uncased (FP32)\", # Smaller model, full-precision \"distilbert-base-uncased (INT8)\" # Smaller, quantized model ] By sweeping r across these 4 models we can figure out how robust LoRA is across model scale, precision regimes, and rank compression limits.\nIn the next two sections, we would have similar code other than one small change.\nVarious r values, no quantization We’d want to just pick out the non-quantized models first.\nmodels_v1 = [models[0], models[2]] We’re going to be borrowing a bunch of stuff from code snippets above, so I won’t write them out here again. Here’s what we would need set up before we can jump in:\nA custom LoRALinear module bert-base-uncased and distilbert-base-uncased downloaded to local A classification dataset (IMDb, SST-2, etc.) LoRA injection (with default r and alpha values):\ndef inject_lora_into_model(model, r=8, alpha=16): for name, module in model.named_modules(): if isinstance(module, nn.Linear) and ('query' in name or 'value' in name): parent = get_parent_module(model, name) key = name.split('.')[-1] orig = getattr(parent, key) lora_layer = LoRALinear(orig.in_features, orig.out_features, r=r, alpha=alpha) setattr(parent, key, lora_layer) def get_parent_module(model, name): parts = name.split('.') for part in parts[:-1]: model = getattr(model, part) return model LoRA rank sweep:\ndef run_lora_rank_sweep(base_model_name, rank_sweep, quantized=False): results = {} for r in rank_sweep: print(f\"Running LoRA with r = {r} on model: {base_model_name}\") # load base model if quantized: from transformers import BitsAndBytesConfig, AutoModel bnb_config = BitsAndBytesConfig(load_in_8bit=True) model = AutoModel.from_pretrained(base_model_name, quantization_config=bnb_config, device_map=\"auto\") else: model = AutoModel.from_pretrained(base_model_name) # inject LoRA manually inject_lora_into_model(model, r=r, alpha=16) # wrap in classification head wrapped_model = BertWithLoRAForClassification(model) # train and evaluate acc = train_and_eval(wrapped_model, r) results[r] = acc return results Classification Wrapper\nclass BertWithLoRAForClassification(nn.Module): def __init__(self, base_model, num_classes=2): super().__init__() self.bert = base_model self.classifier = nn.Linear(base_model.config.hidden_size, num_classes) def forward(self, input_ids, attention_mask, labels=None): outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) pooled = outputs.pooler_output logits = self.classifier(pooled) loss = None if labels is not None: loss = nn.CrossEntropyLoss()(logits, labels) return {\"loss\": loss, \"logits\": logits} Lastly, for training\ndef train_and_eval(model, r_value): model.train() optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4) train_loader = DataLoader(tokenized[\"train\"], batch_size=8, shuffle=True) for epoch in range(2): for batch in train_loader: optimizer.zero_grad() output = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"label\"]) output[\"loss\"].backward() optimizer.step() acc = evaluate(model, tokenized[\"test\"]) print(f\"r = {r_value} → accuracy = {acc:.2f}\") return acc we would have to run this loop for each model in the models_v2 list that we defined above.\nWe can check how well this performed by some basic plots\nimport matplotlib.pyplot as plt def plot_results(results_dict, model_name): ranks = list(results_dict.keys()) accs = list(results_dict.values()) plt.plot(ranks, accs, marker='o') plt.xlabel(\"LoRA Rank (r)\") plt.ylabel(\"Accuracy\") plt.title(f\"LoRA Rank Sweep – {model_name}\") plt.gca().invert_xaxis() plt.grid(True) plt.show() Various r values, with quantization Same deal as before but we’d want the quantized models here.\nmodels_v2 = [models[1], models[3]] Other than that, the steps would be the same.\nComparing results Once we’ve run the rank sweeps for both full-precision and quantized models, we can compare their performance side-by-side.\nIn my opinion, these are the most useful metrics to focus on.\nF1-score This tells us how well the model is actually performing on the classification task. For clean datasets like IMDb or SST-2, accuracy is usually enough. However, for imbalanced datasets, F1-score provides a better picture by balancing precision and recall (especially when false positives or false negatives carry different weights).\nNumber of trainable parameters This helps quantify the efficiency gains from LoRA as we reduce the rank r. Logging the number of parameters in A @ B vs. the full model would show us how much smaller the update space is - espeically the steep drop from r=32 to r=1.\nTraining time per epoch This can highlight how model size and quantization affect training speed which is very useful when comparing quantized vs. non-quantized models. INT8 models have faster matrix multiplications which should reduce forward / backward pass time, if you hardware supports INT8 acceleration.\nThis would show us how much time we are saving per forward call / inference. Quantization usually gives us faster inference but training speed depends on hardware.\nCan we save our (garbage) models? After seeing r=1 and r=2 crumble, I started wondering if there was anything we could do to recover performance without increasing rank. I didn’t test all of these strategies (yet) but here were the main strategies I was considering to make ulta-low-rank LoRA viable.\nLoRA alpha tuning The LoRA α parameter acts like a multiplier on the low-rank update. It controls how much influence A @ B has in W_eff = W + α * (A @ B).\nAt very low rank (r=1 or r=2) the update A @ B might be too weak to meaningfully steer the model, it barely moves the needle. By increasing alpha, we amplify the effect of the adapter, giving it a stronger say in the final output.\nWe can think of giving our tiny adapter a megaphone.\nYou can’t make it (LoRA) smarter but you can make it louder.\nMore epochs \u0026 warmup schedules When rank is low, the model’s update space is extremely restricted. It might just need more time to discover a useful subspace. Especially if we use a r=1 matrix over thousands of training examples it’s going ot be slow to learn anything meaningful. We could do a few things to help the model:\nTrain longer (more epochs) Use a cosine or linear warmp schedule to easy into training Try a smaller learning rate with gradual ramp-up to avoid catastrophic updates early on Targeted LoRA injection Instead of injecting LoRA into every attention block (as we did above), we can try injection only in the last few transformer layers.\nThe later layers of the model tend to be more task-specific while early layers are more general. Instead of spreading a tiny adapter thin across the whole network, we can focus our compression budget on the blocks that matter the most. This reduces interference and makes it easier for the model to converge.\nr=1 everywhere might be too shallow but r=1 in the right place might be just enough\nFor certain tasks, it can be more effective to inject LoRA into output projection layers—such as the attention output or the MLP output layers—instead of the query or value layers. These output projections often carry more task-specific signal, making them better targets for adaptation.\nLoRA dropout Just like regular dropout, LoRA adapters support lora_dropout which randomly zeroes out a fraction of adapter outputs during training (like standard dropout).\nDropout might seem unintuitive at low-rank because of how little signal we get already but it can actually help prevent overfitting on noise, especially when we are working on a small-task specific dataset. Dropout forces our model to generalize a bit more, even when the adapter space is small.\nTrying values like lora_dropout=0.05 or 0.1 can really help stabilize learning.\nConclusion LoRA is one of the most powerful tools in the modern fine-tuning toolbox. Not because it performs miracles, but because it scales well. It gives us a way to adapt massive models with minimal memory, relatively low training time, and surprisingly solid performance.\nBut as we saw, everything breaks if you push it far enough. We can’t just compress forever and expect magic.\n",
  "wordCount" : "4323",
  "inLanguage": "en",
  "datePublished": "2025-05-27T00:00:00Z",
  "dateModified": "2025-05-27T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "ML Theory + Code"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilvreddy.github.io/posts/tiny-lora/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil’s Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilvreddy.github.io/favicon.ico"
    }
  }
}
</script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilvreddy.github.io/" accesskey="h" title="Akhil’s Blog (Alt + H)">Akhil’s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilvreddy.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/reimplementations/" title="Reimplementations">
                    <span>Reimplementations</span>
                </a>
            </li>
            <li>
                <a href="https://akhilvreddy.github.io/tooling/" title="Tooling">
                    <span>Tooling</span>
                </a>
            </li>
            <li>
                <a href="https://www.github.com/akhilvreddy" title="GitHub">
                    <span>GitHub</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="LinkedIn">
                    <span>LinkedIn</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
            <li>
                <a href="https://www.linkedin.com/in/akhilvreddy" title="X">
                    <span>X</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Fun with LoRA: How low-rank can we go before adjacency matrices break down?
    </h1>
    <div class="post-meta"><span title='2025-05-27 00:00:00 +0000 UTC'>May 27, 2025</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;ML Theory &#43; Code

</div>
  </header> 
  <div class="post-content"><hr>
<p><strong>TL;DR</strong>: LoRA lets us fine-tune massive models cheaply by injecting tiny low-rank adapters into frozen weights. This post stress-tests how far we can compress by reducing adapter rank and quantizing the base model before performance falls apart. Turns out, LoRA is surprisingly robust…until it’s not.</p>
<p>The repsitory with the corresponding PyTorch implementation is available <a href="https://github.com/akhilvreddy/word2vec-scratch">here</a>.</p>
<hr>
<p>Whenever I open up Hugging Face, a lot of it feels like:</p>
<p>&ldquo;I took an open source LLaMA / BERT / Mistral, slapped on LoRA, fine-tuned on a niche dataset (anime scripts, law documents), and boom - new model checkpoint which beats some random bench&rdquo;</p>
<p>It&rsquo;s kind of like the ImageNet transfer learning era but this time instead of freezing a CNN and retraining the head, people are freezing 99% of the LLM and training a few low-rank adapters.</p>
<p>But if <strong>these many people are relying on LoRA</strong>, it must be a very powerful architecture: easy to scale, cheap to train, and good enough to ship models.</p>
<p>In this post, I wanted to push LoRA to its limits.</p>
<ul>
<li>Shrink the adapter ranks.</li>
<li>Quantize the backbone (QLoRA).</li>
<li>Compress everything as much as possible.</li>
</ul>
<p>Would LoRA still work? When does LoRA crumble?</p>
<p>That&rsquo;s the question I want to explore in this post. Before we dive into that, what exactly is LoRA?</p>
<h2 id="what-is-lora">What is LoRA?<a hidden class="anchor" aria-hidden="true" href="#what-is-lora">#</a></h2>
<p>And, what is QLoRA? Why are they so effective?</p>
<p>Let&rsquo;s take a look at a trained, massive LLM like LLaMA or Mistral. It&rsquo;s either open weight (LLaMA) or open source (Mistral) with a giant neural network with billions of parameters. It works fine for small chat, questions about common history tasks, and even some basic analytical tasks. Now, you want to fine-tune it on your own task which could be legal contracts, anime scripts, or cooking recipes. Here&rsquo;s the big problem:</p>
<blockquote>
<p>Fine-tuning the entire model is insanely expensive.</p></blockquote>
<p>There are billions of parameters to update, which means high GPU memory usage, heavy compute requirements, and long training times.</p>
<p>LoRA offers a smart workaround to this.</p>
<div align="center">
  <img src="/images/lora1.gif" alt="Skip-gram architecture" width="600"/>
  <p>This shows LoRA in action, with the right side where and how the compression happens.</p>
</div>
<h3 id="the-brain-behind-lora">The brain behind LoRA<a hidden class="anchor" aria-hidden="true" href="#the-brain-behind-lora">#</a></h3>
<p>We aim to freeze our LLM with LoRA. The base model gets frozen completely - no gradients and no weight changes. Essentially, that whole part is going to get evaluated with <code>torch.no_grad</code> (or more precisely, the base weights have <code>requires_grad=False</code>, so they don’t accumulate gradients or get updated). We then inject a tiny set of trainable matrices inside and update parameters on those. It&rsquo;s like saying:</p>
<p>&ldquo;I&rsquo;m going to lock the LLM in place, and instead of editing the whole brain, I&rsquo;ll just train a few small knobs that tweak what is says.&rdquo;</p>
<p>That&rsquo;s where the <em>low-rank adapter</em> idea comes in.</p>
<p>Most LLM layers have giant matrices: a single linear layer might be a <code>4096 x 4096</code> matrix. That&rsquo;s 16 million parameters for just one layer.</p>
<p>LoRA takes use of the fact that you don&rsquo;t need a full-rank matrix to get meaningful updates. So instead of training a full matrix <code>ΔW</code>, we approximate it with two smaller matrices.</p>
<ul>
<li>Matrix A of shape <code>(d x r)</code></li>
<li>Matrix B of shape <code>(r x d)</code></li>
</ul>
<p>where <code>d</code> is the full dim of a linear layer (in this case it&rsquo;s 4096) and <code>r</code> is a small number (usually 4, 8, or 16).</p>
<p>A quick calculation can show us that the size of the new matrix we are adding is the same dimension as the original layer:</p>
<p><code>A @ B</code> = <code>(d x r) x (r x d)</code> = <code>(d x d)</code></p>
<p>But we now only have <code>d x r x 2</code> parameters to update instead of <code>d x d x 2</code>. Doing this at every layer, this makes it way more manageable at scale.</p>
<p>The update now becomes <code>ΔW = A @ B</code>.</p>
<p>This saves us memory, vRAM in GPU, and training time while still giving enough capacity to adapt the model to new data.</p>
<p>If you are like me, this explanation should help you understand <em>how</em> LoRA helps (less params to update gives us a quicker, more stable updating). In my opinion, this doesn&rsquo;t touch on <em>why</em> it really works (why can we approximate <code>ΔW</code> as <code> A @ B</code>).</p>
<p>Let&rsquo;s dive a little under the hood. But first, here&rsquo;s an analogy.</p>
<p>Imagine <code>W</code> is a giant painting:</p>
<ul>
<li>You’re not allowed to repaint it — just allowed to stick a thin sheet of transparent film (<code>ΔW</code>) over it</li>
<li>You draw tiny adjustments on the film (via A @ B)</li>
<li>The final image is still affected, but the original canvas never changes</li>
</ul>
<p>In standard fine-tuning we would have to update <code>W</code> directly.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>W_new = W - α * ∇L(W)
</span></span></code></pre></div><p>With LoRA you freeze <code>W</code> and only train</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>ΔW = A @ B   (A ∈ ℝ^{d×r}, B ∈ ℝ^{r×d})
</span></span></code></pre></div><p>and hence</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>W_eff = W + α * ΔW
</span></span></code></pre></div><p>During training and inference, we compute</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>output = x @ (W + α * (A @ B))
</span></span></code></pre></div><p>where <code>x</code> in this case is the input tensor and α is a scaling factor to make sure your low-rank update doesn&rsquo;t dominate. A and B are initialized carefully:</p>
<ul>
<li>A is often random</li>
<li>B is zero at initialization</li>
</ul>
<p>Given those initialization, ΔW = 0 at first. This is needed because you want your first forward pass to mimic the original LLM.</p>
<p>QLoRA is essentially the same steps as the above, but we quantize the model before we run LoRA (more to come on that below).</p>
<h2 id="lets-fine-tune-a-small-model-2x-lora-implementation">Let&rsquo;s fine-tune a small model (2x LoRA implementation)<a hidden class="anchor" aria-hidden="true" href="#lets-fine-tune-a-small-model-2x-lora-implementation">#</a></h2>
<p>There&rsquo;s two ways to fine-tune with LoRA: using Hugging Face&rsquo;s PEFT and injecting the matrices yourself (a DIY method).</p>
<h3 id="hugging-face-peft">Hugging Face PEFT<a hidden class="anchor" aria-hidden="true" href="#hugging-face-peft">#</a></h3>
<p>PEFT stands for <em>Performance Efficient Fine Tuning</em> - it is the plug and play version which supports Hugging Face transformers. It helps us inject LoRA adapters into any supported model with just a few lines of code.</p>
<p>Let&rsquo;s take a small model to keep things simple and fast: <code>bert-base-uncased</code> and <code>distilbert</code> are decent for experiments and they don&rsquo;t fry your laptop.</p>
<p>Using PEFT takes care of:</p>
<ul>
<li>Freezing the base model</li>
<li>Inserting low-rank matrices into attention or linear layers</li>
<li>Training only the LoRA parameters</li>
</ul>
<p>hf_lora_init.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> peft <span style="color:#f92672">import</span> get_peft_model, LoraConfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>config <span style="color:#f92672">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    r<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;query&#34;</span>, <span style="color:#e6db74">&#34;value&#34;</span>],  <span style="color:#75715e"># where to inject LoRA</span>
</span></span><span style="display:flex;"><span>    bias<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;none&#34;</span>,
</span></span><span style="display:flex;"><span>    task_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;SEQ_CLS&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> get_peft_model(base_model, config)
</span></span></code></pre></div><p>This snippet adds low-rank trainable matrices to specific layers: in this case, the “query” and “value” projections inside each attention block. If you know the internals of a transformer (or you&rsquo;ve read my <a href="http://www.akhilvreddy.com/attention-scratch">previous blog post</a>), you know that each self attention layer has components like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>query <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>key <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#f92672">...</span>)
</span></span><span style="display:flex;"><span>self<span style="color:#f92672">.</span>value <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#f92672">...</span>)
</span></span></code></pre></div><p>we&rsquo;re essentially telling PEFT:</p>
<p>&ldquo;Insert LoRA into the query and value linear layers (the parts that project your input into Q and V vector during attention).&rdquo;</p>
<p>LoRA has been shown to be most effective for language models if you insert them here.</p>
<p>Once the model is LoRA-ified with <code>get_peft_model()</code>, it&rsquo;s ready to train. The training process would look just like any other Hugging Face model.</p>
<p>Now, we would need to</p>
<ul>
<li>Load our dataset (sentiment, instrucitons, legal docs)</li>
<li>Tokenize the inputs</li>
<li>Feed those into a training loop</li>
</ul>
<p>This is the part where your text actually enters the model. We&rsquo;re teaching the adapters how to nudge the base model&rsquo;s outputs for our specific task.</p>
<p>Using Hugging Face Trainer, the setup is identical to a full-fine tuning case except our model now has low-rank matrices attached to it and the base weights are frozen.</p>
<p>peft_train.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> Trainer, TrainingArguments
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#f92672">=</span>TrainingArguments(
</span></span><span style="display:flex;"><span>        output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./results&#34;</span>,
</span></span><span style="display:flex;"><span>        num_train_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
</span></span><span style="display:flex;"><span>        per_device_train_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>        learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-4</span>,
</span></span><span style="display:flex;"><span>        evaluation_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>,
</span></span><span style="display:flex;"><span>        fp16<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>        logging_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>        save_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#f92672">=</span>train_dataset,
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#f92672">=</span>eval_dataset,
</span></span><span style="display:flex;"><span>    tokenizer<span style="color:#f92672">=</span>tokenizer
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span></code></pre></div><p>the training looks exactly the same but we&rsquo;re now updating only a handful of parameters (handful relative to the billions of base parameters).</p>
<h3 id="diy-lora">DIY LoRA<a hidden class="anchor" aria-hidden="true" href="#diy-lora">#</a></h3>
<p>Hugging Face makes it so simple and efficient it almost feels like cheating at times. Let&rsquo;s do it ourselves to see how it train this under the hood. We can expect the code to be <em>much more complicated</em> here.</p>
<p>We first need a LoRA linear class</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LoRALinear</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, in_features, out_features, r<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>W <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(in_features, out_features, bias<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>r <span style="color:#f92672">=</span> r
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> alpha
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>scaling <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>alpha <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>r
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># LoRA parameters</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>A <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>randn(out_features, r) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>B <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(r, in_features))  <span style="color:#75715e"># init ΔW = 0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Freeze W</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        delta_W <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>A <span style="color:#f92672">@</span> self<span style="color:#f92672">.</span>B  <span style="color:#75715e"># shape: (out_features x in_features)</span>
</span></span><span style="display:flex;"><span>        W_eff <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>W<span style="color:#f92672">.</span>weight <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>scaling <span style="color:#f92672">*</span> delta_W
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">@</span> W_eff<span style="color:#f92672">.</span>T  <span style="color:#75715e"># simulate nn.Linear with W_eff</span>
</span></span></code></pre></div><p>Next up, we have to do some model surgery. We need to traverse the transformer (via <code>named_modules()</code> usually) and manually replace the Linear layers inside query, value, and sometimes dense, output, etc. (if you are doing a different version of LoRA).</p>
<p>This is how it would look like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertModel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inject_lora_into_bert</span>(model, r<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;attention&#39;</span> <span style="color:#f92672">in</span> name <span style="color:#f92672">and</span> isinstance(module, nn<span style="color:#f92672">.</span>Linear):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#39;query&#39;</span> <span style="color:#f92672">in</span> name <span style="color:#f92672">or</span> <span style="color:#e6db74">&#39;value&#39;</span> <span style="color:#f92672">in</span> name:
</span></span><span style="display:flex;"><span>                parent <span style="color:#f92672">=</span> get_parent_module(model, name)
</span></span><span style="display:flex;"><span>                key <span style="color:#f92672">=</span> name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>                orig <span style="color:#f92672">=</span> getattr(parent, key)
</span></span><span style="display:flex;"><span>                lora_layer <span style="color:#f92672">=</span> LoRALinear(orig<span style="color:#f92672">.</span>in_features, orig<span style="color:#f92672">.</span>out_features, r, alpha)
</span></span><span style="display:flex;"><span>                setattr(parent, key, lora_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_parent_module</span>(model, name):
</span></span><span style="display:flex;"><span>    parts <span style="color:#f92672">=</span> name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> part <span style="color:#f92672">in</span> parts[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]:
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> getattr(model, part)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span></code></pre></div><p>Now, we got the hard part out of the way. We can load the model and inject LoRA.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertModel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_model <span style="color:#f92672">=</span> BertModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>)
</span></span><span style="display:flex;"><span>inject_lora_into_bert(base_model)
</span></span></code></pre></div><p>Here, BERT is now frozen except for the tiny A and B matrices in the Q and V layers. These are the only things getting updated in backpropagation.</p>
<p>The rest kind of follows the same structure as a regular NLP pipeline. Let&rsquo;s take an example where we are doing binary sentiment classification.</p>
<p>Our model is patched so we can build a simple dataset and tokenizer setup for binary classification (IMDb).</p>
<p>data.py</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#34;imdb&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> BertTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tokenize</span>(batch):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tokenizer(batch[<span style="color:#e6db74">&#34;text&#34;</span>], truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenized <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>map(tokenize, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>tokenized<span style="color:#f92672">.</span>set_format(type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;torch&#39;</span>, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;input_ids&#39;</span>, <span style="color:#e6db74">&#39;attention_mask&#39;</span>, <span style="color:#e6db74">&#39;label&#39;</span>])
</span></span></code></pre></div><p>Since we’re using BERT (which is an <em>encoder-only</em> model) we need to add a classification head if we want to fine-tune it on a task that outputs a label (like sentiment or genre classification). BERT processes input and produces a hidden representation, but it doesn’t naturally generate text, so we map its [CLS] token output to a class via a small linear layer.</p>
<p>On the other hand, if we were fine-tuning a <em>decoder-only</em> model like Mistral or LLaMA, and the task involved generating text (like summarization, dialogue, or instruction tuning), we wouldn’t need a classification head at all. We could just use the model’s built-in autoregressive capabilities.</p>
<p>Here&rsquo;s a quick breakdown</p>
<ul>
<li>If your labels are categories → classification head required</li>
<li>If your labels are text → just use generation (no extra heads)</li>
<li>If your labels are per token → use token classification heads</li>
</ul>
<p>Let&rsquo;s continue with BERT classification. We would add the classification head and train.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BertWithLoRAForClassification</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, base_model, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> base_model
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(base_model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>hidden_size, num_classes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_ids, attention_mask, labels<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_ids<span style="color:#f92672">=</span>input_ids, attention_mask<span style="color:#f92672">=</span>attention_mask)
</span></span><span style="display:flex;"><span>        pooled <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>pooler_output  <span style="color:#75715e"># or use mean over last hidden state</span>
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(pooled)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> labels <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()(logits, labels)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;loss&#34;</span>: loss, <span style="color:#e6db74">&#34;logits&#34;</span>: logits}
</span></span></code></pre></div><p>And here&rsquo;s what a training loop would look like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> torch.utils.data <span style="color:#f92672">import</span> DataLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.optim <span style="color:#66d9ef">as</span> optim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> BertWithLoRAForClassification(base_model)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(filter(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>requires_grad, model<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(tokenized[<span style="color:#e6db74">&#34;train&#34;</span>], batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> train_loader:
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(input_ids<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;input_ids&#34;</span>],
</span></span><span style="display:flex;"><span>                       attention_mask<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;attention_mask&#34;</span>],
</span></span><span style="display:flex;"><span>                       labels<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;label&#34;</span>])
</span></span><span style="display:flex;"><span>        output[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p>Let&rsquo;s recap what we did here:</p>
<ol>
<li>We first froze the base transformer</li>
<li>Manually injected LoRA layers (into query, value)</li>
<li>Trained only tiny matrices (A @ B)</li>
</ol>
<p>And we can expect to get real performance with a tiny parameter budget.</p>
<p>Here&rsquo;s the overall structure of what we just wrote</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>Text → [Tokenized] → [BERT (frozen)] → [LoRA-injected Q &amp; V] → [CLS vector] → [Classification Head] → Prediction
</span></span></code></pre></div><p>Before we move on to another part, I wanted to talk about why we decided to inject LoRA between Q and V. Here&rsquo;s a quick refresher about Q, K, and V:</p>
<ul>
<li>Q (Query): What am I looking for?</li>
<li>K (Key): What is available?</li>
<li>V (Value): What information do I get if there&rsquo;s a match?</li>
</ul>
<p>Changing Q changes what the model <em>pays attention to</em>. This is very important since we want our model to attend more to tokens similar to what is in our fine-tuned training set instead of the current way it is set up.</p>
<p>Changing V is also important because it changes the <em>core information</em> that is passed on once attention is calculated. Fine-tuning takes advantage of this because it can change the <em>meaning</em> of the information that is being passed in.</p>
<p>Combining these two, we steer both <strong>attention focus</strong> and <strong>content delivery</strong> which is extremely powerful for adapting the model.</p>
<p>Now that we have a solid foundation of what LoRA is doing, let&rsquo;s start pushing it.</p>
<h2 id="what-happens-if-we-compress-too-far">What happens if we compress too far?<a hidden class="anchor" aria-hidden="true" href="#what-happens-if-we-compress-too-far">#</a></h2>
<p>But before diving into that, let&rsquo;s talk about how we can compress these models in the first place.</p>
<p>When pushing LoRA to its limits, there are two major levers we can pull:</p>
<ul>
<li>Quantization: Compress model weights by reducing precision (like FB32 → INT8). We can do this in a couple of different ways
<ul>
<li>Post training (static or dynamic quantization)</li>
<li>During training (as in QLoRA which has gradient checkpointing and paged optimizers)</li>
</ul>
</li>
<li>Low-Rank Decomposition: Reduce the rank <code>r</code> of the LoRA adapters (make the trainable <code>A</code> and <code>B</code> matrices smaller).
<ul>
<li>Smaller <code>r</code> gives us fewer trainable params and more compression</li>
<li>If we compress too far, we lose expressive power</li>
</ul>
</li>
</ul>
<p>The craziest part about these two techniques is that they are orthogonal. We can combine them to train small low-rank adapters on top of a quantized base model.</p>
<blockquote>
<p>Think of it like tuning a guitar: quantization is changing the thickness of your strings, and LoRA rank is how many fingers you have on the fretboard. Go too far either way and things start sounding off.</p></blockquote>
<p>And that&rsquo;s exactly what we are going to do. But in this section I want to talk about the theoretical limits first.</p>
<p>Essentially, I want to answer the question, &ldquo;when do we cross the line from &rsquo;efficient&rsquo; to &lsquo;destructive&rsquo;?&rdquo; from a theoretical standpoint.</p>
<h3 id="what-do-we-lose-in-quantization">What do we lose in quantization?<a hidden class="anchor" aria-hidden="true" href="#what-do-we-lose-in-quantization">#</a></h3>
<p>The tradeoff here is <strong>precision loss</strong>.</p>
<p>Specifically, we reduce the precision of the weights and activations used in the neural network. Instead of storing each weight as a 32-bit float (FP32) we might store them in different ways.</p>
<ul>
<li>16-bit float (FP16) → mild compression (used in mixed-precision training)</li>
<li>8-bit integer (INT8) → big compression (common in production)</li>
<li>4-bit integer (INT4) → maximum compression for LLM usage (used in ollama)</li>
<li>2-bit integer (INT2) → ultra compression (serious tradeoff in model quality)</li>
<li>1-bit integer (INT1) → a lost cause most of the time (these are binary weights used in tiny embedded models)</li>
</ul>
<p>Quantization is nice because smaller weights = smaller models which means less vRAM usage while training and inference. Also, integer math is faster on most hardware, giving us a compute advantage as well. But, we lose granularity which can affect model performance.</p>
<p>Let&rsquo;s take an example where a certain weight in a random layer is 0.123456789 in FP32.</p>
<p>If stored as INT8 (which only has 256 discrete values in total), it might get rounded to just 0.12 or even 0.1. For a single weight, that&rsquo;s not bad. But if we do this <strong>for every single weight</strong> inside a massive matrix multiplication, we get a compounding eror.</p>
<p>Here&rsquo;s what can happen with that subtle shift:</p>
<ul>
<li>Attention weights might favor the wrong tokens</li>
<li>LayerNorm might lose balance</li>
<li>Gradients might vanish or explode</li>
</ul>
<p>In a real world model, in self-attention we could see this</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-txt" data-lang="txt"><span style="display:flex;"><span>token A → 0.23891
</span></span><span style="display:flex;"><span>token B → 0.23892
</span></span></code></pre></div><p>in high precision, B wins barely. In INT8, both get quantized to 0.23 meaning that the attention scores are technically wrong here because it has given equal value to both (which is going to hurt after softmax) even though token B should slightly beat out after softmax.</p>
<p>We see these effects in real life. Have you ever used ollama and seen how text is generated there? Those are most likely quantized as much as possible so they don&rsquo;t have to pay for much compute. LLM giants like ChatGPT, Claude, and Gemini can afford to run full-precision models — and that’s exactly why their responses feel sharper, more nuanced, and more accurate.</p>
<h3 id="what-do-we-lose-in-lora-rank-decay">What do we lose in LoRA rank decay?<a hidden class="anchor" aria-hidden="true" href="#what-do-we-lose-in-lora-rank-decay">#</a></h3>
<p>The tradeoff here is <strong>capacity loss</strong>.</p>
<p>Let&rsquo;s think about what LoRA does exactly: it approximates <code>ΔW = A @ B</code>. And both matrices are of rank <code>r</code>. A larger value for <code>r</code> gives us more expressive power whereas lower <code>r</code> is a tighter bottleneck (just like layers in deep learning). Smaller <code>r</code> forces the model to &ldquo;pack&rdquo; more meaning into fewer degrees of freedom, which is exactly what we are going to struggle with.</p>
<blockquote>
<p>Imagine trying to describe a 1000-pixel image using just 4 brush strokes. That’s what low-rank LoRA is doing at r=4.</p></blockquote>
<p>Empiricallly, classification tasks hold up at <code>r=8</code> and sometimes at <code>r=4</code>. Instruction-tuning may degrade below <code>r=8</code>.</p>
<p>Here&rsquo;s what I found online and with consulting with Claude:</p>
<ul>
<li>r=16 → near full-fine-tune performance</li>
<li>r=8 → good for many tasks</li>
<li>r=4 → tolerable</li>
<li>r=2 → fragile</li>
<li>r=1 → mostly breaks</li>
</ul>
<h3 id="what-do-we-lose-when-we-take-advantage-of-orthogonality">What do we lose when we take advantage of orthogonality?<a hidden class="anchor" aria-hidden="true" href="#what-do-we-lose-when-we-take-advantage-of-orthogonality">#</a></h3>
<p>The tradeoff here is that we are <strong>compounding the damage</strong>. We&rsquo;re squeezing both the model&rsquo;s capacity and precision at the same time which puts us in a risky zone.</p>
<p>Quantization distorts <em>how</em> the model sees the world (by rounding off detail) while low-rank LoRA limits what the model <em>can express</em> (by shrinking the update space). We&rsquo;re essentially saying</p>
<p>&ldquo;Here&rsquo;s a tiny canvas and we&rsquo;re giving you blunt crayons to draw with.&rdquo;</p>
<blockquote>
<p>This is like giving a blurry camera (quantized model) to a painter with only 2 colors (LoRA <code>r=2</code>) and expecting a photorealistic portrait.</p></blockquote>
<p>So in the next part, we’ll test just how far we can push until results crumble. We’ll try LoRA at ranks 16, 8, 4, 2, and 1 (both on quantized and unquantized models) and find out exactly where we start to lose meaning.</p>
<h2 id="extreme-lora-experiments">Extreme LoRA experiments<a hidden class="anchor" aria-hidden="true" href="#extreme-lora-experiments">#</a></h2>
<p>Just like before, we have to decide if we want to use the Hugging Face PEFT method or go with the DIY approach. I&rsquo;m leaning toward the DIY method for these experiments so we can:</p>
<ul>
<li>Control exactly where LoRA is injected</li>
<li>Run rank sweeps cleanly</li>
<li>Visualize the effects of low-rank tuning more transparently</li>
</ul>
<p>The goal here is to implement that stress testing that I talked about in the previous section. Here&rsquo;s what the sweep lists would look like</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>rank_sweep <span style="color:#f92672">=</span> [<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>]
</span></span></code></pre></div><p>and we would have different types of models</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>models <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;bert-base-uncased (FP32)&#34;</span>,        <span style="color:#75715e"># Standard full-precision model</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;bert-base-uncased (INT8)&#34;</span>,        <span style="color:#75715e"># Quantized using bitsandbytes (INT8)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;distilbert-base-uncased (FP32)&#34;</span>,  <span style="color:#75715e"># Smaller model, full-precision</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;distilbert-base-uncased (INT8)&#34;</span>   <span style="color:#75715e"># Smaller, quantized model</span>
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><p>By sweeping <code>r</code> across these 4 models we can figure out how robust LoRA is across model scale, precision regimes, and rank compression limits.</p>
<p>In the next two sections, we would have similar code other than one small change.</p>
<h3 id="various-r-values-no-quantization">Various <code>r</code> values, no quantization<a hidden class="anchor" aria-hidden="true" href="#various-r-values-no-quantization">#</a></h3>
<p>We&rsquo;d want to just pick out the non-quantized models first.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>models_v1 <span style="color:#f92672">=</span> [models[<span style="color:#ae81ff">0</span>], models[<span style="color:#ae81ff">2</span>]]
</span></span></code></pre></div><p>We&rsquo;re going to be borrowing a bunch of stuff from code snippets above, so I won&rsquo;t write them out here again. Here&rsquo;s what we would need set up before we can jump in:</p>
<ul>
<li>A custom <code>LoRALinear</code> module</li>
<li><code>bert-base-uncased</code> and <code>distilbert-base-uncased</code> downloaded to local</li>
<li>A classification dataset (IMDb, SST-2, etc.)</li>
</ul>
<p>LoRA injection (with default <code>r</code> and <code>alpha</code> values):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">inject_lora_into_model</span>(model, r<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> name, module <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_modules():
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> isinstance(module, nn<span style="color:#f92672">.</span>Linear) <span style="color:#f92672">and</span> (<span style="color:#e6db74">&#39;query&#39;</span> <span style="color:#f92672">in</span> name <span style="color:#f92672">or</span> <span style="color:#e6db74">&#39;value&#39;</span> <span style="color:#f92672">in</span> name):
</span></span><span style="display:flex;"><span>            parent <span style="color:#f92672">=</span> get_parent_module(model, name)
</span></span><span style="display:flex;"><span>            key <span style="color:#f92672">=</span> name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>            orig <span style="color:#f92672">=</span> getattr(parent, key)
</span></span><span style="display:flex;"><span>            lora_layer <span style="color:#f92672">=</span> LoRALinear(orig<span style="color:#f92672">.</span>in_features, orig<span style="color:#f92672">.</span>out_features, r<span style="color:#f92672">=</span>r, alpha<span style="color:#f92672">=</span>alpha)
</span></span><span style="display:flex;"><span>            setattr(parent, key, lora_layer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_parent_module</span>(model, name):
</span></span><span style="display:flex;"><span>    parts <span style="color:#f92672">=</span> name<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#39;.&#39;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> part <span style="color:#f92672">in</span> parts[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]:
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> getattr(model, part)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span></code></pre></div><p>LoRA rank sweep:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_lora_rank_sweep</span>(base_model_name, rank_sweep, quantized<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span>    results <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> rank_sweep:
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Running LoRA with r = </span><span style="color:#e6db74">{</span>r<span style="color:#e6db74">}</span><span style="color:#e6db74"> on model: </span><span style="color:#e6db74">{</span>base_model_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># load base model</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> quantized:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BitsAndBytesConfig, AutoModel
</span></span><span style="display:flex;"><span>            bnb_config <span style="color:#f92672">=</span> BitsAndBytesConfig(load_in_8bit<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>            model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(base_model_name, quantization_config<span style="color:#f92672">=</span>bnb_config, device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(base_model_name)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># inject LoRA manually</span>
</span></span><span style="display:flex;"><span>        inject_lora_into_model(model, r<span style="color:#f92672">=</span>r, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># wrap in classification head</span>
</span></span><span style="display:flex;"><span>        wrapped_model <span style="color:#f92672">=</span> BertWithLoRAForClassification(model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># train and evaluate</span>
</span></span><span style="display:flex;"><span>        acc <span style="color:#f92672">=</span> train_and_eval(wrapped_model, r)
</span></span><span style="display:flex;"><span>        results[r] <span style="color:#f92672">=</span> acc
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> results
</span></span></code></pre></div><p>Classification Wrapper</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BertWithLoRAForClassification</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">__init__</span>(self, base_model, num_classes<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span><span style="color:#a6e22e">__init__</span>()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> base_model
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>classifier <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(base_model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>hidden_size, num_classes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_ids, attention_mask, labels<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_ids<span style="color:#f92672">=</span>input_ids, attention_mask<span style="color:#f92672">=</span>attention_mask)
</span></span><span style="display:flex;"><span>        pooled <span style="color:#f92672">=</span> outputs<span style="color:#f92672">.</span>pooler_output
</span></span><span style="display:flex;"><span>        logits <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>classifier(pooled)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> labels <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()(logits, labels)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;loss&#34;</span>: loss, <span style="color:#e6db74">&#34;logits&#34;</span>: logits}
</span></span></code></pre></div><p>Lastly, for training</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_and_eval</span>(model, r_value):
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>    optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(filter(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>requires_grad, model<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-4</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    train_loader <span style="color:#f92672">=</span> DataLoader(tokenized[<span style="color:#e6db74">&#34;train&#34;</span>], batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> train_loader:
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>            output <span style="color:#f92672">=</span> model(input_ids<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;input_ids&#34;</span>],
</span></span><span style="display:flex;"><span>                           attention_mask<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;attention_mask&#34;</span>],
</span></span><span style="display:flex;"><span>                           labels<span style="color:#f92672">=</span>batch[<span style="color:#e6db74">&#34;label&#34;</span>])
</span></span><span style="display:flex;"><span>            output[<span style="color:#e6db74">&#34;loss&#34;</span>]<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>            optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    acc <span style="color:#f92672">=</span> evaluate(model, tokenized[<span style="color:#e6db74">&#34;test&#34;</span>])
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;r = </span><span style="color:#e6db74">{</span>r_value<span style="color:#e6db74">}</span><span style="color:#e6db74"> → accuracy = </span><span style="color:#e6db74">{</span>acc<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> acc
</span></span></code></pre></div><p>we would have to run this loop for each model in the <code>models_v2</code> list that we defined above.</p>
<p>We can check how well this performed by some basic plots</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_results</span>(results_dict, model_name):
</span></span><span style="display:flex;"><span>    ranks <span style="color:#f92672">=</span> list(results_dict<span style="color:#f92672">.</span>keys())
</span></span><span style="display:flex;"><span>    accs <span style="color:#f92672">=</span> list(results_dict<span style="color:#f92672">.</span>values())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(ranks, accs, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;LoRA Rank (r)&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Accuracy&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;LoRA Rank Sweep – </span><span style="color:#e6db74">{</span>model_name<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>gca()<span style="color:#f92672">.</span>invert_xaxis()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="various-r-values-with-quantization">Various <code>r</code> values, with quantization<a hidden class="anchor" aria-hidden="true" href="#various-r-values-with-quantization">#</a></h3>
<p>Same deal as before but we&rsquo;d want the quantized models here.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>models_v2 <span style="color:#f92672">=</span> [models[<span style="color:#ae81ff">1</span>], models[<span style="color:#ae81ff">3</span>]]
</span></span></code></pre></div><p>Other than that, the steps would be the same.</p>
<h3 id="comparing-results">Comparing results<a hidden class="anchor" aria-hidden="true" href="#comparing-results">#</a></h3>
<p>Once we&rsquo;ve run the rank sweeps for both full-precision and quantized models, we can compare their performance side-by-side.</p>
<p>In my opinion, these are the most useful metrics to focus on.</p>
<ol>
<li><strong>F1-score</strong></li>
</ol>
<p>This tells us how well the model is actually performing on the classification task. For clean datasets like IMDb or SST-2, accuracy is usually enough. However, for imbalanced datasets, F1-score provides a better picture by balancing precision and recall (especially when false positives or false negatives carry different weights).</p>
<ol start="2">
<li><strong>Number of trainable parameters</strong></li>
</ol>
<p>This helps quantify the efficiency gains from LoRA as we reduce the rank <code>r</code>. Logging the number of parameters in <code>A @ B</code> vs. the full model would show us how much smaller the update space is - espeically the steep drop from <code>r=32</code> to <code>r=1</code>.</p>
<ol start="3">
<li><strong>Training time per epoch</strong></li>
</ol>
<p>This can highlight how model size and quantization affect training speed which is very useful when comparing quantized vs. non-quantized models. INT8 models have faster matrix multiplications which should reduce forward / backward pass time, if you hardware supports INT8 acceleration.</p>
<p>This would show us how much time we are saving per forward call / inference. Quantization usually gives us faster inference but training speed depends on hardware.</p>
<h2 id="can-we-save-our-garbage-models">Can we save our (garbage) models?<a hidden class="anchor" aria-hidden="true" href="#can-we-save-our-garbage-models">#</a></h2>
<p>After seeing <code>r=1</code> and <code>r=2</code> crumble, I started wondering if there was anything we could do to <em>recover performance without increasing rank</em>. I didn&rsquo;t test all of these strategies (yet) but here were the main strategies I was considering to make ulta-low-rank LoRA viable.</p>
<h3 id="lora-alpha-tuning">LoRA alpha tuning<a hidden class="anchor" aria-hidden="true" href="#lora-alpha-tuning">#</a></h3>
<p>The LoRA <code>α</code> parameter acts like a multiplier on the low-rank update. It controls how much influence <code>A @ B</code> has in <code>W_eff = W + α * (A @ B)</code>.</p>
<p>At very low rank (<code>r=1</code> or <code>r=2</code>) the update <code>A @ B</code> might be too weak to meaningfully steer the model, it barely moves the needle. By increasing alpha, we amplify the effect of the adapter, giving it a stronger say in the final output.</p>
<p>We can think of giving our tiny adapter a megaphone.</p>
<blockquote>
<p>You can&rsquo;t make it (LoRA) smarter but you can make it louder.</p></blockquote>
<h3 id="more-epochs--warmup-schedules">More epochs &amp; warmup schedules<a hidden class="anchor" aria-hidden="true" href="#more-epochs--warmup-schedules">#</a></h3>
<p>When rank is low, the model’s update space is extremely restricted.
It might just need more time to discover a useful subspace. Especially if we use a <code>r=1</code> matrix over thousands of training examples it&rsquo;s going ot be slow to learn anything meaningful. We could do a few things to help the model:</p>
<ul>
<li>Train longer (more epochs)</li>
<li>Use a cosine or linear warmp schedule to easy into training</li>
<li>Try a smaller learning rate with gradual ramp-up to avoid catastrophic updates early on</li>
</ul>
<h3 id="targeted-lora-injection">Targeted LoRA injection<a hidden class="anchor" aria-hidden="true" href="#targeted-lora-injection">#</a></h3>
<p>Instead of injecting LoRA into every attention block (as we did above), we can try injection only in the last few transformer layers.</p>
<p>The later layers of the model tend to be more task-specific while early layers are more general. Instead of spreading a tiny adapter thin across the whole network, we can focus our compression budget on the blocks that matter the most. This reduces interference and makes it easier for the model to converge.</p>
<blockquote>
<p><code>r=1</code> everywhere might be too shallow but <code>r=1</code> in the right place might be just enough</p></blockquote>
<p>For certain tasks, it can be more effective to inject LoRA into output projection layers—such as the attention output or the MLP output layers—instead of the query or value layers. These output projections often carry more task-specific signal, making them better targets for adaptation.</p>
<h3 id="lora-dropout">LoRA dropout<a hidden class="anchor" aria-hidden="true" href="#lora-dropout">#</a></h3>
<p>Just like regular dropout, LoRA adapters support <code>lora_dropout</code> which randomly zeroes out a fraction of adapter outputs during training (like standard dropout).</p>
<p>Dropout might seem unintuitive at low-rank because of how little signal we get already but it can actually help prevent overfitting on noise, especially when we are working on a small-task specific dataset. Dropout forces our model to generalize a bit more, even when the adapter space is small.</p>
<p>Trying values like <code>lora_dropout=0.05</code> or <code>0.1</code> can really help stabilize learning.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>LoRA is one of the most powerful tools in the modern fine-tuning toolbox. Not because it performs miracles, but because it scales well. It gives us a way to adapt massive models with minimal memory, relatively low training time, and surprisingly solid performance.</p>
<p>But as we saw, <strong>everything breaks if you push it far enough</strong>. We can’t just compress forever and expect magic.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://akhilvreddy.github.io/"> </a></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
