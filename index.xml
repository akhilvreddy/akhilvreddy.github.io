<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hi, I&#39;m Akhil on Akhil’s Blog</title>
    <link>https://akhilvreddy.github.io/</link>
    <description>Recent content in Hi, I&#39;m Akhil on Akhil’s Blog</description>
    <generator>Hugo -- 0.147.8</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 12 Jul 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://akhilvreddy.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ResNet: Deep Residual Learning for Image Recognition (He et al. - 2015)</title>
      <link>https://akhilvreddy.github.io/reimplementations/resnet/</link>
      <pubDate>Sat, 12 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/resnet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1512.03385&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/resnet-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA really isn&#39;t that bad: Tiling, Fusion, and Triton</title>
      <link>https://akhilvreddy.github.io/posts/cuda2/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/cuda2/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post dives deeper into advanced CUDA performance techniques from tiling and pipelining to occupancy tuning, loop unrolling, and warp-aware memory access. I explain how real-world matrix multiplies use shared memory and grid-stride loops to handle massive inputs efficiently, and how tricks like operator fusion and double buffering unlock GPU throughput. We also look at how OpenAI’s Triton gives you Python-first control over writing custom fused GPU kernels.&lt;/p&gt;
&lt;p&gt;The repsitory with the corresponding PyTorch implementation is available &lt;a href=&#34;https://github.com/akhilvreddy/word2vec-scratch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adam: A Method for Stochastic Optimization (Kingma &amp; Ba - 2014)</title>
      <link>https://akhilvreddy.github.io/reimplementations/adam/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/adam/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1412.6980&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/adam-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CUDA really isn&#39;t that bad: Kernel Ops and Memory Hierarchy</title>
      <link>https://akhilvreddy.github.io/posts/cuda/</link>
      <pubDate>Thu, 03 Jul 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/cuda/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: This post demystifies the core concepts behind CUDA, walking through how GPU kernels work, how threads and memory hierarchies are structured, and how to write and launch a kernel. It’s a hands-on introduction to CUDA for engineers who want to understand how deep learning frameworks really run under the hood (and how writing a few lines of CUDA can unlock massive speedups).&lt;/p&gt;
&lt;p&gt;The repsitory with the corresponding PyTorch implementation is available &lt;a href=&#34;https://github.com/akhilvreddy/word2vec-scratch&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Designing &amp; Building an Agentic AI framework from scratch</title>
      <link>https://akhilvreddy.github.io/posts/agentic-framework/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/agentic-framework/</guid>
      <description>I&amp;rsquo;ve seen a lot of people use pre-made agentic frameworks to handle various tasks but I wanted to create a framework from scratch to see the brain, actions, and tools behind these agents.</description>
    </item>
    <item>
      <title>Attention Is All You Need (Vaswani et al. - 2017)</title>
      <link>https://akhilvreddy.github.io/reimplementations/attentionisallyouneed/</link>
      <pubDate>Fri, 27 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/attentionisallyouneed/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1706.03762&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/transformers-reimplementation/tree/main&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tesla&#39;s Robotaxi &gt; Google&#39;s Waymo: Vision vs. LiDAR</title>
      <link>https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/robotaxi-vs-waymo/</guid>
      <description>In the coming years, I believe Robotaxi will scale like Uber, while Waymo will scale like Lyft - if at all. This reflects a broader paradigm shift: lean vision neural networks vs. bulky, sensor-heavy autonomy.</description>
    </item>
    <item>
      <title>LayerNorm (Ba, Kiros, and Hinton - 2016)</title>
      <link>https://akhilvreddy.github.io/reimplementations/layernorm/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/layernorm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.06450&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/layernorm-reimplementation/tree/main&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In my opinion, LayerNorm was easier to implement than BatchNorm because it&amp;rsquo;s basically the same idea, just applied in a different dimension. Unlike BatchNorm, we don&amp;rsquo;t have to keep track of batch-level statistics in training or running statistics in eval, which helps reduce a lot of overhead. I liked how LayerNorm is fully independent of external hyperparams (like batch size) making it super simple to add in at any layer.
I implemented the &lt;code&gt;__init__&lt;/code&gt; method and the forward pass (just stuck with &lt;code&gt;loss.backward()&lt;/code&gt;). Once I understood that you&amp;rsquo;re just normalizing all features within a single sample, and optionally learning a scale and shift (just like batchnorm), I was able to see how it is really working. Writing it from scratch helped me understand why it helps with stability and training speed, and why it shows up in every transformer or modern deep learning model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying a toy ML model to production</title>
      <link>https://akhilvreddy.github.io/posts/deploying-model/</link>
      <pubDate>Sat, 14 Jun 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/deploying-model/</guid>
      <description>I&amp;rsquo;ve trained hundreds of models in school and on my own, but barely any of them were served or pushed to production - I wanted to document what usually happens &lt;em&gt;after&lt;/em&gt; training.</description>
    </item>
    <item>
      <title>BatchNorm (Ioffe &amp; Szegedy - 2015)</title>
      <link>https://akhilvreddy.github.io/reimplementations/batchnorm/</link>
      <pubDate>Tue, 10 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/reimplementations/batchnorm/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1502.03167&#34;&gt;Here&lt;/a&gt; is the original paper for reference. And &lt;a href=&#34;https://github.com/akhilvreddy/batchnorm-reimplementation&#34;&gt;here&lt;/a&gt; is the reimplementation in PyTorch.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Honestly BatchNorm wasn’t as complicated to implement as I expected. I implemented the &lt;code&gt;__init__&lt;/code&gt; method and the forward pass (just stuck with &lt;code&gt;loss.backward&lt;/code&gt;). Once I understood that you normalize each feature across the batch, then optionally learn a scale and shift, it was pretty straightforward. Writing it from scratch helped me understand why it helps with stability and training speed, and why it shows up in basically every model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fun with LoRA: How low-rank can we go before adjacency matrices break down?</title>
      <link>https://akhilvreddy.github.io/posts/tiny-lora/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/tiny-lora/</guid>
      <description>How far can you compress adjacency matrices before everything falls apart? I pushed LoRA to its limits and watched adjacency melt into noise.</description>
    </item>
    <item>
      <title>Designing AlphaTicTacToe (and AlphaTicTacToeZero)</title>
      <link>https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/</link>
      <pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/alpha-tic-tac-toe/</guid>
      <description>AlphaGo blew my mind and AlphaGoZero was the cherry on top. Recreating this in a simple environment was a long term dream of mine.</description>
    </item>
    <item>
      <title>Can a EMNIST model run on an Amazon Kindle from 2012?</title>
      <link>https://akhilvreddy.github.io/posts/quantized-emnist/</link>
      <pubDate>Thu, 10 Apr 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/quantized-emnist/</guid>
      <description>What happens when you take a modern ML model, quantize it like crazy, and try to deploy it on decade-old hardware (a potato)? I tested EMNIST on the edge — literally.</description>
    </item>
    <item>
      <title>Building Transformers from scratch: Multi-Head Attention, LayerNorm, and the brain behind ChatGPT</title>
      <link>https://akhilvreddy.github.io/posts/attention-from-scratch/</link>
      <pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/posts/attention-from-scratch/</guid>
      <description>I felt like I would never understand how LLMs really generate text until I actually create a transformer from scratch and actually code up multi-head self attention with non-modular, functional-style PyTorch.</description>
    </item>
    <item>
      <title>How did we get to Transformers? The rise of Attention</title>
      <link>https://akhilvreddy.github.io/posts/attention-beginnings/</link>
      <pubDate>Fri, 28 Feb 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/attention-beginnings/</guid>
      <description>Attention is really useful but just like any great invention, it was built out of necessity for the problems at hand. In this post, I aim to dive into the issues researchers were dealing with between 2013-2017.</description>
    </item>
    <item>
      <title>Word2Vec from scratch: Intuition to Implementation</title>
      <link>https://akhilvreddy.github.io/posts/word2vec-scratch/</link>
      <pubDate>Fri, 10 Jan 2025 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/word2vec-scratch/</guid>
      <description>Q(king) - Q(man) + Q(woman) ≈ Q(queen) is beautiful but the approach we take to get there is even cooler. I wanted to dive into how these emergent properties come to be and the bottlenecks we face.</description>
    </item>
    <item>
      <title>My goals for this blog</title>
      <link>https://akhilvreddy.github.io/posts/first-post/</link>
      <pubDate>Sun, 29 Dec 2024 18:12:56 -0400</pubDate>
      <guid>https://akhilvreddy.github.io/posts/first-post/</guid>
      <description>I guess I&amp;rsquo;m writing a blog now.</description>
    </item>
    <item>
      <title>Tooling</title>
      <link>https://akhilvreddy.github.io/tooling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akhilvreddy.github.io/tooling/</guid>
      <description>&lt;p&gt;After a lot of trial, error, and late-night debugging, these are the tools, libraries, and frameworks I’ve come to rely on.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;development-environment&#34;&gt;Development Environment &lt;p&gt;&lt;p&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Machine&lt;/strong&gt; I use a MacBook Pro 16” (M4, 64GB Unified Memory, 512GB SSD). When getting a laptop, I wanted a machine that could train some models locally if needed and have enough compute to play around with open source models on the laptop itself. I love the 64GB because it lets me run a training loop + Cursor + FastAPI + Docker + a bunch of browser tabs without my machine slowing down at all.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
